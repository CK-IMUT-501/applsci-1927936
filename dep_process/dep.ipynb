{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "animal-august",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "#encoding=utf-8 \n",
    "\n",
    "# nlp环境\n",
    "# 测试stanfordcorenlp是否可用\n",
    "# ('xx',父节点,子节点)\n",
    "# token节点编号从1开始\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import datetime\n",
    "import shutil\n",
    "import os\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import sys\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecological-jimmy",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' ')\n",
    "print('dep.ipynb')\n",
    "\n",
    "MAX_LENGTH = int(sys.argv[1])\n",
    "print('MAX_LENGTH:',MAX_LENGTH)\n",
    "current_datasets_path = sys.argv[2] #'/home/chengkun/jupyter_projects/Magic-NLPer-main/data/en_zh/'\n",
    "print('current_datasets_path:',current_datasets_path)\n",
    "\n",
    "train_pairs_path = current_datasets_path + 'train_pairs'\n",
    "# val_pairs_path = current_datasets_path + 'val_pairs'\n",
    "# test_pairs_path = current_datasets_path + 'test_pairs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "tutorial-forestry",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_pairs_path,'rb') as f:\n",
    "    train_pairs = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "# with open(r'/home/chengkun/jupyter_projects/Magic-NLPer-main/data/data_sets/en_zh/val_pairs','rb') as f:\n",
    "#     val_pairs = pickle.load(f)\n",
    "#     f.close()\n",
    "    \n",
    "# with open(r'/home/chengkun/jupyter_projects/Magic-NLPer-main/data/data_sets/en_zh/test_pairs','rb') as f:\n",
    "#     test_pairs = pickle.load(f)\n",
    "#     f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "reported-adelaide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161754 将 这 些 个 人 加 到 被 官 方 失 业 统 计 口 径 中 ， 则 有 1 5 的 潜 在 劳 动 参 与 者 不 能 如 愿 工 作 。 \n",
      " 但 难 民 和 经 济 移 民 之 间 的 界 限 逐 渐 模 糊 。\n"
     ]
    }
   ],
   "source": [
    "print(len(train_pairs),train_pairs[0][1],'\\n',train_pairs[-1][1])\n",
    "# print(len(val_pairs),val_pairs[0][1],'\\n',val_pairs[-1][1])\n",
    "# print(len(test_pairs),test_pairs[0][1],'\\n',test_pairs[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "logical-philadelphia",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未找到文件\n",
      "0 2021-11-25 11:14:05\n",
      "5000 2021-11-25 11:18:22\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def f1(pairs,save_file):\n",
    "    nlp = StanfordCoreNLP(r'/home/chengkun/java/stanford-corenlp-4.2.2/', lang='zh', memory='8g')\n",
    "    for step, item in enumerate(pairs):\n",
    "        if step%5000==0:\n",
    "            print(step, datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "        item = item[1].replace(' ','')\n",
    "        length_item = len(item)\n",
    "#         print(length_item, item)\n",
    "        \n",
    "        tokens =  nlp.word_tokenize(item)\n",
    "        dep_outputs = nlp.dependency_parse(item) # 解析语法\n",
    "        length_tokens = len(''.join(tokens))\n",
    "#         print(length_tokens, tokens)\n",
    "#         print()\n",
    "        \n",
    "    #     print(tokens)\n",
    "    #     print(dep_outputs)\n",
    "    # nlp.close()\n",
    "\n",
    "        # 查找根结点对应的索引\n",
    "        root_index=[]\n",
    "        for i in range(len(dep_outputs)):\n",
    "            if dep_outputs[i][0]=='ROOT':\n",
    "                root_index.append(i)\n",
    "\n",
    "        # 修改依存关系三元组\n",
    "        new_dep_outputs=[]\n",
    "        for i in range(len(dep_outputs)):\n",
    "            for index in root_index:\n",
    "                if i+1>index:\n",
    "                    tag=index\n",
    "\n",
    "            if dep_outputs[i][0]=='ROOT':\n",
    "                dep_output=(dep_outputs[i][0],dep_outputs[i][1],dep_outputs[i][2]+tag)\n",
    "            else:\n",
    "                dep_output = (dep_outputs[i][0], dep_outputs[i][1] + tag, dep_outputs[i][2] + tag)\n",
    "            new_dep_outputs.append(dep_output)\n",
    "\n",
    "    #     print('new_dep_outputs','\\n',new_dep_outputs)\n",
    "\n",
    "        jilu = []\n",
    "        count = -1\n",
    "        for step1,token in enumerate(tokens):\n",
    "            t = [step1]\n",
    "            for step2,char in enumerate(token):\n",
    "    #             print(count+1,char)\n",
    "                count = count + 1\n",
    "                t.append(count)\n",
    "\n",
    "\n",
    "            jilu.append(t)\n",
    "    #     print('jilu:',jilu)\n",
    "\n",
    "        for step3, item in enumerate(new_dep_outputs):\n",
    "            if item[0] != 'ROOT':\n",
    "                new_dep_outputs[step3] = (item[0],jilu[item[1]-1][1:],jilu[item[2]-1][1:])\n",
    "            else:\n",
    "                new_dep_outputs[step3] = (item[0],[item[1]],jilu[item[2]-1][1:])\n",
    "    #             print(item[2]-1)\n",
    "        # print('new_dep_outputs',new_dep_outputs)\n",
    "\n",
    "        final_dep_outputs = []\n",
    "        for item in new_dep_outputs:\n",
    "            if item[0] != 'ROOT':\n",
    "                for j in item[1]:\n",
    "                    for k in item[2]:\n",
    "                        final_dep_outputs.append((item[0],j,k))\n",
    "\n",
    "                for j in item[2]:\n",
    "                    for k in item[1]:\n",
    "                        final_dep_outputs.append((item[0],k,j)) \n",
    "            else:\n",
    "                for j in item[1]:\n",
    "                    for k in item[2]:\n",
    "                        final_dep_outputs.append((item[0],j,k))\n",
    "    #     print(final_dep_outputs)\n",
    "\n",
    "    #     length = len(''.join(tokens))\n",
    "        # transfomer的最后面需要补个endMAX_LENGTH+1\n",
    "        dep = np.zeros((MAX_LENGTH+1, MAX_LENGTH+1), dtype=np.float32)\n",
    "        for item in final_dep_outputs:\n",
    "            if item[0] != 'ROOT':\n",
    "                dep[item[1],item[2]]=1\n",
    "                dep[item[2],item[1]]=1\n",
    "    #     print(dep.shape)\n",
    "    #     print(dep)\n",
    "        if (dep.transpose() == dep).all() and length_item == length_tokens:\n",
    "            pass\n",
    "        else:\n",
    "            print('处理错误：', step, ''.join(tokens))\n",
    "            dep = np.zeros((MAX_LENGTH+1, MAX_LENGTH+1), dtype=np.float32)\n",
    "    #     print(dep.shape)\n",
    "\n",
    "        dep = torch.from_numpy(dep).unsqueeze(0)\n",
    "        pickle.dump(dep,save_file)\n",
    "\n",
    "    save_file.close()\n",
    "    nlp.close()\n",
    "    return True\n",
    "\n",
    "# os.system('ps -ef | grep java | grep -v grep | cut -c 9-15 | xargs kill -9')\n",
    "# path = r\"/home/chengkun/jupyter_projects/Magic-NLPer-main/data/data_sets/dep_val_pairs\"  # 文件路径\n",
    "# if os.path.exists(path):  # 如果文件存在\n",
    "#     # 删除文件，可使用以下两种方法。\n",
    "#     os.remove(path)  \n",
    "#     #os.unlink(path)\n",
    "# else:\n",
    "#     print('未找到文件')  # 则返回文件不存在\n",
    "# save_file = open(path, \"wb\")\n",
    "# f1(val_pairs,save_file)\n",
    "\n",
    "# load_file.close()\n",
    "#（‘ROOT’,0,2）表示第二个词（like）是“I like swimming .”这句话的根节点；（‘nsubj’，2，1）表示第1个词（I）的父节点（也就是它的head）是第2个词like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-pearl",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('ps -ef | grep java | grep -v grep | cut -c 9-15 | xargs kill -9')\n",
    "path = current_datasets_path + \"dep_train_pairs\"  # 文件路径\n",
    "if os.path.exists(path):  # 如果文件存在\n",
    "    # 删除文件，可使用以下两种方法。\n",
    "    os.remove(path)  \n",
    "    #os.unlink(path)\n",
    "else:\n",
    "    print('未找到文件')  # 则返回文件不存在  \n",
    "\n",
    "save_file = open(path, \"wb\")\n",
    "f1(train_pairs,save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-modem",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('ps -ef | grep java | grep -v grep | cut -c 9-15 | xargs kill -9')\n",
    "print('运行结束')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-furniture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_file = open(\"/home/chengkun/jupyter_projects/Magic-NLPer-main/data/dep.bin\", \"rb\")\n",
    "# data = pickle.load(load_file)\n",
    "# load_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acute-technique",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import datetime\n",
    "# final_dep = torch.zeros(1,102,102)\n",
    "# for i in range(500000):\n",
    "#     if i%1000==0:\n",
    "#         print(final_dep.shape)\n",
    "#         time2 = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "#         print(time2)\n",
    "#     a = torch.zeros(1,102,102)\n",
    "#     final_dep = torch.cat((final_dep,a),0)\n",
    "# print(final_dep.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-hamburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import pickle\n",
    "# import time\n",
    "# A = np.zeros((1,2,2), dtype=np.float32)\n",
    "# B = np.ones((1,2,2), dtype=np.float32)\n",
    "# A=torch.from_numpy(A)    #2x3的张量（矩阵）\n",
    "# print(A)\n",
    "# B=torch.from_numpy(B)  #4x3的张量（矩阵）\n",
    "# print(B)\n",
    "# C=torch.cat((A,B),0)  #按维数0（行）拼接\n",
    "# # print(C.shape)\n",
    "\n",
    "# save_file = open(\"/home/chengkun/jupyter_projects/Magic-NLPer-main/data/dep.bin\", \"wb\")\n",
    "# load_file = open(\"/home/chengkun/jupyter_projects/Magic-NLPer-main/data/dep.bin\", \"rb+\")\n",
    "# pickle.dump(C, save_file)\n",
    "\n",
    "# data = pickle.load(load_file)\n",
    "# print(data)\n",
    "# save_file.close()\n",
    "# load_file.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
