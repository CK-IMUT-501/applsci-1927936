{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "使用transformer解决法语=>英语的翻译任务\n",
    "\n",
    "transformer的核心：self-attentions\n",
    "\n",
    "transformer **优点**：\n",
    "- 无需对跨数据的时间/空间域的关系作出假设\n",
    "- 并行计算\n",
    "- distant item 互相影响彼此的输出\n",
    "- 可以学习到长距离依赖关系\n",
    "\n",
    "transformer **缺点**：\n",
    "- 对于每一个step的xt的输出，是由整个历史信息计算得出，而不再是当前输入和hidden，这可能效率较低\n",
    "- 如果输入具有时间/空间域的关系，则需要加入位置编码，否则整个model也只能看作是一个词袋模型\n",
    "\n",
    "目录\n",
    "* [1.加载数据 建立input pipeline](#)\n",
    "* [2.位置编码 positional encoding](#)\n",
    "* [3.掩码 masking](#3)\n",
    "* [4.scaled dot product attention](#)\n",
    "* [5.multi-head attention](#)\n",
    "* [6.point wise feed forward network](#)\n",
    "* [7.encoder layer](#)\n",
    "* [8.decoder layer](#)\n",
    "* [9.encoder](#)\n",
    "* [10.decoder](#)\n",
    "* [11.搭建transformer](#)\n",
    "* [12.设置超参](#)\n",
    "* [13.优化器](#)\n",
    "* [14.损失和评价准则](#)\n",
    "* [15.生成mask](#)\n",
    "* [16.训练和保存](#)\n",
    "* [17.评估](#)\n",
    "* [18.attention的可视化](#)\n",
    "* [19.总结](#)\n",
    "\n",
    "\n",
    "## 1.加载数据 建立input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer_improved_decoder\n",
      "2021-11-14_10:27:30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import random\n",
    "import re\n",
    "# from tqdm import tqdm  # 进度条\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import unicodedata\n",
    "import datetime\n",
    "import time\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import sacrebleu\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import pickle\n",
    "import torch_optimizer as optim\n",
    "# import adamod\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "from labml_helpers.module import Module\n",
    "from labml_nn.transformers import MultiHeadAttention\n",
    "\n",
    "# print(os.getcwd()) # /home/xijian\n",
    "file_name = 'transformer_improved_decoder1'\n",
    "print(file_name)\n",
    "\n",
    "train_model_save = '/home/chengkun/jupyter_projects/Magic-NLPer-main/train_save/transformer_improved_decoder/'\n",
    "shutil.rmtree(train_model_save)\n",
    "os.mkdir(train_model_save)\n",
    "\n",
    "#保存必要结果\n",
    "begin_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()).replace(' ','_')\n",
    "print(begin_time)\n",
    "result_save = '/home/chengkun/jupyter_projects/Magic-NLPer-main/best_save/' + begin_time +'/'\n",
    "os.mkdir(result_save)\n",
    "\n",
    "os.system('cp  {} {} {}'.format('/home/chengkun/jupyter_projects/Magic-NLPer-main/re/primer-ez/transformer_improved_decoder1.py','/home/chengkun/jupyter_projects/Magic-NLPer-main/re/primer-ez/transformer_improved_decoder1.ipynb',result_save))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquaredReLU(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.relu = torch.nn.LeakyReLU()\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.relu(x)\n",
    "        return x * x\n",
    "\n",
    "class SpatialDepthWiseConvolution(Module):\n",
    "    def __init__(self, d_k: int, kernel_size: int = 3):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = torch.nn.Conv1d(in_channels=d_k, out_channels=d_k,\n",
    "                              kernel_size=(kernel_size,), padding=(kernel_size - 1,), groups=d_k)\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x.unsqueeze(-1).permute(1, 0, 3, 2)\n",
    "        seq_len, batch_size, heads, d_k= x.shape\n",
    "        x = x.permute(1, 2, 3, 0)\n",
    "        x = x.view(batch_size * heads, d_k, seq_len)\n",
    "        x = self.conv(x)\n",
    "        x = x[:, :, :-(self.kernel_size - 1)]\n",
    "        x = x.view(batch_size, heads, d_k, seq_len)\n",
    "        x = x.permute(0, 3, 2, 1) # [batch_size, seq_len, heads, d_k]\n",
    "        x = x.view(batch_size, seq_len, heads, d_k)\n",
    "        x = torch.squeeze(x,2)\n",
    "        return x\n",
    "\n",
    "# class MultiDConvHeadAttention(MultiHeadAttention):\n",
    "#     def __init__(self, heads: int, d_model: int, dropout_prob: float = 0.1):\n",
    "#         super().__init__(heads, d_model, dropout_prob)\n",
    "        \n",
    "#         self.query = torch.nn.Sequential(self.query, SpatialDepthWiseConvolution(self.d_k))\n",
    "#         self.key = torch.nn.Sequential(self.key, SpatialDepthWiseConvolution(self.d_k))\n",
    "#         self.value = torch.nn.Sequential(self.value, SpatialDepthWiseConvolution(self.d_k))\n",
    "        \n",
    "# m = SquaredReLU()\n",
    "# n = torch.nn.LeakyReLU()\n",
    "# input = torch.randn(2)\n",
    "# print(m(input))\n",
    "# print(n(input)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu0 = int(sys.argv[1])\n",
    "gpu1 = int(sys.argv[2])\n",
    "seed = int(sys.argv[3])\n",
    "print('gpu0:',gpu0,'gpu1:',gpu1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngpu： 2\n",
      "batch： 120\n",
      "MAX_LENGTH： 100\n",
      "EPOCHS： 1\n",
      "warm_steps： 3000\n",
      "seed： 100\n"
     ]
    }
   ],
   "source": [
    "# 设置超参数\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"{},{}\".format(gpu0, gpu1)\n",
    "ngpu = 2\n",
    "print('ngpu：', ngpu)\n",
    "\n",
    "batch = 120\n",
    "print('batch：', batch)\n",
    "\n",
    "# MAX_LENGTH = d_model//num_heads\n",
    "MAX_LENGTH = 100\n",
    "print('MAX_LENGTH：', MAX_LENGTH)\n",
    "EPOCHS = 35\n",
    "print('EPOCHS：', EPOCHS)\n",
    "warm_steps=3000\n",
    "print('warm_steps：', warm_steps)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()  #检测是否有可用的gpu\n",
    "device = torch.device(\"cuda:0\" if (use_cuda and ngpu > 0) else \"cpu\")\n",
    "\n",
    "# seed = 256\n",
    "print('seed：', seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "# torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 数据读取\n",
    "# 当你用read_csv读文件的时候，如果文本里包含英文双引号，直接读取会导致行数变少或是直接如下报错停止\n",
    "# 此时应该对read_csv设置参数控制csv中的引号常量，设定quoting=3或是quoting=csv.QUOTE_NONE”（注：用第二种要先导csv库）然后问题就解决了。\n",
    "\n",
    "data_dir = '/home/chengkun/jupyter_projects/Magic-NLPer-main/data/'\n",
    "\n",
    "data_df = pd.read_csv(data_dir + 'ch_mn_50_nodict.txt',  # 数据格式：英语\\t法语，注意我们的任务源语言是法语，目标语言是英语\n",
    "                      encoding='UTF-8', sep='\\t', header=None,quoting=3,\n",
    "                      names=['mn', 'ch'], index_col=False)\n",
    "\n",
    "# print(data_df.shape)\n",
    "# print(data_df.values.shape)\n",
    "# print(data_df.values[0])\n",
    "# print(data_df.values[0].shape)\n",
    "# data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "\n",
    "# 规范化字符串\n",
    "def normalizeString(s):\n",
    "    # print(s) # list  ['Go.']\n",
    "    # s = s[0]\n",
    "    s = s.lower().strip()\n",
    "    #s = unicodeToAscii(s)\n",
    "    #s = re.sub(r\"([.!?])\", r\" \\1\", s)  # \\1表示group(1)即第一个匹配到的 即匹配到'.'或者'!'或者'?'后，一律替换成'空格.'或者'空格!'或者'空格？'\n",
    "    #s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)  # 非字母以及非.!?的其他任何字符 一律被替换成空格\n",
    "    s = re.sub(r'[\\s]+', \" \", s)  # 将出现的多个空格，都使用一个空格代替。例如：w='abc  1   23  1' 处理后：w='abc 1 23 1'\n",
    "    return s\n",
    "\n",
    "\n",
    "# print(normalizeString('Va !'))\n",
    "# print(normalizeString('Go.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairs num= 500000\n"
     ]
    }
   ],
   "source": [
    "pairs = [[normalizeString(s) for s in line] for line in data_df.values]\n",
    "\n",
    "print('pairs num=', len(pairs))\n",
    "# print(pairs[0])\n",
    "# print(pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "经过过滤后平行语料数目为： 498921\n"
     ]
    }
   ],
   "source": [
    "# 文件是英译法，我们实现的是法译英，所以进行了reverse，所以pair[1]是英语\n",
    "# 为了快速训练，仅保留“我是”“你是”“他是”等简单句子，并且删除原始文本长度大于10个标记的样本\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH \n",
    "\n",
    "def filterPairs(pairs):\n",
    "    # 过滤，并交换句子顺序，得到法英句子对（之前是英法句子对）\n",
    "    return [[pair[1], pair[0]] for pair in pairs if filterPair(pair)]\n",
    "\n",
    "\n",
    "pairs = filterPairs(pairs)\n",
    "\n",
    "print('经过过滤后平行语料数目为：', len(pairs))\n",
    "# print(pairs[0])\n",
    "# print(random.choice(pairs))\n",
    "# print(np.array(pairs).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集句子数目： 479163\n",
      "验证集句子数目： 9979\n",
      "测试集句子数目： 9779\n"
     ]
    }
   ],
   "source": [
    "# 划分数据集：训练集和验证集\n",
    "##0.0338 0.03485\n",
    "##50 0.020 0.020\n",
    "train_test, val_pairs = train_test_split(pairs, test_size=0.020, random_state=1234)\n",
    "train_pairs, test_pairs = train_test_split(train_test, test_size=0.020, random_state=1234)\n",
    "\n",
    "print('训练集句子数目：', len(train_pairs))\n",
    "print('验证集句子数目：', len(val_pairs))\n",
    "print('测试集句子数目：', len(test_pairs))\n",
    "# print(test_pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(r'/home/chengkun/jupyter_projects/Magic-NLPer-main/data/data_sets/train_pairs','wb') as f:\n",
    "#     pickle.dump(train_pairs, f)\n",
    "#     f.close()\n",
    "\n",
    "# with open(r'/home/chengkun/jupyter_projects/Magic-NLPer-main/data/data_sets/val_pairs','wb') as f:\n",
    "#     pickle.dump(val_pairs, f)\n",
    "#     f.close()\n",
    "\n",
    "# with open(r'/home/chengkun/jupyter_projects/Magic-NLPer-main/data/data_sets/test_pairs','wb') as f:\n",
    "#     pickle.dump(test_pairs, f)\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(r'/home/chengkun/jupyter_projects/Magic-NLPer-main/data/data_sets/train_pairs','rb') as f:\n",
    "#     train_pairs = pickle.load(f)\n",
    "#     f.close()\n",
    "\n",
    "# with open(r'/home/chengkun/jupyter_projects/Magic-NLPer-main/data/data_sets/val_pairs','rb') as f:\n",
    "#     val_pairs = pickle.load(f)\n",
    "#     f.close()\n",
    "\n",
    "# with open(r'/home/chengkun/jupyter_projects/Magic-NLPer-main/data/data_sets/test_pairs','rb') as f:\n",
    "#     test_pairs = pickle.load(f)\n",
    "#     f.close()\n",
    "\n",
    "# print('训练集句子数目：', len(train_pairs))\n",
    "# print('验证集句子数目：', len(val_pairs))\n",
    "# print('测试集句子数目：', len(test_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = lambda x: x.split() # 分词器\n",
    "\n",
    "SRC_TEXT = torchtext.data.Field(sequential=True,\n",
    "                                tokenize=tokenizer,\n",
    "                                # lower=True,\n",
    "                                fix_length=MAX_LENGTH + 2,\n",
    "                                preprocessing=lambda x: ['<start>'] + x + ['<end>'],\n",
    "                                # after tokenizing but before numericalizing\n",
    "                                # postprocessing # after numericalizing but before the numbers are turned into a Tensor\n",
    "                                )\n",
    "TARG_TEXT = torchtext.data.Field(sequential=True,\n",
    "                                 tokenize=tokenizer,\n",
    "                                 # lower=True,\n",
    "                                 fix_length=MAX_LENGTH + 2,\n",
    "                                 preprocessing=lambda x: ['<start>'] + x + ['<end>'],\n",
    "                                 )\n",
    "\n",
    "\n",
    "def get_dataset(pairs, src, targ):\n",
    "    fields = [('src', src), ('targ', targ)]  # filed信息 fields dict[str, Field])\n",
    "    examples = []  # list(Example)\n",
    "    for mn, ch in pairs: # 进度条\n",
    "        # 创建Example时会调用field.preprocess方法\n",
    "        examples.append(torchtext.data.Example.fromlist([mn, ch], fields))\n",
    "    return examples, fields\n",
    "\n",
    "\n",
    "# examples, fields = get_dataset(pairs, SRC_TEXT, TARG_TEXT)\n",
    "\n",
    "ds_train = torchtext.data.Dataset(*get_dataset(train_pairs, SRC_TEXT, TARG_TEXT))\n",
    "ds_val = torchtext.data.Dataset(*get_dataset(val_pairs, SRC_TEXT, TARG_TEXT))\n",
    "ds_test = torchtext.data.Dataset(*get_dataset(test_pairs, SRC_TEXT, TARG_TEXT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds_train\n",
      "20 ['<start>', 'ᠴᠢᠯᠠᠭᠤᠨ', 'ᠲᠣᠰᠣ', 'ᠭᠠᠳᠠᠭᠱᠢᠯᠠᠭᠤᠯᠬᠤ', 'ᠤᠯᠤᠰ', 'ᠤ᠋ᠨ', 'ᠵᠣᠬᠢᠶᠠᠨ', 'ᠪᠠᠢᠭᠤᠯᠤᠯᠲᠠ', 'ᠴᠢᠯᠠᠭᠤᠨ', 'ᠲᠣᠰᠣ', 'ᠭᠠᠳᠠᠭᠱᠢᠯᠠᠭᠤᠯᠬᠤ', 'ᠪᠠᠨ', 'ᠵᠣᠭᠰᠣᠭᠠᠨ', '.', 'ᠦᠨ\\u180eᠡ', 'ᠥᠰᠬᠦ', 'ᠶ᠋ᠢ', 'ᠬᠦᠯᠢᠶᠡᠨ\\u180eᠡ', '.', '<end>']\n",
      "22 ['<start>', '石', '油', '输', '出', '国', '家', '组', '织', '冻', '结', '石', '油', '输', '出', '.', '价', '格', '看', '涨', '.', '<end>']\n",
      "ds_val\n",
      "25 ['<start>', 'ᠴᠠᠭᠠᠨ', 'ᠰᠠᠷ\\u180eᠠ', 'ᠶ᠋ᠢᠨ', 'ᠦᠶ\\u180eᠡ', 'ᠪᠡᠷ', 'ᠬᠥᠮᠦᠰ', 'ᠬᠠᠭᠤᠴᠢᠨ', 'ᠶᠣᠰᠣ', 'ᠵᠠᠩᠱᠢᠯ', 'ᠢ᠋ᠶ᠋ᠠᠨ', 'ᠪᠠᠷᠢᠮᠲᠠᠯᠠᠵᠤ', 'ᠪᠠᠢᠭ\\u180eᠠ', 'ᠪᠥᠭᠡᠳ', 'ᠤᠯᠠᠮ', 'ᠢ᠋ᠶ᠋ᠠᠷ', 'ᠱᠢᠨ\\u180eᠡ', 'ᠵᠦᠢᠯ', 'ᠢ᠋', 'ᠡᠷᠢᠨ', 'ᠬᠠᠢᠬᠤ', 'ᠪᠣᠯᠵᠤ', 'ᠪᠠᠢᠨ\\u180eᠠ', '.', '<end>']\n",
      "35 ['<start>', '人', '们', '在', '固', '守', '原', '有', '过', '年', '消', '费', '方', '式', '的', '同', '时', '.', '也', '在', '欣', '赏', '.', '渴', '望', '着', '浪', '漫', '的', '时', '尚', '情', '趣', '.', '<end>']\n",
      "ds_test\n",
      "13 ['<start>', 'ᠵᠠᠷᠯᠠᠨ', 'ᠨᠡᠢᠲᠡᠯᠡᠬᠦ', 'ᠬᠤᠭᠤᠴᠠᠭ\\u180eᠠ', 'ᠨᠢ', 'ᠬᠣᠷᠢᠨ', 'ᠡᠳᠦᠷ', 'ᠡᠴᠡ', 'ᠳᠤᠲᠠᠭᠤ', 'ᠪᠠᠢᠵᠤ', 'ᠪᠣᠯᠬᠤ', 'ᠦᠭᠡᠢ', '<end>']\n",
      "13 ['<start>', '公', '示', '时', '间', '不', '得', '少', '于', '二', '十', '日', '<end>']\n"
     ]
    }
   ],
   "source": [
    "# # 查看1个样本的信息\n",
    "print('ds_train')\n",
    "print(len(ds_train[0].src), ds_train[0].src)\n",
    "print(len(ds_train[0].targ), ds_train[0].targ)\n",
    "print('ds_val')\n",
    "print(len(ds_val[0].src), ds_val[0].src)\n",
    "print(len(ds_val[0].targ), ds_val[0].targ)\n",
    "print('ds_test')\n",
    "print(len(ds_test[0].src), ds_test[0].src)\n",
    "print(len(ds_test[0].targ), ds_test[0].targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型大小与词表大小正相关，控制词表大小\n",
      "0-20：<unk> <pad> . <end> <start> ᠶ᠋ᠢᠨ ᠤ᠋ᠨ ᠤ᠋ ᠦ᠋ᠨ ᠶ᠋ᠢ ᠢ᠋ ᠳ᠋ᠤ᠌ ᠦ᠌ ᠨᠢ ᠦᠭᠡᠢ ᠳ᠋ᠦ᠍ ᠠ᠋ᠴᠠ ᠤᠯᠤᠰ ᠬᠡᠷᠡᠭᠲᠡᠢ ᠭᠠᠵᠠᠷ\n",
      "\n",
      "input_vocab_size： 110560\n",
      "target_vocab_size： 6085\n"
     ]
    }
   ],
   "source": [
    "# 构建词典\n",
    "print('模型大小与词表大小正相关，控制词表大小')\n",
    "SRC_TEXT.build_vocab(ds_train,min_freq=1)  # 建立词表 并建立token和ID的映射关系\n",
    "# print(len(SRC_TEXT.vocab))\n",
    "# print(SRC_TEXT.vocab.itos[0])\n",
    "# print(SRC_TEXT.vocab.itos[1])\n",
    "# print(SRC_TEXT.vocab.itos[2])\n",
    "# print(SRC_TEXT.vocab.itos[3])\n",
    "# print(SRC_TEXT.vocab.stoi['<start>'])\n",
    "# print(SRC_TEXT.vocab.stoi['<end>'])\n",
    "\n",
    "# 模拟decode\n",
    "res = []\n",
    "for id in range(20):\n",
    "    res.append(SRC_TEXT.vocab.itos[id])\n",
    "print('0-20：'+' '.join(res)+'\\n')\n",
    "\n",
    "TARG_TEXT.build_vocab(ds_train,min_freq=1)\n",
    "\n",
    "# print(len(TARG_TEXT.vocab))\n",
    "# print(TARG_TEXT.vocab.itos[0])\n",
    "# print(TARG_TEXT.vocab.itos[1])\n",
    "# print(TARG_TEXT.vocab.itos[2])\n",
    "# print(TARG_TEXT.vocab.itos[3])\n",
    "# print(TARG_TEXT.vocab.stoi['<start>'])\n",
    "# print(TARG_TEXT.vocab.stoi['<end>'])\n",
    "\n",
    "input_vocab_size = len(SRC_TEXT.vocab)\n",
    "target_vocab_size = len(TARG_TEXT.vocab)\n",
    "\n",
    "print('input_vocab_size：', input_vocab_size)\n",
    "print('target_vocab_size：', target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = batch * ngpu\n",
    "\n",
    "# 构建数据管道迭代器\n",
    "train_iter, val_iter, test_iter= torchtext.data.Iterator.splits(\n",
    "    (ds_train, ds_val, ds_test),\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x.src),\n",
    "    batch_sizes=(BATCH_SIZE, BATCH_SIZE, BATCH_SIZE)\n",
    ")\n",
    "\n",
    "\n",
    "# # 查看数据管道信息，此时会触发postprocessing，如果有的话\n",
    "# for BATCH in train_iter:\n",
    "#     # 注意，这里text第0维不是batch，而是seq_len\n",
    "#     print(BATCH.src.shape, BATCH.targ.shape)  # [12,64], [12,64]\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 将数据管道组织成与torch.utils.data.DataLoader相似的inputs, targets的输出形式\n",
    "class DataLoader:\n",
    "    def __init__(self, data_iter):\n",
    "        self.data_iter = data_iter\n",
    "        self.length = len(data_iter)  # 一共有多少个batch？\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __iter__(self):\n",
    "        # 注意，在此处调整text的shape为batch first\n",
    "        for batch in self.data_iter:\n",
    "            yield (torch.transpose(batch.src, 0, 1), torch.transpose(batch.targ, 0, 1))\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_iter)\n",
    "val_dataloader = DataLoader(val_iter)\n",
    "test_dataloader = DataLoader(test_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_dataloader): 1997\n",
      "len(val_dataloader): 42\n",
      "len(test_dataloader): 41\n"
     ]
    }
   ],
   "source": [
    "# 查看数据管道\n",
    "print('len(train_dataloader):', len(train_dataloader))  # 句子总数/batch数\n",
    "print('len(val_dataloader):', len(val_dataloader))  # 句子总数/batch数\n",
    "print('len(test_dataloader):', len(test_dataloader))  # 句子总数/batch数\n",
    "\n",
    "# for batch_src, batch_targ in train_dataloader:\n",
    "#     print('batch_src.shape:',batch_src.shape,'\\n','batch_targ.shape:',batch_targ.shape)  # [256,12], [256,12]\n",
    "#     print(batch_src, batch_src.dtype)\n",
    "#     print(batch_targ, batch_targ.dtype)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dep_train = torch.randn(len(train_pairs),MAX_LENGTH+2,MAX_LENGTH+2)\n",
    "# dep_val = torch.randn(len(val_pairs),MAX_LENGTH+2,MAX_LENGTH+2)\n",
    "# dep_test = torch.randn(len(test_pairs),MAX_LENGTH+2,MAX_LENGTH+2)\n",
    "\n",
    "# for step, (inp, targ) in enumerate(train_dataloader, start=1):\n",
    "#     print(step)\n",
    "#     st = (step-1) * inp.shape[0]\n",
    "#     end = st + inp.shape[0]\n",
    "#     dependency_matrix = dep_train[st:end,:,:]\n",
    "#     print(inp.shape)\n",
    "#     print(targ.shape)\n",
    "#     print(dependency_matrix.shape)\n",
    "    \n",
    "# for step, (inp, targ) in enumerate(test_dataloader, start=1):\n",
    "#     print(step)\n",
    "#     st = (step-1) * inp.shape[0]\n",
    "#     end = st + inp.shape[0]\n",
    "#     dependency_matrix = dep_test[st:end,:,:]\n",
    "#     print(inp.shape)\n",
    "#     print(targ.shape)\n",
    "#     print(dependency_matrix.shape)\n",
    "\n",
    "# for step, (inp, targ) in enumerate(val_dataloader, start=1):\n",
    "#     print(step)\n",
    "#     st = (step-1) * inp.shape[0]\n",
    "#     end = st + inp.shape[0]\n",
    "#     dependency_matrix = dep_val[st:end,:,:]\n",
    "#     print(inp.shape)\n",
    "#     print(targ.shape)\n",
    "#     print(dependency_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.位置编码 positional encoding\n",
    "绝对位置编码\n",
    "\n",
    "由于model中不含有任何recurrence or convolution，所以句子中token的相对位置关系无法体现，\n",
    "所以就需要在embedding vector中加入position encoding vector（维度相同）。这样每个词的词向量\n",
    "在 $d_{model}$ 维的空间中，就可以基于meaning和position来计算相似度或相关性\n",
    "\n",
    "$$\\begin{array}{ll} & PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}}) \\\\ & PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})\\end{array}$$\n",
    "\n",
    "**特点：**\n",
    "- （1）后面位置是前面位置的线性组合，保证了即使位置不是相邻的，也可能有关系（[参考这里](#https://blog.csdn.net/zhulinniao/article/details/104462228/)）\n",
    "- （2）每个位置的编码又是独特的\n",
    "- （3）每两个位置的encoding互相做点积，位置越远，点积的值越小，自己和自己点积，值最大\n",
    "\n",
    "![jupyter-img1](./imgs/im1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 计算角度：pos * 1/(10000^(2i/d))\n",
    "def get_angles(pos, i, d_model):\n",
    "    # 2*(i//2)保证了2i，这部分计算的是1/10000^(2i/d)\n",
    "    angle_rates = 1 / np.power(10000, 2 * (i // 2) / np.float32(d_model))  # => [1, 512]\n",
    "    return pos * angle_rates  # [50,1]*[1,512]=>[50, 512]\n",
    "\n",
    "\n",
    "# np.arange()函数返回一个有终点和起点的固定步长的排列，如[1,2,3,4,5]，起点是1，终点是5，步长为1\n",
    "# 注意：起点终点是左开右闭区间，即start=1,end=6，才会产生[1,2,3,4,5]\n",
    "# 只有一个参数时，参数值为终点，起点取默认值0，步长取默认值1。\n",
    "def positional_encoding(position, d_model):  # d_model是位置编码的长度，相当于position encoding的embedding_dim？\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],  # [50, 1]\n",
    "                            np.arange(d_model)[np.newaxis, :],  # [1, d_model=512]\n",
    "                            d_model)\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])  # 2i\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])  # 2i+2\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]  # [50,512]=>[1,50,512]\n",
    "    return torch.tensor(pos_encoding, dtype=torch.float32)\n",
    "\n",
    "pos_encoding = positional_encoding(50, 512)\n",
    "# print(pos_encoding.shape) # [1,50,512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def draw_pos_encoding(pos_encoding):\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(pos_encoding[0], cmap='RdBu') # 绘制分类图\n",
    "    plt.xlabel('Depth')\n",
    "    plt.xlim((0, 512))\n",
    "    plt.ylabel('Position')\n",
    "    plt.colorbar() # 条形bar颜色图例\n",
    "    plt.savefig(result_save+'pos_encoding.png')\n",
    "    #plt.show()\n",
    "\n",
    "draw_pos_encoding(pos_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.掩码 masking\n",
    "这里用到的mask有2种：\n",
    "- padding mask：mask pad，即句子中为pad(value=0)的位置处其mask值为1\n",
    "- look-ahead mask：mask future token，将当前token后面的词mask掉，只让看到前面的词，即future token位置的mask值为1\n",
    "\n",
    "**【注意】：** 因为我这里使用的是torchtext里的tokenizer,从前面可以看出它的词表里pad的index=1，而不是常规的0。\n",
    "这里要特别注意，不然容易出错！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "mask 表示掩码，它对某些值进行掩盖，使其在参数更新时不产生效果。\n",
    "Transformer 模型里面涉及两种 mask，分别是 padding mask 和 sequence mask。\n",
    "其中，padding mask 在所有的 scaled dot-product attention 里面都需要用到，\n",
    "而 sequence mask 只有在 decoder 的 self-attention 里面用到。\n",
    "'''\n",
    "# 需要mask的位置，替换为1，seq：sentence_len x d_model \n",
    "#返回值为：sentence_len x 1 x 1 x d_model 为什么要扩展维度\n",
    "\n",
    "pad = 1 # 重要！\n",
    "def create_padding_mask(seq):  # seq [b, seq_len]\n",
    "    # seq = torch.eq(seq, torch.tensor(0)).float() # pad=0的情况\n",
    "    seq = torch.eq(seq, torch.tensor(pad)).float()  # pad!=0\n",
    "    return seq[:, np.newaxis, np.newaxis, :]  # =>[b, 1, 1, seq_len]\n",
    "\n",
    "# x = torch.tensor([[7, 6, 0, 0, 1],\n",
    "#                   [1, 2, 3, 0, 0],\n",
    "#                   [0, 0, 0, 4, 5]])\n",
    "# print(x.shape) # [3,5]\n",
    "# print(x)\n",
    "# mask = create_padding_mask(x)\n",
    "# print(mask.shape, mask.dtype) # [3,1,1,5]\n",
    "# print(mask)\n",
    "\n",
    "# 用train_dataloader的第一个BATCH_SIZE中的input来测试\n",
    "# for step, (inp, targ) in enumerate(train_dataloader, start=1):\n",
    "#     print(inp.shape,'\\t',targ.shape)\n",
    "#     print(inp,'\\n',targ)\n",
    "#     break\n",
    "\n",
    "# input_mask = create_padding_mask(inp)\n",
    "# targ_mask = create_padding_mask(targ)\n",
    "# print(input_mask,input_mask.shape)\n",
    "# print(targ_mask,targ_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = []\n",
    "# b = []\n",
    "# for step, (inp, targ) in enumerate(test_dataloader):\n",
    "#     a.append(inp.shape[1])\n",
    "#     b.append(targ.shape[1])\n",
    "# #     print(inp,'\\n',inp.shape,targ.shape)\n",
    "# #     break\n",
    "# print(a)\n",
    "# print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pad = 1 # 重要！\n",
    "# def mn_create_padding_mask(seq):  # seq [b, seq_len]\n",
    "#     if seq.shape[1] < 10:\n",
    "#         d = torch.ones([seq.shape[0],10-seq.shape[1]])\n",
    "#         seq = torch.cat((seq, d), dim=1)\n",
    "#     print(seq)\n",
    "#     # seq = torch.eq(seq, torch.tensor(0)).float() # pad=0的情况\n",
    "#     seq = torch.eq(seq, torch.tensor(pad)).float()  # pad!=0\n",
    "#     return seq[:, np.newaxis, np.newaxis, :]  # =>[b, 1, 1, seq_len]\n",
    "\n",
    "# x = torch.tensor([[7, 6, 0, 0, 1],\n",
    "#                   [1, 2, 3, 0, 0],\n",
    "#                   [0, 0, 0, 4, 5]])\n",
    "# print(x.shape) # [3,5]\n",
    "# print(x)\n",
    "# mask = mn_create_padding_mask(x)\n",
    "# print(mask.shape, mask.dtype) # [3,1,1,5]\n",
    "# print(mask)\n",
    "\n",
    "# # create two sample vectors\n",
    "# # inps = torch.randn([64, 161])\n",
    "# d = torch.ones([3,4])\n",
    "\n",
    "# # bring d into the same format, and then concatenate tensors\n",
    "# # new_inps = torch.cat((x, d.unsqueeze(2)), dim=-1)\n",
    "# new_inps = torch.cat((x,d),dim=1)\n",
    "# print(new_inps.shape)  # [64, 161, 2]\n",
    "# print(new_inps)\n",
    "# print(x.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# torch.triu(tensor, diagonal=0) 求上三角矩阵，diagonal默认为0表示主对角线的上三角矩阵\n",
    "# diagonal>0，则主对角上面的第|diagonal|条次对角线的上三角矩阵\n",
    "# diagonal<0，则主对角下面的第|diagonal|条次对角线的上三角矩阵\n",
    "#look-ahead_mask 用于对未预测的token进行掩码，这意味着要预测第三个单词，只会使用第一个和第二个单词。 \n",
    "\n",
    "def create_look_ahead_mask(size):  # seq_len\n",
    "    mask = torch.triu(torch.ones((size, size)), diagonal=1)\n",
    "    # mask = mask.device() #\n",
    "    return mask  # [seq_len, seq_len]\n",
    "\n",
    "# x = torch.rand(1,3)\n",
    "# print(x.shape)\n",
    "# print(x)\n",
    "# mask = create_look_ahead_mask(x.shape[1])\n",
    "# print(mask.shape, mask.dtype)\n",
    "# print(mask)\n",
    "\n",
    "# 用train_dataloader的第一个BATCH_SIZE中的target来测试\n",
    "# for step, (inp, targ) in enumerate(train_dataloader, start=1):\n",
    "#     print(targ,'\\n',inp.shape,targ.shape)\n",
    "#     break\n",
    "\n",
    "# look_ahead_mask = create_look_ahead_mask(targ.shape[-1])\n",
    "# print(look_ahead_mask,look_ahead_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4.scaled dot product attention\n",
    "![jupyter-img2](./imgs/im2.jpg)\n",
    "\n",
    "$$Attention(Q,K,V)=softmax_{(k)}(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "注意：实现时对mask的处理\n",
    "\n",
    "mask=1的位置是pad或者future token，乘以-1e9（-1*10^9）成为负无穷，经过softmax后会趋于0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    \"\"\"\n",
    "    #计算注意力权重。\n",
    "    q, k, v 必须具有匹配的前置维度。 且dq=dk\n",
    "    k, v 必须有匹配的倒数第二个维度，例如：seq_len_k = seq_len_v。\n",
    "    #虽然 mask 根据其类型（填充或前瞻）有不同的形状，\n",
    "    #但是 mask 必须能进行广播转换以便求和。\n",
    "\n",
    "    #参数:\n",
    "        q: 请求的形状 == (..., seq_len_q, depth)\n",
    "        k: 主键的形状 == (..., seq_len_k, depth)\n",
    "        v: 数值的形状 == (..., seq_len_v, depth_v)  seq_len_k = seq_len_v\n",
    "        mask: Float 张量，其形状能转换成\n",
    "              (..., seq_len_q, seq_len_k)。默认为None。\n",
    "    \n",
    "    # self-attention中q=k=v这点和attention不同，需要先明确\n",
    "    #q和k相似度计算是为了获取到最合适的值，也就是值的给与注意力的值\n",
    "    #softmax是为了获取这一系列相似度值的占比（这也就是所谓的权重值）\n",
    "    #加权是和v也就是本身进行加权，求和是为了获取粒度单词和完整句子之间的关系值计算\n",
    "    #那么自然而然的，所谓的qkv这一系列操作的目的就是\n",
    "    #先通过对本身的各个向量值进行相似度计算，然后通过softmax获取本身向量的权重值，在和本身进行加权计算，\n",
    "    #最后在求和，这样子就获取了一个词和本身所有的词的权重值，然后将所有词的权重值作为输入向量，这也就是所谓的自注意力机制。\n",
    "\n",
    "    #返回值:\n",
    "        #输出，注意力权重\n",
    "    \"\"\"\n",
    "    # matmul(a,b)矩阵乘:a b的最后2个维度要能做乘法，即a的最后一个维度值==b的倒数第2个纬度值，\n",
    "    # 除此之外，其他维度值必须相等或为1（为1时会广播）\n",
    "    matmul_qk = torch.matmul(q, k.transpose(-1, -2))  # 矩阵乘 =>[..., seq_len_q, seq_len_k]\n",
    "\n",
    "    # 缩放matmul_qk\n",
    "    dk = torch.tensor(k.shape[-1], dtype=torch.float32)  # k的深度dk，或叫做depth_k\n",
    "    scaled_attention_logits = matmul_qk / torch.sqrt(dk)  # [..., seq_len_q, seq_len_k]\n",
    "\n",
    "    # 将 mask 加入到缩放的张量上(重要！)\n",
    "    if mask is not None:  # mask: [b, 1, 1, seq_len]\n",
    "        # mask=1的位置是pad，乘以-1e9（-1*10^9）成为负无穷，经过softmax后会趋于0\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax 在最后一个轴（seq_len_k）上归一化\n",
    "    attention_weights = torch.nn.functional.softmax(scaled_attention_logits, dim=-1)  # [..., seq_len_q, seq_len_k]\n",
    "\n",
    "    output = torch.matmul(attention_weights, v)  # =>[..., seq_len_q, depth_v]\n",
    "    return output, attention_weights  # [..., seq_len_q, depth_v], [..., seq_len_q, seq_len_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mn_scaled_dot_product_attention(q, k, v, m, mask=None):\n",
    "    \"\"\"\n",
    "    #计算注意力权重。\n",
    "    q, k, v 必须具有匹配的前置维度。 且dq=dk\n",
    "    k, v 必须有匹配的倒数第二个维度，例如：seq_len_k = seq_len_v。\n",
    "    #虽然 mask 根据其类型（填充或前瞻）有不同的形状，\n",
    "    #但是 mask 必须能进行广播转换以便求和。\n",
    "\n",
    "    #参数:\n",
    "        q: 请求的形状 == (..., seq_len_q, depth)\n",
    "        k: 主键的形状 == (..., seq_len_k, depth)\n",
    "        v: 数值的形状 == (..., seq_len_v, depth_v)  seq_len_k = seq_len_v\n",
    "        m: 数值的形状 == (..., seq_len_k, seq_len_v) m必须是个方阵\n",
    "        mask: Float 张量，其形状能转换成\n",
    "              (..., seq_len_q, seq_len_k)。默认为None。\n",
    "\n",
    "    #返回值:\n",
    "        #输出，注意力权重\n",
    "    \"\"\"\n",
    "    # matmul(a,b)矩阵乘:a b的最后2个维度要能做乘法，即a的最后一个维度值==b的倒数第2个纬度值，\n",
    "    # 除此之外，其他维度值必须相等或为1（为1时会广播）\n",
    "    matmul_qk = torch.matmul(q, k.transpose(-1, -2))  # 矩阵乘 =>[..., seq_len_q, seq_len_k]\n",
    "\n",
    "    # 缩放matmul_qk\n",
    "    dk = torch.tensor(k.shape[-1], dtype=torch.float32)  # k的深度dk，或叫做depth_k\n",
    "    scaled_attention_logits = matmul_qk / torch.sqrt(dk) # [..., seq_len_q, seq_len_k]\n",
    "#     print('scaled_attention_logits.shape',scaled_attention_logits.shape)\n",
    "#     print('m.shape',m.shape)\n",
    "#     print('q*k^t：', scaled_attention_logits.shape)\n",
    "    \n",
    "#     scaled_attention_logits = torch.matmul(scaled_attention_logits,m)\n",
    "    scaled_attention_logits = scaled_attention_logits * m\n",
    "#     print('q*k^t*m：', scaled_attention_logits.shape)\n",
    "    \n",
    "    # 将 mask 加入到缩放的张量上(重要！)\n",
    "    if mask is not None:  # mask: [b, 1, 1, seq_len]\n",
    "        # mask=1的位置是pad，乘以-1e9（-1*10^9）成为负无穷，经过softmax后会趋于0\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax 在最后一个轴（seq_len_k）上归一化\n",
    "    attention_weights = torch.nn.functional.softmax(scaled_attention_logits, dim=-1)  # [..., seq_len_q, seq_len_k]\n",
    "    \n",
    "    output = torch.matmul(attention_weights, v)  # =>[..., seq_len_q, depth_v]\n",
    "#     print('output：', output.shape)\n",
    "    return output, attention_weights  # [..., seq_len_q, depth_v], [..., seq_len_q, seq_len_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dependency_parsing_scaled_dot_product_attention(q, k, v, dependency_parsing_matrix, mask=None):\n",
    "    \"\"\"\n",
    "    #计算注意力权重。\n",
    "    q, k, v 必须具有匹配的前置维度。 且dq=dk\n",
    "    k, v 必须有匹配的倒数第二个维度，例如：seq_len_k = seq_len_v。\n",
    "    #虽然 mask 根据其类型（填充或前瞻）有不同的形状，\n",
    "    #但是 mask 必须能进行广播转换以便求和。\n",
    "\n",
    "    #参数:\n",
    "        q: 请求的形状 == (..., seq_len_q, depth)\n",
    "        k: 主键的形状 == (..., seq_len_k, depth)\n",
    "        v: 数值的形状 == (..., seq_len_v, depth_v)  seq_len_k = seq_len_v\n",
    "        mask: Float 张量，其形状能转换成\n",
    "              (..., seq_len_q, seq_len_k)。默认为None。\n",
    "    \n",
    "    # self-attention中q=k=v这点和attention不同，需要先明确\n",
    "    #q和k相似度计算是为了获取到最合适的值，也就是值的给与注意力的值\n",
    "    #softmax是为了获取这一系列相似度值的占比（这也就是所谓的权重值）\n",
    "    #加权是和v也就是本身进行加权，求和是为了获取粒度单词和完整句子之间的关系值计算\n",
    "    #那么自然而然的，所谓的qkv这一系列操作的目的就是\n",
    "    #先通过对本身的各个向量值进行相似度计算，然后通过softmax获取本身向量的权重值，在和本身进行加权计算，\n",
    "    #最后在求和，这样子就获取了一个词和本身所有的词的权重值，然后将所有词的权重值作为输入向量，这也就是所谓的自注意力机制。\n",
    "\n",
    "    #返回值:\n",
    "        #输出，注意力权重\n",
    "    \"\"\"\n",
    "    # matmul(a,b)矩阵乘:a b的最后2个维度要能做乘法，即a的最后一个维度值==b的倒数第2个纬度值，\n",
    "    # 除此之外，其他维度值必须相等或为1（为1时会广播）\n",
    "    matmul_qk = torch.matmul(q, k.transpose(-1, -2))  # 矩阵乘 =>[..., seq_len_q, seq_len_k]\n",
    "#     print('matmul_qk:', matmul_qk.size())\n",
    "#     print('dependency_parsing_matrix:', dependency_parsing_matrix.size())\n",
    "    matmul_qk = matmul_qk + dependency_parsing_matrix\n",
    "\n",
    "    # 缩放matmul_qk\n",
    "    dk = torch.tensor(k.shape[-1], dtype=torch.float32)  # k的深度dk，或叫做depth_k\n",
    "    scaled_attention_logits = matmul_qk / torch.sqrt(dk)  # [..., seq_len_q, seq_len_k]\n",
    "#     print('scaled_attention_logits.shape:', scaled_attention_logits.shape)\n",
    "    # 将 mask 加入到缩放的张量上(重要！)\n",
    "    if mask is not None:  # mask: [b, 1, 1, seq_len]\n",
    "#         print('mask.shape', mask.shape)\n",
    "        mask = mask.reshape(-1,mask.shape[-1],mask.shape[-1])\n",
    "        # mask=1的位置是pad，乘以-1e9（-1*10^9）成为负无穷，经过softmax后会趋于0\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "    \n",
    "    # softmax 在最后一个轴（seq_len_k）上归一化\n",
    "    attention_weights = torch.nn.functional.softmax(scaled_attention_logits, dim=-1)  # [..., seq_len_q, seq_len_k]\n",
    "\n",
    "    output = torch.matmul(attention_weights, v)  # =>[..., seq_len_q, depth_v]\n",
    "#     print('output.shape',output.shape)\n",
    "    return output, attention_weights  # [..., seq_len_q, depth_v], [..., seq_len_q, seq_len_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原来的\n",
    "# class dependency_parsing_Attention(torch.nn.Module):\n",
    "#     def __init__(self, d_model):\n",
    "#         super(dependency_parsing_Attention, self).__init__()\n",
    "\n",
    "#         self.d_model = d_model\n",
    "\n",
    "#         self.wq = torch.nn.Linear(d_model, d_model)\n",
    "#         self.wk = torch.nn.Linear(d_model, d_model)\n",
    "#         self.wv = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "#         self.final_linear = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "#     def forward(self, q, k, v,dependency_parsing_matrix,mask):  # q=k=v=x [b, seq_len, embedding_dim] embedding_dim其实也=d_model\n",
    "#         batch_size = q.shape[0]\n",
    "\n",
    "#         q = self.wq(q)  # =>[b, seq_len, d_model]\n",
    "#         k = self.wk(k)  # =>[b, seq_len, d_model]\n",
    "#         v = self.wv(v)  # =>[b, seq_len, d_model]\n",
    "# #         print('q:',q.shape)\n",
    "# #         print('k:',q.shape)\n",
    "# #         print('v:',q.shape)\n",
    "\n",
    "#         scaled_attention, attention_weights = dependency_parsing_scaled_dot_product_attention(q, k, v, dependency_parsing_matrix, mask)\n",
    "#         # => [b, seq_len_q, d_model=512], [b, seq_len_q, seq_len_k]\n",
    "\n",
    "#         output = self.final_linear(scaled_attention)  # =>[b, seq_len_q, d_model=512]\n",
    "#         return output, attention_weights  # [b, seq_len_q, d_model=512], [b, seq_len_q, seq_len_k]\n",
    "\n",
    "\n",
    "# # x = torch.rand(2, 5, 6) # [b,seq_len,d_model,embedding_dim]\n",
    "# # dependency_parsing_matrix = torch.rand(2,5,5)\n",
    "\n",
    "# # temp_mha = dependency_parsing_Attention(d_model=6)\n",
    "\n",
    "# # out, attn_weights = temp_mha(x, x, x, dependency_parsing_matrix, mask=None)\n",
    "# # print(out.shape, attn_weights.shape) # [1, 60, 512], [1, 8, 60, 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加入premer-ez的\n",
    "class dependency_parsing_Attention(torch.nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(dependency_parsing_Attention, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.wq = torch.nn.Linear(d_model, d_model)\n",
    "        self.wk = torch.nn.Linear(d_model, d_model)\n",
    "        self.wv = torch.nn.Linear(d_model, d_model)\n",
    "        self.SDWC = SpatialDepthWiseConvolution(d_k=d_model,kernel_size=3)\n",
    "        #primer_ez\n",
    "        self.final_linear = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v,dependency_parsing_matrix,mask):  # q=k=v=x [b, seq_len, embedding_dim] embedding_dim其实也=d_model\n",
    "        batch_size = q.shape[0]\n",
    "        \n",
    "        q = self.wq(q)  # =>[b, seq_len, d_model]\n",
    "#         print('q',q.shape)\n",
    "        q = self.SDWC(q)\n",
    "        k = self.wk(k)  # =>[b, seq_len, d_model]\n",
    "        k = self.SDWC(k)\n",
    "        v = self.wv(v)  # =>[b, seq_len, d_model]\n",
    "        v = self.SDWC(v)\n",
    "#         print('q:',q.shape)\n",
    "#         print('k:',q.shape)\n",
    "#         print('v:',q.shape)\n",
    "\n",
    "\n",
    "        scaled_attention, attention_weights = dependency_parsing_scaled_dot_product_attention(q, k, v, dependency_parsing_matrix, mask)\n",
    "        # => [b, seq_len_q, d_model=512], [b, seq_len_q, seq_len_k]\n",
    "\n",
    "        output = self.final_linear(scaled_attention)  # =>[b, seq_len_q, d_model=512]\n",
    "        return output, attention_weights  # [b, seq_len_q, d_model=512], [b, seq_len_q, seq_len_k]\n",
    "\n",
    "# b = 120\n",
    "# s = 100\n",
    "# d = 256\n",
    "# x = torch.rand(b, s, d) # [b,seq_len,d_model]\n",
    "# dependency_parsing_matrix = torch.rand(b,s,s)\n",
    "# temp_mha = dependency_parsing_Attention(d_model=d)\n",
    "# out, attn_weights = temp_mha(x, x, x, dependency_parsing_matrix, mask=None)\n",
    "# print(out.shape, attn_weights.shape) # [1, 60, 512], [1, 8, 60, 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def print_out(q, k, v, m):\n",
    "#     temp_out, temp_attn = mn_scaled_dot_product_attention(q, k, v, m, None)\n",
    "#     print('Attention weights are:')\n",
    "#     print(temp_attn)\n",
    "#     print('Output is:')\n",
    "#     print(temp_out)\n",
    "\n",
    "# np.set_printoptions(suppress=True) # 设置不以科学计数法的形式显示数据\n",
    "\n",
    "# temp_k = torch.tensor([[10,0,0],\n",
    "#                        [0,10,0],\n",
    "#                        [0,0,10],\n",
    "#                        [0,0,10]], dtype=torch.float32) # [4,3]\n",
    "# temp_v = torch.tensor([[1,0],\n",
    "#                        [10,0],\n",
    "#                        [100,5],\n",
    "#                        [1000,6]], dtype=torch.float32) #[4,2]\n",
    "\n",
    "# temp_m = torch.tensor([[10,0,0,0],\n",
    "#                        [0,10,0,0],\n",
    "#                        [0,0,10,0],\n",
    "#                        [0,0,10,0]], dtype=torch.float32) # [4,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # query aligns with第2个key (key的第2列)，得到attention weights：[0. 1. 0. 0.]\n",
    "# # 所以第2行 的value值被返回\n",
    "# temp_q = torch.tensor([[0,10,0]], dtype=torch.float32) # [1,3]\n",
    "# print_out(temp_q, temp_k, temp_v,temp_m)\n",
    "# # Attention weights：[0. 1. 0. 0.]， Output：[[10.  0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # query aligns with 重复的key (key的第3列和第4列)，得到attention weights：[0. 0. 0.5 0.5]\n",
    "# # 所以第3行的value值与第4行的value值平均化后，被返回 【(100+1000)/2=550, (5+6)/2=5.5】\n",
    "# temp_q = torch.tensor([[0,0,10]], dtype=torch.float32) # [1,3]\n",
    "# print_out(temp_q, temp_k, temp_v,temp_m)\n",
    "# # Attention weights：[0. 0. 0.5 0.5]， Output：[[550.  5.5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # query aligns with 第1个key (key的第1列)和第2个key (key的第2列)，得到attention weights：[0.5 0.5 0. 0.]\n",
    "# # 所以第1行的value值与第2行的value值平均化后，被返回 【(1+10)/2=5.5, (0.+0.)/2=0.】\n",
    "# temp_q = torch.tensor([[10,10,0]], dtype=torch.float32) # [1,3]\n",
    "# print_out(temp_q, temp_k, temp_v,temp_m)\n",
    "# # Attention weights：[0.5 0.5 0. 0.]， Output：[[5.5.  0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # 传入所有的query\n",
    "# temp_q = torch.tensor([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=torch.float32)  # (3, 3)\n",
    "# print_out(temp_q, temp_k, temp_v,temp_m)\n",
    "# \"\"\"\n",
    "# # Attention weights：tensor(\n",
    "# [[0.  0.  0.5 0.5]\n",
    "#  [0.  1.  0.  0. ]\n",
    "#  [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)，\n",
    "# # Output：tensor(\n",
    "# [[550.    5.5]\n",
    "#  [ 10.    0. ]\n",
    "#  [  5.5   0. ]], shape=(3, 2), dtype=float32)\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # relative_positions\n",
    "# def relative_positions(seq_len):\n",
    "#     result = []\n",
    "#     for i in range(seq_len):\n",
    "#         front = list(range(-i, 0))\n",
    "#         end = list(range(seq_len - i))\n",
    "#         result.append(front + end)\n",
    "#     return result\n",
    "# relative_positions(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import math\n",
    "# # import torch\n",
    "# # from torch import nn\n",
    "\n",
    "# class RelativeGlobalAttention(torch.nn.Module):\n",
    "#     def __init__(self, d_model, num_heads, max_len=100, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.d_model = d_model\n",
    "#         self.num_heads = num_heads\n",
    "        \n",
    "#         assert d_model % self.num_heads == 0\n",
    "#         self.d_head = d_model // self.num_heads\n",
    "# #         d_head, remainder = divmod(d_model, num_heads)\n",
    "# #         if remainder:\n",
    "# #             raise ValueError(\n",
    "# #                 \"incompatible `d_model` and `num_heads`\"\n",
    "# #             )\n",
    "\n",
    "#         self.max_len = max_len\n",
    "\n",
    "#         self.key = torch.nn.Linear(d_model, d_model)\n",
    "#         self.value = torch.nn.Linear(d_model, d_model)\n",
    "#         self.query = torch.nn.Linear(d_model, d_model)\n",
    "#         self.dropout = torch.nn.Dropout(dropout)\n",
    "#         self.Er = torch.nn.Parameter(torch.randn(max_len, self.d_head))\n",
    "        \n",
    "#         self.register_buffer(\n",
    "#             \"mask\", \n",
    "#             torch.tril(torch.randn(max_len, max_len))\n",
    "#             .unsqueeze(0).unsqueeze(0)\n",
    "#         )\n",
    "# #         print('self.mask.shape：', self.mask.shape)\n",
    "#         # self.mask.shape = (1, 1, max_len, max_len)\n",
    "\n",
    "#     def skew(self, QEr):\n",
    "#         # QEr.shape = (batch_size, num_heads, seq_len, seq_len)\n",
    "# #         print('QEr.shape:',QEr.shape)\n",
    "#         padded = torch.nn.functional.pad(QEr, (1, 0), 'constant', 1)\n",
    "# #         print('padded.shape:',padded.shape)\n",
    "# #         print(padded)\n",
    "#         # padded.shape = (batch_size, num_heads, seq_len, 1 + seq_len)\n",
    "#         batch_size, num_heads, num_rows, num_cols = padded.shape\n",
    "#         reshaped = padded.reshape(batch_size, num_heads, num_cols, num_rows)\n",
    "#         # reshaped.size = (batch_size, num_heads, 1 + seq_len, seq_len)\n",
    "#         Srel = reshaped[:, :, 1:, :]\n",
    "#         # Srel.shape = (batch_size, num_heads, seq_len, seq_len)\n",
    "#         return Srel\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         # x.shape == (batch_size, seq_len, d_model)\n",
    "#         batch_size, seq_len, _ = x.shape\n",
    "#         if seq_len > self.max_len:\n",
    "#             raise ValueError(\n",
    "#                 \"sequence length exceeds model capacity\"\n",
    "#             )\n",
    "        \n",
    "#         k_t = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
    "#         # k_t.shape = (batch_size, num_heads, d_head, seq_len)\n",
    "#         v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
    "#         q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
    "#         # shape = (batch_size, num_heads, seq_len, d_head)\n",
    "        \n",
    "#         start = self.max_len - seq_len\n",
    "#         Er_t = self.Er[start:, :].transpose(0, 1)\n",
    "#         print(Er_t)\n",
    "#         # Er_t.shape = (d_head, seq_len)\n",
    "#         QEr = torch.matmul(q, Er_t)\n",
    "#         # QEr.shape = (batch_size, num_heads, seq_len, seq_len)\n",
    "#         Srel = self.skew(QEr)\n",
    "#         # Srel.shape = (batch_size, num_heads, seq_len, seq_len)\n",
    "        \n",
    "#         QK_t = torch.matmul(q, k_t)\n",
    "#         # QK_t.shape = (batch_size, num_heads, seq_len, seq_len)\n",
    "#         # （Q*K^T+Srel）\n",
    "#         print('QK_t.shape：',QK_t.shape)\n",
    "#         print('Srel.shape：',Srel.shape)\n",
    "#         attn = (QK_t + Srel) / math.sqrt(q.size(-1))\n",
    "#         mask = self.mask[:, :, :seq_len, :seq_len]\n",
    "#         # mask.shape = (1, 1, seq_len, seq_len)\n",
    "#         attn = attn.masked_fill(mask == 1, float(\"-inf\"))\n",
    "#         # attn.shape = (batch_size, num_heads, seq_len, seq_len)\n",
    "#         attn = torch.nn.functional.softmax(attn, dim=-1)\n",
    "#         out = torch.matmul(attn, v)\n",
    "#         # out.shape = (batch_size, num_heads, seq_len, d_head)\n",
    "#         out = out.transpose(1, 2)\n",
    "#         # out.shape == (batch_size, seq_len, num_heads, d_head)\n",
    "#         out = out.reshape(batch_size, seq_len, -1)\n",
    "#         # out.shape == (batch_size, seq_len, d_model)\n",
    "#         return self.dropout(out)\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# test_in = torch.zeros(2, 5, 4) # batch_size, seq_len, d_model\n",
    "# l = RelativeGlobalAttention(d_model=4, num_heads=2,max_len=5)\n",
    "\n",
    "# l(test_in).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = 1\n",
    "# channal = 2\n",
    "# width = 5\n",
    "# height = 5\n",
    "# data = torch.randn(batch, channal, width, height) #　batch, channal, width, height\n",
    "# # 当pad只有两个参数时，如pad=(1,2)代表对最后一个维度改变——左边扩充一列，右边扩充2列\n",
    "# # 当pad有四个参数，代表对最后两个维度扩充，pad = (左边填充数， 右边填充数， 上边填充数， 下边填充数)\n",
    "# # 当pad有六个参数时，代表队最后三个维度扩充，\n",
    "# # pad = (左边填充数， 右边填充数， 上边填充数， 下边填充数， 前边填充数，后边填充数)\n",
    "# pad = (1,0)\n",
    "# data = torch.nn.functional.pad(data, pad, 'constant', 0)\n",
    "# print(data)\n",
    "# print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import torch\n",
    "# # import torch.nn as nn\n",
    "# # import math\n",
    "\n",
    "# # def d(tensor=None):\n",
    "# #     if tensor is None:\n",
    "# #         return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# #     return 'cuda' if tensor.is_cuda else 'cpu'\n",
    "\n",
    "# # class RelativeAttentionError(Exception):\n",
    "# #     pass\n",
    "\n",
    "# class RelativeMultiheadedAttention(torch.nn.Module):\n",
    "#     \"\"\"\n",
    "#     Narrow multiheaded attention. Each attention head inspects a \n",
    "#     fraction of the embedding space and expresses attention vectors for each sequence position as a weighted average of all (earlier) positions.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, d_model, heads=8, dropout=0.1, max_length=102, relative_pos=False):\n",
    "\n",
    "#         super().__init__()\n",
    "        \n",
    "# #         if d_model % heads != 0:\n",
    "# #             raise RelativeAttentionError(\"Number of heads does not divide model dimension\")\n",
    "            \n",
    "#         assert d_model % heads == 0\n",
    "#         self.d_model = d_model\n",
    "#         self.heads = heads\n",
    "#         s = d_model // heads\n",
    "        \n",
    "#         self.linears = torch.nn.ModuleList([torch.nn.Linear(s, s, bias=False) for i in range(3)])\n",
    "#         self.recombine_heads = torch.nn.Linear(heads * s, d_model)\n",
    "#         self.dropout = torch.nn.Dropout(p=dropout)\n",
    "#         self.max_length = max_length\n",
    "#         #relative positional embeddings\n",
    "#         self.relative_pos = relative_pos\n",
    "#         if relative_pos:\n",
    "#             self.Er = torch.randn([heads, self.max_length, s])\n",
    "#         else:\n",
    "#             self.Er = None\n",
    "\n",
    "#     def forward(self, x, mask):\n",
    "#         #batch size, sequence length, embedding dimension\n",
    "#         b, t, e = x.size()\n",
    "#         h = self.heads\n",
    "#         #each head inspects a fraction of the embedded space\n",
    "#         #head dimension\n",
    "#         s = e // h\n",
    "#         #start index of position embedding\n",
    "#         embedding_start = self.max_length - t\n",
    "#         x = x.view(b,t,h,s)\n",
    "#         queries, keys, values = [w(x).transpose(1,2)\n",
    "#                 for w, x in zip(self.linears, (x,x,x))]\n",
    "#         if self.relative_pos:\n",
    "#             #apply same position embeddings across the batch\n",
    "#             #Is it possible to apply positional self-attention over\n",
    "#             #only half of all relative distances?\n",
    "#             Er  = self.Er[:, embedding_start:, :].unsqueeze(0)\n",
    "#             print('Er:',Er.shape)\n",
    "#             QEr = torch.matmul(queries, Er.transpose(-1,-2))\n",
    "#             QEr = self.mask_positions(QEr)\n",
    "#             #Get relative position attention scores\n",
    "#             #combine batch with head dimension\n",
    "#             SRel = self.skew(QEr).contiguous().view(b*h, t, t)\n",
    "#         else:\n",
    "#             SRel = torch.zeros([b*h, t, t])\n",
    "#         print('SRel:',SRel.shape)\n",
    "#         queries, keys, values = map(lambda x: x.contiguous()\\\n",
    "#                 .view(b*h, t, s), (queries, keys, values))\n",
    "#         print('queries:',queries.shape)\n",
    "#         print('keys:',keys.shape)\n",
    "#         print('values:',values.shape)\n",
    "#         #Compute scaled dot-product self-attention\n",
    "#         #scale pre-matrix multiplication   \n",
    "#         queries = queries / (e ** (1/4))\n",
    "#         keys    = keys / (e ** (1/4))\n",
    "#         print('queries:',queries.shape)\n",
    "#         print('keys:',keys.shape)\n",
    "#         print('keys.transpose(1, 2):',keys.transpose(1, 2).shape)\n",
    "\n",
    "#         scores = torch.bmm(queries, keys.transpose(1, 2))\n",
    "#         print('scores:',scores.shape)\n",
    "#         scores = scores + SRel\n",
    "#         #(b*h, t, t)\n",
    "\n",
    "#         subsequent_mask = torch.triu(torch.ones(1, t, t),\n",
    "#                 1)\n",
    "#         scores = scores.masked_fill(subsequent_mask == 1, -1e9)\n",
    "#         if mask is not None:\n",
    "#             mask = mask.repeat_interleave(h, 0)\n",
    "#             wtf = (mask == 0).nonzero().transpose(0,1)\n",
    "#             scores[wtf[0], wtf[1], :] = -1e9\n",
    "\n",
    "        \n",
    "#         #Convert scores to probabilities\n",
    "#         attn_probs = torch.nn.functional.softmax(scores, dim=2)\n",
    "#         attn_probs = self.dropout(attn_probs)\n",
    "#         #use attention to get a weighted average of values\n",
    "#         out = torch.bmm(attn_probs, values).view(b, h, t, s)\n",
    "#         #transpose and recombine attention heads\n",
    "#         out = out.transpose(1, 2).contiguous().view(b, t, s * h)\n",
    "#         #last linear layer of weights\n",
    "#         return self.recombine_heads(out)\n",
    "\n",
    "\n",
    "#     def mask_positions(self, qe):\n",
    "#         #QEr is a matrix of queries (absolute position) dot distance embeddings (relative pos).\n",
    "#         #Mask out invalid relative positions: e.g. if sequence length is L, the query at\n",
    "#         #L-1 can only attend to distance r = 0 (no looking backward).\n",
    "#         L = qe.shape[-1]\n",
    "#         mask = torch.triu(torch.ones(L, L), 1).flip(1)\n",
    "#         return qe.masked_fill((mask == 1), 0)\n",
    "\n",
    "#     def skew(self, qe):\n",
    "#         #pad a column of zeros on the left\n",
    "#         padded_qe = torch.nn.functional.pad(qe, [1,0])\n",
    "#         s = padded_qe.shape\n",
    "#         padded_qe = padded_qe.view(s[0], s[1], s[3], s[2])\n",
    "#         #take out first (padded) row\n",
    "#         return padded_qe[:,:,1:,:]\n",
    "\n",
    "# test_in = torch.zeros(2, 5, 4) # batch_size, seq_len, d_model\n",
    "# # print(test_in)\n",
    "# test = RelativeMultiheadedAttention(d_model=4, heads=2, dropout=0.1, max_length=102, relative_pos=True)\n",
    "# result = test(test_in, mask = None)\n",
    "# print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.multi-head attention\n",
    "![jupyter-img3](./imgs/im3.jpg)\n",
    "\n",
    "多头注意允许模型在不同的位置联合处理来自不同表示子空间的信息。\n",
    "（Multi-head attention allows the model to jointly attend to information\n",
    "from different representation subspaces at different positions.）\n",
    "\n",
    "$$\\begin{array}{ll}  & MultiHead(Q,K,V)=Concat(head_1,...,head_h)W^O \\\\ & where\\; head_i=Attention(QW_i^Q,KW_i^K,VW_i^V)\\end{array}$$\n",
    "\n",
    "其中投影维参数矩阵$W_i^Q\\in R^{d_{model}\\text{x}\\,d_k}$，$W_i^K\\in R^{d_{model}\\text{x}\\,d_k}$， $W_i^V\\in R^{d_{model}\\text{x}\\,d_v}$，$W^O\\in R^{hd_v\\text{x}\\,d_{model}}$。\n",
    "\n",
    "$h=8,\\;d_k = d_v = d_{model}/h = 64$\n",
    "\n",
    "multi-head attention有4部分组成：\n",
    "- linear layer 将 linear layer的结果 split到不同的head\n",
    "- scaled dot-product attention\n",
    "- 将所有的head 进行拼接 concat\n",
    "- final linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0  # 因为输入要被（平均？）split到不同的head\n",
    "\n",
    "        self.depth = d_model // self.num_heads  # 512/8=64，所以在scaled dot-product atten中dq=dk=64,dv也是64\n",
    "\n",
    "        self.wq = torch.nn.Linear(d_model, d_model)\n",
    "        self.wk = torch.nn.Linear(d_model, d_model)\n",
    "        self.wv = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.final_linear = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):  # x [b, seq_len, d_model]\n",
    "        x = x.view(batch_size, -1, self.num_heads,\n",
    "                   self.depth)  # [b, seq_len, d_model=512]=>[b, seq_len, num_head=8, depth=64]\n",
    "        return x.transpose(1, 2)  # [b, seq_len, num_head=8, depth=64]=>[b, num_head=8, seq_len, depth=64]\n",
    "\n",
    "    def forward(self, q, k, v, mask):  # q=k=v=x [b, seq_len, embedding_dim] embedding_dim其实也=d_model\n",
    "        batch_size = q.shape[0]\n",
    "\n",
    "        q = self.wq(q)  # =>[b, seq_len, d_model]\n",
    "        k = self.wk(k)  # =>[b, seq_len, d_model]\n",
    "        v = self.wv(v)  # =>[b, seq_len, d_model]\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # =>[b, num_head=8, seq_len, depth=64]\n",
    "        k = self.split_heads(k, batch_size)  # =>[b, num_head=8, seq_len, depth=64]\n",
    "        v = self.split_heads(v, batch_size)  # =>[b, num_head=8, seq_len, depth=64]\n",
    "\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        # => [b, num_head=8, seq_len_q, depth=64], [b, num_head=8, seq_len_q, seq_len_k]\n",
    "\n",
    "        scaled_attention = scaled_attention.transpose(1, 2)  # =>[b, seq_len_q, num_head=8, depth=64]\n",
    "        # 转置操作让张量存储结构扭曲，直接使用view方法会失败，可以使用reshape方法\n",
    "        concat_attention = scaled_attention.reshape(batch_size, -1, self.d_model)  # =>[b, seq_len_q, d_model=512]\n",
    "\n",
    "        output = self.final_linear(concat_attention)  # =>[b, seq_len_q, d_model=512]\n",
    "        return output, attention_weights  # [b, seq_len_q, d_model=512], [b, num_head=8, seq_len_q, seq_len_k]\n",
    "\n",
    "\n",
    "# temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "# x = torch.rand(1, 64, 512) # [b,seq_len,d_model,embedding_dim]\n",
    "# print(x.shape)\n",
    "# out, attn_weights = temp_mha(x, x, x, mask=None)\n",
    "# print(out.shape, attn_weights.shape) # [1, 60, 512], [1, 8, 60, 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class mn_MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads, seq_len):\n",
    "        super(mn_MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        assert d_model % self.num_heads == 0  # 因为输入要被（平均？）split到不同的head\n",
    "        \n",
    "        # 512/8=64，所以在mn_scaled dot-product atten中dq=dk=64,dv也是64\n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "\n",
    "        self.wq = torch.nn.Linear(d_model, d_model)\n",
    "        self.wk = torch.nn.Linear(d_model, d_model)\n",
    "        self.wv = torch.nn.Linear(d_model, d_model)\n",
    "        self.wm = torch.nn.Linear(d_model, seq_len*num_heads)\n",
    "\n",
    "        self.final_linear = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):  # x [b, seq_len, d_model]\n",
    "        x = x.view(batch_size, -1, self.num_heads,\n",
    "                   self.depth)  # [b, seq_len, d_model=512]=>[b, seq_len, num_head=8, depth=64]\n",
    "        return x.transpose(1, 2)  # [b, seq_len, num_head=8, depth=64]=>[b, num_head=8, seq_len, depth=64]\n",
    "    \n",
    "    def split_heads_m(self, x, batch_size):  # x [b, seq_len, d_model]\n",
    "        x = x.view(batch_size, -1, self.num_heads,\n",
    "                   self.seq_len)  # [b, seq_len, d_model=512]=>[b, seq_len, num_head=8, depth=64]\n",
    "        return x.transpose(1, 2)  # [b, seq_len, num_head=8, depth=64]=>[b, num_head=8, seq_len, depth=seq_len]\n",
    "    \n",
    "    def forward(self, q, k, v, m, mask):  # q=k=v=x [b, seq_len, embedding_dim] embedding_dim其实也=d_model\n",
    "        batch_size = q.shape[0]\n",
    "        \n",
    "        q = self.wq(q)  # =>[b, seq_len, d_model]\n",
    "#         print('q：', q.shape)\n",
    "        k = self.wk(k)  # =>[b, seq_len, d_model]\n",
    "#         print('k：', k.shape)\n",
    "        v = self.wv(v)  # =>[b, seq_len, d_model]\n",
    "#         print('v：', v.shape)\n",
    "        m = self.wm(m)  # =>[b, seq_len, seq_len]\n",
    "#         print('m：', m.shape)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # =>[b, num_head=8, seq_len, depth=64]\n",
    "#         print('q：', q.shape)\n",
    "        k = self.split_heads(k, batch_size)  # =>[b, num_head=8, seq_len, depth=64]\n",
    "#         print('k：', k.shape)\n",
    "        v = self.split_heads(v, batch_size)  # =>[b, num_head=8, seq_len, depth=64]\n",
    "#         print('v：', v.shape)\n",
    "        m = self.split_heads_m(m, batch_size)  # =>[b, num_head=8, seq_len, depth=64]\n",
    "#         print('m：', m.shape)\n",
    "        scaled_attention, attention_weights = mn_scaled_dot_product_attention(q, k, v, m, mask)\n",
    "        # => [b, num_head=8, seq_len_q, depth=64], [b, num_head=8, seq_len_q, seq_len_k]\n",
    "\n",
    "        scaled_attention = scaled_attention.transpose(1, 2)  # =>[b, seq_len_q, num_head=8, depth=64]\n",
    "        # 转置操作让张量存储结构扭曲，直接使用view方法会失败，可以使用reshape方法\n",
    "        concat_attention = scaled_attention.reshape(batch_size, -1, self.d_model)  # =>[b, seq_len_q, d_model=512]\n",
    "\n",
    "        output = self.final_linear(concat_attention)  # =>[b, seq_len_q, d_model=512]\n",
    "        return output, attention_weights  # [b, seq_len_q, d_model=512], [b, num_head=8, seq_len_q, seq_len_k]\n",
    "\n",
    "\n",
    "# temp_mha = mn_MultiHeadAttention(d_model=256, num_heads=4, seq_len=MAX_LENGTH)\n",
    "# x = torch.rand(128, MAX_LENGTH, 256) # [b,seq_len,d_model,embedding_dim]\n",
    "# print('输入：', x.shape)\n",
    "# out, attn_weights = temp_mha(x, x, x, x, mask=None)\n",
    "# print(out.shape, attn_weights.shape) # [1, 60, 512], [1, 8, 60, 60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 6.point wise feed forward network\n",
    "2层线性变换和一个ReLU激活：\n",
    "$$FFN(x) = max(0, xW1 + b1)W2 + b2$$\n",
    "\n",
    "这一层的input和output的维度都是$d_{model}$，而内层的维度的是$d_{ff}=2048$\n",
    "\n",
    "其实就是(nn.relu(x w1+b1))w2+b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 点式前馈网络\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    feed_forward_net = torch.nn.Sequential(\n",
    "        torch.nn.Linear(d_model, dff),  # [b, seq_len, d_model]=>[b, seq_len, dff=2048]\n",
    "        #torch.nn.ReLU(),\n",
    "        \n",
    "        # 更换激活函数为SquaredReLU()\n",
    "#         torch.nn.LeakyReLU(),\n",
    "        SquaredReLU(), #primer_ez\n",
    "        torch.nn.Linear(dff, d_model),  # [b, seq_len, dff=2048]=>[b, seq_len, d_model=512]\n",
    "    )\n",
    "    return feed_forward_net\n",
    "\n",
    "# sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
    "# input = torch.rand(64, 50, 512) # [b, seq_len, d_model]\n",
    "# print(sample_ffn(input).shape) # [b=64, seq_len=50, d_model=512]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "transformer model:\n",
    "\n",
    "(1) input sentence 传入N个encoder layer，句子的每个token都会有一个输出\n",
    "\n",
    "(2) decoder attends on encoder 的输出（encoder-decoder attention） 和 它自己的输入（self-attention），然后预测出下一个词\n",
    "\n",
    "## 7.encoder layer\n",
    "每个编码器层包括以下2个子层：\n",
    "- 多头注意力（有padding mask）\n",
    "- 点式前馈网络（Point wise feed forward networks）\n",
    "\n",
    "注意：每个子层还伴随着一个残差连接，然后进行“层归一化”（LayerNorm）。残差连接有助于避免深度网络中的梯度消失问题。\n",
    "\n",
    "每个子层的输出是 LayerNorm(x + Sublayer(x))。归一化是在 d_model（最后一个）维度完成的。\n",
    "\n",
    "注意：实现时在每个sub layer 之后加入了dropout层，再才进行add the sub layer input 和 normalized，即\n",
    "LayerNorm(x + Dropout(Sublayer(x)))\n",
    "\n",
    "Transformer 中有 N 个编码器层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)  # 多头注意力（padding mask）(self-attention)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "        self.layernorm2 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "\n",
    "        self.dropout1 = torch.nn.Dropout(rate)\n",
    "        self.dropout2 = torch.nn.Dropout(rate)\n",
    "\n",
    "    # x [b, inp_seq_len, embedding_dim] embedding_dim其实也=d_model\n",
    "    # mask [b,1,1,inp_seq_len]\n",
    "    def forward(self, x, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # =>[b, seq_len, d_model]\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)  # 残差&层归一化 =>[b, seq_len, d_model]\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # =>[b, seq_len, d_model]\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # 残差&层归一化 =>[b, seq_len, d_model]\n",
    "\n",
    "        return out2  # [b, seq_len, d_model]\n",
    "\n",
    "# layernorm = torch.nn.LayerNorm(normalized_shape=512, eps=1e-6)\n",
    "# x = torch.rand(4, 64, 512)\n",
    "# print(layernorm(x).shape)\n",
    "\n",
    "# sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "# x = torch.rand(64, 64, 512) # [b, seq_len, d_model]\n",
    "# sample_encoder_layer_output = sample_encoder_layer(x, None)\n",
    "# print(sample_encoder_layer_output.shape) # [64, 50, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mn_EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff, seq_len, rate=0.1):\n",
    "        super(mn_EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = mn_MultiHeadAttention(d_model, num_heads, seq_len)  # 多头注意力（padding mask）(self-attention)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "        self.layernorm2 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "\n",
    "        self.dropout1 = torch.nn.Dropout(rate)\n",
    "        self.dropout2 = torch.nn.Dropout(rate)\n",
    "\n",
    "    # x [b, inp_seq_len, embedding_dim] embedding_dim其实也=d_model\n",
    "    # mask [b,1,1,inp_seq_len]\n",
    "    def forward(self, x, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, x, mask)  # =>[b, seq_len, d_model]\n",
    "#         print(attn_output.shape)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)  # 残差&层归一化 =>[b, seq_len, d_model]\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # =>[b, seq_len, d_model]\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # 残差&层归一化 =>[b, seq_len, d_model]\n",
    "\n",
    "        return out2  # [b, seq_len, d_model]\n",
    "\n",
    "# layernorm = torch.nn.LayerNorm(normalized_shape=512, eps=1e-6)\n",
    "# x = torch.rand(4, MAX_LENGTH, 512)\n",
    "# print(layernorm(x).shape)\n",
    "\n",
    "# sample_encoder_layer = mn_EncoderLayer(512, 8, 2048,MAX_LENGTH)\n",
    "# x = torch.rand(64, MAX_LENGTH, 512) # [b, seq_len, d_model]\n",
    "# sample_encoder_layer_output = sample_encoder_layer(x, None)\n",
    "# print(sample_encoder_layer_output.shape) # [64, 50, 512]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 8.decoder layer\n",
    "\n",
    "每个解码器层包括以下3个子层：\n",
    "- masked的多头注意力（look ahead mask 和 padding mask）(self-attention)\n",
    "- 多头注意力（padding mask）(encoder-decoder attention)。\n",
    "    V和 K接收encoder的输出作为输入。Q接收masked的多头注意力子层的输出。\n",
    "- 点式前馈网络\n",
    "\n",
    "每个子层还伴随着一个残差连接，然后进行“层归一化”（LayerNorm）。\n",
    "\n",
    "每个子层的输出是 LayerNorm(x + Sublayer(x))。归一化是在 d_model（最后一个）维度完成的。\n",
    "\n",
    "Transformer 中共有 N 个解码器层。\n",
    "\n",
    "当 Q 接收到decoder的第一个注意力块的输出，并且 K 接收到encoder的输出时，注意力权重表示根据encoder的输出赋予decoder输入的重要性。\n",
    "换一种说法，decoder通过查看encoder输出和对其自身输出的自注意力，预测下一个词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model,\n",
    "                                       num_heads)  # masked的多头注意力（look ahead mask 和 padding mask）(self-attention)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)  # 多头注意力（padding mask）(encoder-decoder attention)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "        self.layernorm2 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "        self.layernorm3 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "\n",
    "        self.dropout1 = torch.nn.Dropout(rate)\n",
    "        self.dropout2 = torch.nn.Dropout(rate)\n",
    "        self.dropout3 = torch.nn.Dropout(rate)\n",
    "\n",
    "    # x [b, targ_seq_len, embedding_dim] embedding_dim其实也=d_model=512\n",
    "    # look_ahead_mask [b, 1, targ_seq_len, targ_seq_len] 这里传入的look_ahead_mask应该是已经结合了look_ahead_mask和padding mask的mask\n",
    "    # enc_output [b, inp_seq_len, d_model]\n",
    "    # padding_mask [b, 1, 1, inp_seq_len]\n",
    "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x,\n",
    "                                               look_ahead_mask)  # =>[b, targ_seq_len, d_model], [b, num_heads, targ_seq_len, targ_seq_len]\n",
    "        attn1 = self.dropout1(attn1)\n",
    "        out1 = self.layernorm1(x + attn1)  # 残差&层归一化 [b, targ_seq_len, d_model]\n",
    "\n",
    "        # Q: receives the output from decoder's first attention block，即 masked multi-head attention sublayer\n",
    "        # K V: V (value) and K (key) receive the encoder output as inputs\n",
    "        attn2, attn_weights_block2 = self.mha2(out1, enc_output, enc_output,\n",
    "                                               padding_mask)  # =>[b, targ_seq_len, d_model], [b, num_heads, targ_seq_len, inp_seq_len]\n",
    "        attn2 = self.dropout2(attn2)\n",
    "        out2 = self.layernorm2(out1 + attn2)  # 残差&层归一化 [b, targ_seq_len, d_model]\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # =>[b, targ_seq_len, d_model]\n",
    "        ffn_output = self.dropout3(ffn_output)\n",
    "        out3 = self.layernorm3(out2 + ffn_output)  # 残差&层归一化 =>[b, targ_seq_len, d_model]\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "        # [b, targ_seq_len, d_model], [b, num_heads, targ_seq_len, targ_seq_len], [b, num_heads, targ_seq_len, inp_seq_len]\n",
    "\n",
    "\n",
    "# sample_decoder_layer = DecoderLayer(512, 8, 2048) #\n",
    "# y = torch.rand(64, 40, 512) # [b, seq_len, d_model]\n",
    "# sample_decoder_layer_output,_,_ = sample_decoder_layer(y, sample_encoder_layer_output, None, None)\n",
    "# print(sample_decoder_layer_output.shape) # [64, 40, 512]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 9.encoder\n",
    "编码器 包括：\n",
    "- 输入嵌入（Input Embedding）\n",
    "- 位置编码（Positional Encoding）\n",
    "- N 个编码器层（encoder layers）\n",
    "\n",
    "输入经过嵌入（embedding）后，该嵌入与位置编码相加。该加法结果的输出是编码器层的输入。编码器的输出是解码器的输入\n",
    "\n",
    "注意：缩放 embedding\n",
    "\n",
    "原始论文的3.4节Embeddings and Softmax最后一句有提到： In the embedding layers, we multiply those weights by $\\sqrt{d_{model}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers,  # N个encoder layer\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dff,  # 点式前馈网络内层fn的维度\n",
    "                 input_vocab_size,  # 输入词表大小（源语言（法语））\n",
    "                 maximun_position_encoding,\n",
    "                 rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=input_vocab_size, embedding_dim=d_model)\n",
    "        self.pos_encoding = positional_encoding(maximun_position_encoding,\n",
    "                                                d_model)  # =>[1, max_pos_encoding, d_model=512]\n",
    "\n",
    "        # self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate).cuda() for _ in range(num_layers)] # 不行\n",
    "        self.enc_layers = torch.nn.ModuleList([EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)])\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(rate)\n",
    "\n",
    "    # x [b, inp_seq_len]\n",
    "    # mask [b, 1, 1, inp_sel_len]\n",
    "    def forward(self, x, mask):\n",
    "        inp_seq_len = x.shape[-1]\n",
    "\n",
    "        # adding embedding and position encoding\n",
    "        x = self.embedding(x)  # [b, inp_seq_len]=>[b, inp_seq_len, d_model]\n",
    "        # 缩放 embedding 原始论文的3.4节有提到： In the embedding layers, we multiply those weights by \\sqrt{d_model}.\n",
    "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
    "        pos_encoding = self.pos_encoding[:, :inp_seq_len, :]\n",
    "        pos_encoding = pos_encoding.cuda()  # ###############\n",
    "        x += pos_encoding  # [b, inp_seq_len, d_model]\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, mask)  # [b, inp_seq_len, d_model]=>[b, inp_seq_len, d_model]\n",
    "        return x  # [b, inp_seq_len, d_model]\n",
    "\n",
    "\n",
    "# sample_encoder = Encoder(num_layers=2,\n",
    "#                          d_model=512,\n",
    "#                          num_heads=8,\n",
    "#                          dff=2048,\n",
    "#                          input_vocab_size=8500,\n",
    "#                          maximun_position_encoding=10000)\n",
    "# sample_encoder = sample_encoder.to(device)\n",
    "\n",
    "# x = torch.tensor(np.random.randint(low=0, high=200, size=(64, 42))) # [b, seq_len]\n",
    "# # print(x.shape)\n",
    "# sample_encoder_output = sample_encoder(x.cuda(), None)\n",
    "# print(sample_encoder_output.shape) # [b, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mn_Encoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers,  # N个encoder layer\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dff,  # 点式前馈网络内层fn的维度\n",
    "                 input_vocab_size,  # 输入词表大小（源语言（法语））\n",
    "                 maximun_position_encoding,\n",
    "                 seq_len,\n",
    "                 rate=0.1):\n",
    "        super(mn_Encoder, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=input_vocab_size, embedding_dim=d_model)\n",
    "        self.pos_encoding = positional_encoding(maximun_position_encoding,\n",
    "                                                d_model)  # =>[1, max_pos_encoding, d_model=512]\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate).cuda() for _ in range(num_layers)] # 不行\n",
    "        self.enc_layers = torch.nn.ModuleList([mn_EncoderLayer(d_model, num_heads, dff, seq_len, rate) for _ in range(num_layers)])\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(rate)\n",
    "\n",
    "    # x [b, inp_seq_len]\n",
    "    # mask [b, 1, 1, inp_sel_len]\n",
    "    def forward(self, x, mask):\n",
    "        inp_seq_len = x.shape[-1]\n",
    "\n",
    "        # adding embedding and position encoding\n",
    "        x = self.embedding(x)  # [b, inp_seq_len]=>[b, inp_seq_len, d_model]\n",
    "        # 缩放 embedding 原始论文的3.4节有提到： In the embedding layers, we multiply those weights by \\sqrt{d_model}.\n",
    "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
    "        pos_encoding = self.pos_encoding[:, :inp_seq_len, :]\n",
    "        pos_encoding = pos_encoding.cuda()  # ###############\n",
    "        x += pos_encoding  # [b, inp_seq_len, d_model]\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, mask)  # [b, inp_seq_len, d_model]=>[b, inp_seq_len, d_model]\n",
    "        return x  # [b, inp_seq_len, d_model]\n",
    "\n",
    "\n",
    "# sample_encoder = mn_Encoder(num_layers=2,\n",
    "#                          d_model=512,\n",
    "#                          num_heads=8,\n",
    "#                          dff=2048,\n",
    "#                          input_vocab_size=8500,\n",
    "#                          maximun_position_encoding=10000,\n",
    "#                          seq_len = MAX_LENGTH)\n",
    "# sample_encoder = sample_encoder.to(device)\n",
    "\n",
    "# temp_inp = torch.tensor(np.random.randint(low=0, high=200, size=(64, MAX_LENGTH))) # [b, inp_seq_len]\n",
    "# print(x.shape)\n",
    "# sample_encoder_output = sample_encoder(temp_inp.cuda(), None)\n",
    "# print(sample_encoder_output.shape) # [b, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 10.decoder\n",
    "解码器包括：\n",
    "- 输出嵌入（Output Embedding）\n",
    "- 位置编码（Positional Encoding）\n",
    "\n",
    "N 个解码器层（decoder layers）\n",
    "\n",
    "目标（target）经过一个嵌入后，该嵌入和位置编码相加。该加法结果是解码器层的输入。解码器的输出是最后的线性层的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers,  # N个encoder layer\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dff,  # 点式前馈网络内层fn的维度\n",
    "                 target_vocab_size,  # target词表大小（目标语言（英语））\n",
    "                 maximun_position_encoding,\n",
    "                 rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=target_vocab_size, embedding_dim=d_model)\n",
    "        self.pos_encoding = positional_encoding(maximun_position_encoding,\n",
    "                                                d_model)  # =>[1, max_pos_encoding, d_model=512]\n",
    "\n",
    "        # self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate).cuda() for _ in range(num_layers)] # 不行\n",
    "        self.dec_layers = torch.nn.ModuleList([DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)])\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(rate)\n",
    "\n",
    "    # x [b, targ_seq_len]\n",
    "    # look_ahead_mask [b, 1, targ_seq_len, targ_seq_len] 这里传入的look_ahead_mask应该是已经结合了look_ahead_mask和padding mask的mask\n",
    "    # enc_output [b, inp_seq_len, d_model]\n",
    "    # padding_mask [b, 1, 1, inp_seq_len]\n",
    "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        targ_seq_len = x.shape[-1]\n",
    "\n",
    "        attention_weights = {}\n",
    "\n",
    "        # adding embedding and position encoding\n",
    "        x = self.embedding(x)  # [b, targ_seq_len]=>[b, targ_seq_len, d_model]\n",
    "        # 缩放 embedding 原始论文的3.4节有提到： In the embedding layers, we multiply those weights by \\sqrt{d_model}.\n",
    "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
    "        # x += self.pos_encoding[:, :targ_seq_len, :]  # [b, targ_seq_len, d_model]\n",
    "        pos_encoding = self.pos_encoding[:, :targ_seq_len, :]  # [b, targ_seq_len, d_model]\n",
    "        pos_encoding = pos_encoding.cuda() # ###############\n",
    "        x += pos_encoding  # [b, inp_seq_len, d_model]\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        #此处添加相对距离\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x, attn_block1, attn_block2 = self.dec_layers[i](x, enc_output, look_ahead_mask, padding_mask)\n",
    "            # => [b, targ_seq_len, d_model], [b, num_heads, targ_seq_len, targ_seq_len], [b, num_heads, targ_seq_len, inp_seq_len]\n",
    "\n",
    "            attention_weights[f'decoder_layer{i + 1}_block1'] = attn_block1\n",
    "            attention_weights[f'decoder_layer{i + 1}_block2'] = attn_block2\n",
    "\n",
    "        return x, attention_weights\n",
    "        # => [b, targ_seq_len, d_model],\n",
    "        # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "        #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "\n",
    "# sample_decoder = Decoder(num_layers=2,\n",
    "#                          d_model=512,\n",
    "#                          num_heads=8,\n",
    "#                          dff=2048,\n",
    "#                          target_vocab_size=8000,\n",
    "#                          maximun_position_encoding=5000)\n",
    "# sample_decoder = sample_decoder.to(device)\n",
    "\n",
    "# y = torch.tensor(np.random.randint(low=0, high=200, size=(64, 42))) # [b, seq_len]\n",
    "# # print(y.shape) # [64, 36]\n",
    "# output, attn = sample_decoder(y.cuda(),\n",
    "#                               enc_output=sample_encoder_output, # [64, 42, 512]\n",
    "#                               look_ahead_mask=None,\n",
    "#                               padding_mask=None)\n",
    "# print(output.shape) # [64, 36, 512]\n",
    "# print(attn['decoder_layer2_block1'].shape) # [64, 8, 36, 36]\n",
    "# print(attn['decoder_layer2_block2'].shape) # [64, 8, 36, 42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dependency_parsing_Decoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers,  # N个encoder layer\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dff,  # 点式前馈网络内层fn的维度\n",
    "                 target_vocab_size,  # target词表大小（目标语言（英语））\n",
    "                 maximun_position_encoding,\n",
    "                 rate=0.1):\n",
    "        super(dependency_parsing_Decoder, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=target_vocab_size, embedding_dim=d_model)\n",
    "        self.pos_encoding = positional_encoding(maximun_position_encoding,\n",
    "                                                d_model)  # =>[1, max_pos_encoding, d_model=512]\n",
    "        \n",
    "        self.dependency_parsing_Attention = torch.nn.ModuleList([dependency_parsing_Attention(d_model)])\n",
    "        # self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate).cuda() for _ in range(num_layers)] # 不行\n",
    "        self.dec_layers = torch.nn.ModuleList([DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)])\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(rate)\n",
    "\n",
    "    # x [b, targ_seq_len]\n",
    "    # look_ahead_mask [b, 1, targ_seq_len, targ_seq_len] 这里传入的look_ahead_mask应该是已经结合了look_ahead_mask和padding mask的mask\n",
    "    # enc_output [b, inp_seq_len, d_model]\n",
    "    # padding_mask [b, 1, 1, inp_seq_len]\n",
    "    def forward(self, x, dependency_parsing_matrix, enc_output, look_ahead_mask, padding_mask):\n",
    "#         print('dependency_parsing_Decoder_input:',x.shape)\n",
    "        targ_seq_len = x.shape[-1]\n",
    "\n",
    "        attention_weights = {}\n",
    "\n",
    "        # adding embedding and position encoding\n",
    "        x = self.embedding(x)  # [b, targ_seq_len]=>[b, targ_seq_len, d_model]\n",
    "        # 缩放 embedding 原始论文的3.4节有提到： In the embedding layers, we multiply those weights by \\sqrt{d_model}.\n",
    "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
    "        # x += self.pos_encoding[:, :targ_seq_len, :]  # [b, targ_seq_len, d_model]\n",
    "        pos_encoding = self.pos_encoding[:, :targ_seq_len, :]  # [b, targ_seq_len, d_model]\n",
    "        pos_encoding = pos_encoding.cuda() # ###############\n",
    "        x += pos_encoding  # [b, inp_seq_len, d_model]\n",
    "#         print('x += pos_encoding:',x.shape)\n",
    "        \n",
    "        #此处添加相对距离\n",
    "        x, _ = self.dependency_parsing_Attention[0](x, x, x, dependency_parsing_matrix, look_ahead_mask)\n",
    "        \n",
    "#         print('dependency_parsing_Attention:',x.shape)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x, attn_block1, attn_block2 = self.dec_layers[i](x, enc_output, look_ahead_mask, padding_mask)\n",
    "            # => [b, targ_seq_len, d_model], [b, num_heads, targ_seq_len, targ_seq_len], [b, num_heads, targ_seq_len, inp_seq_len]\n",
    "\n",
    "            attention_weights[f'decoder_layer{i + 1}_block1'] = attn_block1\n",
    "            attention_weights[f'decoder_layer{i + 1}_block2'] = attn_block2\n",
    "\n",
    "        return x, attention_weights\n",
    "        # => [b, targ_seq_len, d_model],\n",
    "        # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "        #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "\n",
    "# sample_decoder = dependency_parsing_Decoder(num_layers=2,\n",
    "#                          d_model=512,\n",
    "#                          num_heads=8,\n",
    "#                          dff=2048,\n",
    "#                          target_vocab_size=8000,\n",
    "#                          maximun_position_encoding=5000)\n",
    "# sample_decoder = sample_decoder.to(device)\n",
    "\n",
    "# y = torch.tensor(np.random.randint(low=0, high=200, size=(64, 42))) # [b, seq_len]\n",
    "# dependency_parsing_matrix = torch.tensor(np.random.randint(low=0, high=200, size=(64, 42, 42)))\n",
    "\n",
    "# output, attn = sample_decoder(y.cuda(),\n",
    "#                               dependency_parsing_matrix.cuda(),\n",
    "#                               enc_output=sample_encoder_output, # [64, 42, 512]\n",
    "#                               look_ahead_mask=None,\n",
    "#                               padding_mask=None)\n",
    "# print(output.shape) # [64, 36, 512]\n",
    "# print(attn['decoder_layer2_block1'].shape) # [64, 8, 36, 36]\n",
    "# print(attn['decoder_layer2_block2'].shape) # [64, 8, 36, 42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 11.搭建transformer\n",
    "transformer 含有3各部分\n",
    "- encoder\n",
    "- decoder\n",
    "- final linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# class Transformer(torch.nn.Module):\n",
    "#     def __init__(self,\n",
    "#                  num_layers,  # N个encoder layer\n",
    "#                  d_model,\n",
    "#                  num_heads,\n",
    "#                  dff,  # 点式前馈网络内层fn的维度\n",
    "#                  input_vocab_size,  # input此表大小（源语言（法语））\n",
    "#                  target_vocab_size,  # target词表大小（目标语言（英语））\n",
    "#                  pe_input,  # input max_pos_encoding\n",
    "#                  pe_target,  # input max_pos_encoding\n",
    "#                  rate=0.1):\n",
    "#         super(Transformer, self).__init__()\n",
    "\n",
    "#         self.encoder = Encoder(num_layers,\n",
    "#                                d_model,\n",
    "#                                num_heads,\n",
    "#                                dff,\n",
    "#                                input_vocab_size,\n",
    "#                                pe_input,\n",
    "#                                rate)\n",
    "#         self.decoder = Decoder(num_layers,\n",
    "#                                d_model,\n",
    "#                                num_heads,\n",
    "#                                dff,\n",
    "#                                target_vocab_size,\n",
    "#                                pe_target,\n",
    "#                                rate)\n",
    "#         self.final_layer = torch.nn.Linear(d_model, target_vocab_size)\n",
    "\n",
    "#     # inp [b, inp_seq_len]\n",
    "#     # targ [b, targ_seq_len]\n",
    "#     # enc_padding_mask [b, 1, 1, inp_seq_len]\n",
    "#     # look_ahead_mask [b, 1, targ_seq_len, targ_seq_len]\n",
    "#     # dec_padding_mask [b, 1, 1, inp_seq_len] # 注意这里的维度是inp_seq_len\n",
    "#     def forward(self, inp, targ, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "#         enc_output = self.encoder(inp, enc_padding_mask)  # =>[b, inp_seq_len, d_model]\n",
    "\n",
    "#         dec_output, attention_weights = self.decoder(targ, enc_output, look_ahead_mask, dec_padding_mask)\n",
    "#         # => [b, targ_seq_len, d_model],\n",
    "#         # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "#         #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "#         final_output = self.final_layer(dec_output)  # =>[b, targ_seq_len, target_vocab_size]\n",
    "\n",
    "#         return final_output, attention_weights\n",
    "#         # [b, targ_seq_len, target_vocab_size]\n",
    "#         # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "#         #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "\n",
    "\n",
    "# sample_transformer = Transformer(num_layers=2,\n",
    "#                                  d_model=512,\n",
    "#                                  num_heads=8,\n",
    "#                                  dff=2048,\n",
    "#                                  input_vocab_size=8500,\n",
    "#                                  target_vocab_size=8000,\n",
    "#                                  pe_input=10000,\n",
    "#                                  pe_target=6000)\n",
    "# sample_transformer = sample_transformer.to(device)\n",
    "\n",
    "# temp_inp = torch.tensor(np.random.randint(low=0, high=200, size=(64, 36))) # [b, inp_seq_len]\n",
    "# temp_targ = torch.tensor(np.random.randint(low=0, high=200, size=(64, 42))) # [b, targ_seq_len]\n",
    "\n",
    "# fn_out, attn = sample_transformer(temp_inp.cuda(), temp_targ.cuda(), None, None, None)\n",
    "# print(fn_out.shape) # [64, 36, 8000]\n",
    "# print(attn['decoder_layer2_block1'].shape) # [64, 8, 36, 36]\n",
    "# print(attn['decoder_layer2_block2'].shape) # [64, 8, 36, 42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mn_Transformer(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers,  # N个encoder layer\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dff,  # 点式前馈网络内层fn的维度\n",
    "                 input_vocab_size,  # input此表大小（源语言（法语））\n",
    "                 target_vocab_size,  # target词表大小（目标语言（英语））\n",
    "                 pe_input,  # input max_pos_encoding\n",
    "                 pe_target,  # input max_pos_encoding\n",
    "                 rate=0.1):\n",
    "        super(mn_Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers,\n",
    "                               d_model,\n",
    "                               num_heads,\n",
    "                               dff,\n",
    "                               input_vocab_size,\n",
    "                               pe_input,\n",
    "                               rate)\n",
    "        # 修改\n",
    "        self.decoder = dependency_parsing_Decoder(num_layers,\n",
    "                               d_model,\n",
    "                               num_heads,\n",
    "                               dff,\n",
    "                               target_vocab_size,\n",
    "                               pe_target,\n",
    "                               rate)\n",
    "        self.final_layer = torch.nn.Linear(d_model, target_vocab_size)\n",
    "\n",
    "    # inp [b, inp_seq_len]\n",
    "    # targ [b, targ_seq_len]\n",
    "    # enc_padding_mask [b, 1, 1, inp_seq_len]\n",
    "    # look_ahead_mask [b, 1, targ_seq_len, targ_seq_len]\n",
    "    # dec_padding_mask [b, 1, 1, inp_seq_len] # 注意这里的维度是inp_seq_len\n",
    "    def forward(self, inp, targ, dependency_parsing_matrix, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        enc_output = self.encoder(inp, enc_padding_mask)  # =>[b, inp_seq_len, d_model]\n",
    "        dec_output, attention_weights = self.decoder(targ, dependency_parsing_matrix, enc_output, look_ahead_mask, dec_padding_mask)\n",
    "        # => [b, targ_seq_len, d_model],\n",
    "        # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "        #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "        final_output = self.final_layer(dec_output)  # =>[b, targ_seq_len, target_vocab_size]\n",
    "\n",
    "        return final_output, attention_weights\n",
    "        # [b, targ_seq_len, target_vocab_size]\n",
    "        # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "        #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_transformer = mn_Transformer(num_layers=2,\n",
    "#                                  d_model=128,\n",
    "#                                  num_heads=8,\n",
    "#                                  dff=2048,\n",
    "#                                  input_vocab_size=8500,\n",
    "#                                  target_vocab_size=8000,\n",
    "#                                  pe_input=10000,\n",
    "#                                  pe_target=10000,\n",
    "#                                  seq_len=MAX_LENGTH+2)\n",
    "# sample_transformer = sample_transformer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_inp = torch.tensor(np.random.randint(low=0, high=200, size=(64, MAX_LENGTH+2))) # [b, inp_seq_len]\n",
    "# temp_targ = torch.tensor(np.random.randint(low=0, high=200, size=(64, MAX_LENGTH+2))) # [b, targ_seq_len]\n",
    "# dependency_parsing_matrix = torch.tensor(np.random.randint(low=0, high=200, size=(64, MAX_LENGTH+2, MAX_LENGTH+2)))\n",
    "\n",
    "# fn_out, attn = sample_transformer(temp_inp.cuda(), temp_targ.cuda(),dependency_parsing_matrix.cuda(), None, None, None)\n",
    "\n",
    "\n",
    "# # for step, (inp, targ) in enumerate(train_dataloader, start=1):\n",
    "# #     print(inp,inp.shape,'\\n',targ,targ.shape)\n",
    "# #     fn_out, attn = sample_transformer(inp.cuda(), targ.cuda(), None, None, None)\n",
    "# #     break\n",
    "\n",
    "\n",
    "\n",
    "# print(fn_out.shape) # [64, 36, 8000]\n",
    "# print(attn['decoder_layer2_block1'].shape) # [64, 8, 36, 36]\n",
    "# print(attn['decoder_layer2_block2'].shape) # [64, 8, 36, 42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 12.设置超参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_layers： 4\n",
      "d_model： 128\n",
      "dff： 1024\n",
      "num_heads： 8\n",
      "dropout_rate： 0.2\n"
     ]
    }
   ],
   "source": [
    "#Transformer 的基础模型使用的数值为：num_layers=6，d_model = 512，dff = 2048\n",
    "#为了让本示例小且相对较快，已经减小了num_layers、 d_model 和 dff 的值。\n",
    "num_layers = 5\n",
    "d_model = 256\n",
    "dff = 2048\n",
    "num_heads = 8\n",
    "\n",
    "print('num_layers：', num_layers)\n",
    "print('d_model：', d_model)\n",
    "print('dff：', dff)\n",
    "print('num_heads：', num_heads)\n",
    "\n",
    "dropout_rate = 0.2\n",
    "print('dropout_rate：', dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 13.优化器\n",
    "根据论文中的公式，将 Adam 优化器与自定义的学习速率调度程序（scheduler）配合使用。\n",
    "\n",
    "$$lrate = d_{model}^{-0.5} * min(\\text{step_num}^{-0.5},\\; \\text{step_num}*\\text{warmup_steps}^{-1.5})$$\n",
    "\n",
    "关于pytorch optimizer，参考：\n",
    "\n",
    "https://www.cnblogs.com/wanghui-garcia/p/10895397.html\n",
    "\n",
    "https://www.jianshu.com/p/5d85a59f1bac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, d_model, warm_steps=4):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warm_steps\n",
    "\n",
    "        super(CustomSchedule, self).__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"\n",
    "        # rsqrt 函数用于计算 x 元素的平方根的倒数.  即= 1 / sqrt{x}\n",
    "        arg1 = torch.rsqrt(torch.tensor(self._step_count, dtype=torch.float32))\n",
    "        arg2 = torch.tensor(self._step_count * (self.warmup_steps ** -1.5), dtype=torch.float32)\n",
    "        dynamic_lr = torch.rsqrt(self.d_model) * torch.minimum(arg1, arg2)\n",
    "        \"\"\"\n",
    "        # print('*'*27, self._step_count)\n",
    "        arg1 = self._step_count ** (-0.5)\n",
    "        arg2 = self._step_count * (self.warmup_steps ** -1.5)\n",
    "        dynamic_lr = (self.d_model ** (-0.5)) * min(arg1, arg2)\n",
    "        # print('dynamic_lr:', dynamic_lr)\n",
    "        return [dynamic_lr for group in self.optimizer.param_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # 测试\n",
    "# model = sample_transformer\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "# learning_rate = CustomSchedule(optimizer, d_model, warm_steps=4000)\n",
    "\n",
    "# lr_list = []\n",
    "# for i in range(1, 20000):\n",
    "#     learning_rate.step()\n",
    "#     lr_list.append(learning_rate.get_lr()[0])\n",
    "# plt.figure()\n",
    "# plt.plot(np.arange(1, 20000), lr_list)\n",
    "# plt.legend(['warmup=4000 steps'])\n",
    "# plt.ylabel(\"Learning Rate\")\n",
    "# plt.xlabel(\"Train Step\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # 其他的学习率调度器测试，例如pytorch自带的StepLR\n",
    "# _model = sample_transformer\n",
    "# _optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# _learning_rate = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# lr_list = []\n",
    "# for i in range(1, 50):\n",
    "#     _learning_rate.step()\n",
    "#     lr_list.append(_learning_rate.get_lr()[0])\n",
    "# plt.figure()\n",
    "# plt.plot(np.arange(1, 50), lr_list)\n",
    "# plt.legend(['StepLR:gamma=0.5'])\n",
    "# plt.ylabel(\"Learning Rate\")\n",
    "# plt.xlabel(\"Train Step\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 14.损失和评价准则\n",
    "\n",
    "计算loss时要把mask=1的位置的去除掉\n",
    "\n",
    "### 【大坑！】\n",
    "【注意】，当输入是多维时交叉熵的参数维度，跟tf2不一样，tf2中pred是【b,seq_len,vocab_size】\n",
    "pytorch中pred应该调整为【b,vocab_size,seq_len】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 'none'表示直接返回b个样本的loss，默认求平均\n",
    "loss_object = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "# 【注意】，当输入是多维时交叉熵的参数维度，跟tf2不一样，tf2中pred是【b,seq_len,vocab_size】\n",
    "# pytorch中pred应该调整为【b,vocab_size,seq_len】\n",
    "\"\"\"\n",
    "- Input: :math:`(N, C)` where `C = number of classes`, or\n",
    "          :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1`\n",
    "          in the case of `K`-dimensional loss.\n",
    "\n",
    "- Target: :math:`(N)` where each value is :math:`0 \\leq \\text{targets}[i] \\leq C-1`, or\n",
    "          :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \\geq 1` in the case of\n",
    "          K-dimensional loss.\n",
    "\"\"\"\n",
    "\n",
    "# real [b, targ_seq_len]\n",
    "# pred [b, targ_seq_len, target_vocab_size]\n",
    "def mask_loss_func(real, pred):\n",
    "    # print(real.shape, pred.shape)\n",
    "    # _loss = loss_object(pred, real) # [b, targ_seq_len]\n",
    "    _loss = loss_object(pred.transpose(-1, -2), real)  # [b, targ_seq_len]\n",
    "\n",
    "    # logical_not  取非\n",
    "    # mask 每个元素为bool值，如果real中有pad，则mask相应位置就为False\n",
    "    # mask = torch.logical_not(real.eq(0)).type(_loss.dtype) # [b, targ_seq_len] pad=0的情况\n",
    "    mask = torch.logical_not(real.eq(pad)).type(_loss.dtype)  # [b, targ_seq_len] pad!=0的情况\n",
    "\n",
    "    # 对应位置相乘，token上的损失被保留了下来，pad的loss被置为0或False 去掉，不计算在内\n",
    "    _loss *= mask\n",
    "\n",
    "    return _loss.sum() / mask.sum().item()\n",
    "\n",
    "# 另一种实现方式\n",
    "def mask_loss_func2(real, pred):\n",
    "    # _loss = loss_object(pred, real) # [b, targ_seq_len]\n",
    "    _loss = loss_object(pred.transpose(-1, -2), real)  # [b, targ_seq_len]\n",
    "    # mask = torch.logical_not(real.eq(0)) # [b, targ_seq_len] bool值\n",
    "    mask = torch.logical_not(real.eq(pad)) # [b, targ_seq_len] bool值\n",
    "    _loss = _loss.masked_select(mask) # mask必须是BoolTensor或ByteTensor类型\n",
    "    return _loss.mean()\n",
    "\n",
    "\n",
    "# y_pred = torch.randn(3,3) # [3,3]\n",
    "# y_true = torch.tensor([1,2,0]) # [3]\n",
    "# # print(y_true.shape, y_pred.shape)\n",
    "# print(loss_object(y_pred, y_true))\n",
    "# print('计算loss时把mask的也算进去了,损失会偏大？：', loss_object(y_pred, y_true).mean())\n",
    "# print('计算loss时去除mask:', mask_loss_func(y_true, y_pred))\n",
    "# print('计算loss时去除mask:', mask_loss_func2(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "同样计算metric（如acc）时要把mask=1的位置的去除掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# real [b, targ_seq_len]\n",
    "# pred [b, targ_seq_len, target_vocab_size]\n",
    "def mask_accuracy_func(real, pred):\n",
    "    _pred = pred.argmax(dim=-1)  # [b, targ_seq_len, target_vocab_size]=>[b, targ_seq_len]\n",
    "    corrects = _pred.eq(real)  # [b, targ_seq_len] bool值\n",
    "\n",
    "    # logical_not  取非\n",
    "    # mask 每个元素为bool值，如果real中有pad，则mask相应位置就为False\n",
    "    # mask = torch.logical_not(real.eq(0)) # [b, targ_seq_len] bool值 pad=0的情况\n",
    "    mask = torch.logical_not(real.eq(pad))  # [b, targ_seq_len] bool值 pad!=0的情况\n",
    "\n",
    "    # 对应位置相乘，token上的值被保留了下来，pad上的值被置为0或False 去掉，不计算在内\n",
    "    corrects *= mask\n",
    "\n",
    "    return corrects.sum().float() / mask.sum().item()\n",
    "\n",
    "# 另一种实现方式\n",
    "def mask_accuracy_func2(real, pred):\n",
    "    _pred = pred.argmax(dim=-1) # [b, targ_seq_len, target_vocab_size]=>[b, targ_seq_len]\n",
    "    corrects = _pred.eq(real).type(torch.float32) # [b, targ_seq_len]\n",
    "    # mask = torch.logical_not(real.eq(0)) # [b, targ_seq_len] bool值\n",
    "    mask = torch.logical_not(real.eq(pad)) # [b, targ_seq_len] bool值\n",
    "    corrects = corrects.masked_select(mask) # [真正有token的个数] 平摊开成1维的\n",
    "\n",
    "    return corrects.mean()\n",
    "\n",
    "def mask_accuracy_func3(real, pred):\n",
    "    _pred = pred.argmax(dim=-1) # [b, targ_seq_len, target_vocab_size]=>[b, targ_seq_len]\n",
    "    corrects = _pred.eq(real) # [b, targ_seq_len] bool值\n",
    "    # mask = torch.logical_not(real.eq(0)) # [b, targ_seq_len] bool值\n",
    "    mask = torch.logical_not(real.eq(pad)) # [b, targ_seq_len] bool值\n",
    "    corrects = torch.logical_and(corrects, mask)\n",
    "    # print(corrects.dtype) # bool\n",
    "    # print(corrects.sum().dtype) #int64\n",
    "    return corrects.sum().float()/mask.sum().item()\n",
    "\n",
    "# y_pred = torch.randn(3,3) # [3,3]\n",
    "# y_true = torch.tensor([0,2,1]) # [3] 最后一个1表示pad噢~\n",
    "# print(y_true)\n",
    "# print(y_pred)\n",
    "# print('计算acc时去除mask:', mask_accuracy_func(y_true, y_pred))\n",
    "# print('计算acc时去除mask:', mask_accuracy_func2(y_true, y_pred))\n",
    "# print('计算acc时去除mask:', mask_accuracy_func3(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 15.生成mask\n",
    "\n",
    "$$Attention(Q,K,V)=softmax_{(k)}(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "注意：实现时对mask的处理\n",
    "\n",
    "mask=1的位置是pad或者future token，乘以-1e9（-1*10^9）成为负无穷，经过softmax后会趋于0\n",
    "\n",
    "【注意】：在计算decoder的第2个attention block时（encoder-decoder attention），\n",
    "从scaled dot-product的公式可以看出，此时Q是decoder的第一个attention block的输出，\n",
    "而K，V都来自encoder的输出。QK^T后得到[...,seq_len_q, seq_len_k]，而softmax是在seq_len_k维\n",
    "上进行的，softmax后seq_len_k维上pad的位置被置于0，乘以V（seq_len_k=seq_len_v=encoder output的seq_len）\n",
    "后那些pad的位置还是只会是0。所以仅从公式就可以看出decoder的第2个attention block的padding mask是基于\n",
    "encoder output 的seq_len即整个模型的inp_seq_len来设置的，而不是targ_seq_len。\n",
    "\n",
    "从注意力角度来看，可以理解成：当 Q 接收到decoder的第一个注意力块的输出，并且 K 接收到encoder的输出时，注意力权重表示根据encoder的输出赋予decoder输入的重要性。\n",
    "换一种说法，decoder通过查看encoder输出和对其自身输出的自注意`力，预测下一个词。\n",
    "\n",
    "另外 padding mask 的维度是[b,1,1,seq_len] 一般只关注最后一个seq_len维度上pad(无论是在\n",
    "self-attention 或者encoder-decoder attention上都比较容易理解)\n",
    "\n",
    "而 look-ahead mask 是decoder self-attention时用于mask future token的，因为是“自”注意力，\n",
    "所以维度是[...,seq_len, seq_len]，需要自己跟自己运算，所以在QK（此时Q=K=V）矩阵乘的结果的最后2个维度上都需要mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# inp [b, inp_seq_len] 序列已经加入pad填充\n",
    "# targ [b, targ_seq_len] 序列已经加入pad填充\n",
    "def create_mask(inp, targ):\n",
    "    # encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(inp)  # =>[b,1,1,inp_seq_len] mask=1的位置为pad\n",
    "\n",
    "    # decoder's first attention block(self-attention)\n",
    "    # 使用的padding create_mask & look-ahead create_mask\n",
    "    look_ahead_mask = create_look_ahead_mask(targ.shape[-1])  # =>[targ_seq_len,targ_seq_len] ##################\n",
    "    dec_targ_padding_mask = create_padding_mask(targ)  # =>[b,1,1,targ_seq_len]\n",
    "    combined_mask = torch.max(look_ahead_mask, dec_targ_padding_mask)  # 结合了2种mask =>[b,1,targ_seq_len,targ_seq_len]\n",
    "\n",
    "    # decoder's second attention block(encoder-decoder attention) 使用的padding create_mask\n",
    "    # 【注意】：这里的mask是用于遮挡encoder output的填充pad，而encoder的输出与其输入shape都是[b,inp_seq_len,d_model]\n",
    "    # 所以这里mask的长度是inp_seq_len而不是targ_mask_len\n",
    "    dec_padding_mask = create_padding_mask(inp)  # =>[b,1,1,inp_seq_len] mask=1的位置为pad\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
    "    # [b,1,1,inp_seq_len], [b,1,targ_seq_len,targ_seq_len], [b,1,1,inp_seq_len]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 16.训练和保存\n",
    "\n",
    "### 配置检查点 save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 18,685,637 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "transformer = mn_Transformer(num_layers,\n",
    "                          d_model,\n",
    "                          num_heads,\n",
    "                          dff,\n",
    "                          input_vocab_size,\n",
    "                          target_vocab_size,\n",
    "                          pe_input=input_vocab_size,\n",
    "                          pe_target=target_vocab_size,\n",
    "                          rate=dropout_rate)\n",
    "\n",
    "# print(transformer) # 打印模型基本信息\n",
    "\n",
    "transformer = transformer.to(device)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {count_parameters(transformer):,} trainable parameters')\n",
    "\n",
    "\n",
    "if ngpu > 1: # 并行化\n",
    "    transformer = torch.nn.DataParallel(transformer,  device_ids=list(range(ngpu))) # 设置并行执行  device_ids=[0,1]\n",
    "\n",
    "# optimizer = torch.optim.Adam(transformer.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "# lr_scheduler = CustomSchedule(optimizer, d_model, warm_steps=4000)\n",
    "\n",
    "optimizer = torch.optim.AdamW(transformer.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.02, amsgrad=True)\n",
    "\n",
    "lr_scheduler = CustomSchedule(optimizer, d_model, warm_steps)\n",
    "lr_scheduler2 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max',factor=0.5,verbose=True,min_lr= 0,patience=1)\n",
    "\n",
    "#optimizer = adamod.AdaMod(transformer.parameters(), lr=1e-3, beta3=0.999)\n",
    "# print('optimizer：', optimizer)\n",
    "\n",
    "#lr_scheduler2 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max',factor=0.5,verbose=True,min_lr= 0,patience=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 关于position encoding的一点自己的理解：\n",
    "\"\"\"\n",
    "为什么这里传入的pe_input是input_vocab_size，也就是说position encoding传入的pos参数是词表大小\n",
    "而我一开始理解的max_seq_len的大小，所以才在encoder中获取位置编码是self.pos_encoding[:, :seq_len, :]\n",
    "但知道看到这里传入的是词表大小我才发现我理解错了\n",
    "\n",
    "其实position encoding 就跟embedding 一样，就是【vocab_size x d_model】的\n",
    "但此时另一个问题又出现了，那为什么在获取position encoding时 使用的是pos_encoding[:, :seq_len, :]而不是\n",
    "像embedding一样 去look-up 查询每个token的位置表示呢？而是直接取得前seq_len个表示作为位置表示？\n",
    "\n",
    "这就体现了这种位置编码的巧妙之处，他不需要训练，就能独特的表示每一个单独的token，而每个token之间又存在关系\n",
    "所以这并不需要位置编码和token是一一对应（固定死的），只需要位置编码能传达这2点信息（即token的独特性和相对依赖性）就够了\n",
    "即使是同一个token 在一个句子的不同的位置时 他的位置编码当然也是不一样的了 如果像embedding那样去查 那一个句子的\n",
    "不同位置的相同token  其表示就会是一样的  就体现不出位置关系\n",
    "\n",
    "这样一想的话，似乎把pe_input设置成max_seq_len也是可以的。但是这里实现position encoding方式（sin  cos）\n",
    "可以在测试阶段接受长度超过训练集实例的情况！所以干脆把pe_input设置成词表大小？\n",
    "\"\"\"\n",
    "\n",
    "# inp [b,inp_seq_len]\n",
    "# targ [b,targ_seq_len]\n",
    "\"\"\"\n",
    "拆分targ, 例如：sentence = \"SOS A lion in the jungle is sleeping EOS\"\n",
    "tar_inp = \"<start>> A lion in the jungle is sleeping\"\n",
    "tar_real = \"A lion in the jungle is sleeping <end>\"\n",
    "\"\"\"\n",
    "def train_step(model, inp, targ, dependency_matrix):\n",
    "    # 目标（target）被分成了 tar_inp 和 tar_real\n",
    "    # tar_inp 作为输入传递到解码器。\n",
    "    # tar_real 是位移了 1 的同一个输入：在 tar_inp 中的每个位置，tar_real 包含了应该被预测到的下一个标记（token）。\n",
    "    targ_inp = targ[:, :-1] # targ_inp的尺寸是[batch,MAX_LENGTH+1],因为舍弃了<end> https://zhuanlan.zhihu.com/p/166608727\n",
    "#     print('targ_inp:',targ_inp.shape)\n",
    "    targ_real = targ[:, 1:]\n",
    "    \n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_mask(inp, targ_inp)\n",
    "#     print('enc_padding_mask',enc_padding_mask.shape)\n",
    "#     print('combined_mask',combined_mask.shape)\n",
    "#     print('dec_padding_mask',dec_padding_mask.shape)\n",
    "    \n",
    "    inp = inp.to(device)\n",
    "    targ_inp = targ_inp.to(device)\n",
    "    targ_real = targ_real.to(device)\n",
    "    dependency_matrix = dependency_matrix.to(device)\n",
    "#     print('inp.shape',inp.shape)\n",
    "#     print('targ_inp.shape',targ_inp.shape)\n",
    "#     print('dependency_matrix.shape',dependency_matrix.shape)\n",
    "    \n",
    "    enc_padding_mask = enc_padding_mask.to(device)\n",
    "    combined_mask = combined_mask.to(device)\n",
    "    dec_padding_mask = dec_padding_mask.to(device)\n",
    "    \n",
    "    # print('device:', inp.device, targ_inp)\n",
    "\n",
    "    model.train()  # 设置train mode\n",
    "\n",
    "    optimizer.zero_grad()  # 梯度清零\n",
    "\n",
    "    # forward\n",
    "    prediction, _ = model(inp, targ_inp, dependency_matrix, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "    # [b, targ_seq_len, target_vocab_size]\n",
    "    # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "    #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "\n",
    "    loss = mask_loss_func(targ_real, prediction)\n",
    "    metric = mask_accuracy_func(targ_real, prediction)\n",
    "\n",
    "    # backward\n",
    "    loss.backward()  # 反向传播计算梯度\n",
    "    optimizer.step()  # 更新参数\n",
    "\n",
    "    return loss.item(), metric.item()\n",
    "\n",
    "\n",
    "# # 检查train_step()的效果\n",
    "# batch_src, batch_targ = next(iter(train_dataloader)) # [64,10], [64,10]\n",
    "# print(train_step(transformer, batch_src, batch_targ))\n",
    "# \"\"\"\n",
    "# x += pos_encoding  # [b, inp_seq_len, d_model]\n",
    "# RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0!\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def validate_step(model, inp, targ, dependency_matrix):\n",
    "    targ_inp = targ[:, :-1]\n",
    "    targ_real = targ[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_mask(inp, targ_inp)\n",
    "\n",
    "    inp = inp.to(device)\n",
    "    targ_inp = targ_inp.to(device)\n",
    "    targ_real = targ_real.to(device)\n",
    "    dependency_matrix = dependency_matrix.to(device)\n",
    "    \n",
    "#     print('inp.shape:',inp.shape)\n",
    "#     print('targ_inp.shape:',targ_inp.shape)\n",
    "#     print('dependency_matrix.shape:',dependency_matrix.shape)\n",
    "    \n",
    "    enc_padding_mask = enc_padding_mask.to(device)\n",
    "    combined_mask = combined_mask.to(device)\n",
    "    dec_padding_mask = dec_padding_mask.to(device)\n",
    "\n",
    "    model.eval()  # 设置eval mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # forward\n",
    "        prediction, _ = model(inp, targ_inp, dependency_matrix, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "        # [b, targ_seq_len, target_vocab_size]\n",
    "        # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "        #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "\n",
    "        val_loss = mask_loss_func(targ_real, prediction)\n",
    "        val_metric = mask_accuracy_func(targ_real, prediction)\n",
    "\n",
    "    return val_loss.item(), val_metric.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print_trainstep_every = 500  # 每50个step做一次打印\n",
    "\n",
    "metric_name = 'acc'\n",
    "# df_history = pd.DataFrame(columns=['epoch', 'loss', metric_name]) # 记录训练历史信息\n",
    "df_history = pd.DataFrame(columns=['epoch', 'loss', metric_name, 'val_loss', 'val_' + metric_name])\n",
    "\n",
    "\n",
    "# 打印时间\n",
    "def printbar():\n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m_%d %H:%M:%S')\n",
    "    print('\\n' + \"====\"*8 + '%s'%nowtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, epochs, train_dataloader, val_dataloader, dep_train_batch_path, dep_val, print_every):\n",
    "    starttime = time.time()\n",
    "    print('*' * 10, 'start training...')\n",
    "    printbar()\n",
    "\n",
    "    best_acc = 0.\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        loss_sum = 0.\n",
    "        metric_sum = 0.\n",
    "        dep_train_batch_pairs = open(dep_train_batch_path,'rb')\n",
    "        \n",
    "        # dependency_matrix\n",
    "        for step, (inp, targ) in enumerate(train_dataloader, start=1):\n",
    "            # inp [64, 10] , targ [64, 10] \n",
    "#             print('inp.shape:',inp.shape)\n",
    "#             print('targ.shape:',targ.shape)\n",
    "            \n",
    "            # 提取依存句法矩阵\n",
    "#             st = (step-1) * inp.shape[0]\n",
    "#             end = st + inp.shape[0]\n",
    "#             dependency_matrix = dep_train[st:end,:,:]\n",
    "            dependency_matrix = pickle.load(dep_train_batch_pairs)\n",
    "#             print('dependency_matrix.shape:',dependency_matrix.shape)\n",
    "            \n",
    "            loss, metric = train_step(model, inp, targ, dependency_matrix)\n",
    "\n",
    "            loss_sum += loss\n",
    "            metric_sum += metric\n",
    "\n",
    "            # 打印batch级别日志\n",
    "            if step % print_every == 0:\n",
    "                print('*' * 5, f'[step = {step}] loss: {loss_sum / step:.5f}, {metric_name}: {metric_sum / step:.5f}')\n",
    "\n",
    "            lr_scheduler.step()  # 更新学习率\n",
    "            \n",
    "        dep_train_batch_pairs.close()\n",
    "\n",
    "        # 一个epoch的train结束，做一次验证\n",
    "        # test(model, train_dataloader)\n",
    "#         print('val')\n",
    "        val_loss_sum = 0.\n",
    "        val_metric_sum = 0.\n",
    "#         dep_val_batch_pairs = open(dep_val_batch_path,'rb')\n",
    "        \n",
    "        for val_step, (inp, targ) in enumerate(val_dataloader, start=1):\n",
    "            # inp [64, 10] , targ [64, 10]\n",
    "            \n",
    "            # 提取依存句法矩阵\n",
    "            st = (val_step-1) * inp.shape[0]\n",
    "            end = st + inp.shape[0]\n",
    "            dependency_matrix = dep_val[st:end,:,:]\n",
    "#             dependency_matrix = pickle.load(dep_val_batch_pairs)\n",
    "            \n",
    "            loss, metric = validate_step(model, inp, targ, dependency_matrix)\n",
    "\n",
    "            val_loss_sum += loss\n",
    "            val_metric_sum += metric\n",
    "            \n",
    "#         dep_val_batch_pairs.close()\n",
    "        \n",
    "        # 记录和收集1个epoch的训练（和验证）信息\n",
    "        # record = (epoch, loss_sum/step, metric_sum/step)\n",
    "        record = (epoch, loss_sum/step, metric_sum/step, val_loss_sum/val_step, val_metric_sum/val_step)\n",
    "        df_history.loc[epoch - 1] = record\n",
    "\n",
    "        # 打印epoch级别的日志\n",
    "        # print('*'*8, 'EPOCH = {} loss: {:.3f}, {}: {:.3f}'.format(\n",
    "        #        record[0], record[1], metric_name, record[2]))\n",
    "        print('EPOCH = {} loss: {:.5f}, {}: {:.5f}, val_loss: {:.5f}, val_{}: {:.5f}'.format(\n",
    "            record[0], record[1], metric_name, record[2], record[3], metric_name, record[4]))\n",
    "        printbar()\n",
    "        \n",
    "        # 监视loss的同时监视val_acc，防止loss下降而val_loss不下降\n",
    "        lr_scheduler2.step(record[4])\n",
    "        \n",
    "        # 保存模型\n",
    "        # current_acc_avg = metric_sum / step\n",
    "        current_acc_avg = val_metric_sum / val_step # 看验证集指标\n",
    "        \n",
    "        # 每隔几个epoch保存更好的模型，最后的几个epoch只要优于best_acc都保存\n",
    "        if (current_acc_avg > best_acc and epoch%6==0) or (current_acc_avg > best_acc and (epochs + 1 - epoch < 2)):\n",
    "            best_acc = current_acc_avg\n",
    "            checkpoint = train_model_save + '{:03d}_ckpt.tar'.format(epoch)\n",
    "            if device.type == 'cuda' and ngpu > 1:\n",
    "                # model_sd = model.module.state_dict()  ##################\n",
    "                model_sd = copy.deepcopy(model.module.state_dict())\n",
    "            else:\n",
    "                # model_sd = model.state_dict(),  ##################\n",
    "                model_sd = copy.deepcopy(model.state_dict())  ##################\n",
    "            torch.save({\n",
    "                'loss': loss_sum / step,\n",
    "                'epoch': epoch,\n",
    "                'net': model_sd,\n",
    "                'opt': optimizer.state_dict(),\n",
    "                'lr_scheduler': lr_scheduler.state_dict()\n",
    "            }, checkpoint)\n",
    "    \n",
    "    print('finishing training...')\n",
    "    endtime = time.time()\n",
    "    time_elapsed = endtime - starttime\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    return df_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dep_train = torch.zeros(len(train_pairs), MAX_LENGTH+1, MAX_LENGTH+1)\n",
    "dep_val = torch.zeros(len(val_pairs), MAX_LENGTH+1, MAX_LENGTH+1)\n",
    "dep_test = torch.zeros(len(test_pairs), MAX_LENGTH+1, MAX_LENGTH+1)\n",
    "\n",
    "dep_train_batch_path = data_dir + 'data_sets/dep_train_batch_pairs'\n",
    "dep_val_batch_path = data_dir + 'data_sets/dep_val_batch_pairs'\n",
    "# for step, (inp, targ) in enumerate(test_dataloader, start=1):\n",
    "    \n",
    "#     st = (step-1) * inp.shape[0]\n",
    "#     end = st + inp.shape[0]\n",
    "#     dependency_matrix = dep_test[st:end,:,:]\n",
    "    \n",
    "#     if step%1 == 0:\n",
    "#         print(step)\n",
    "#         print(inp.shape)\n",
    "#         print(targ.shape)\n",
    "#         print(dependency_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import pickle\n",
    "# # import torch\n",
    "# with open(r'/home/chengkun/jupyter_projects/Magic-NLPer-main/data/data_sets/dep_train_pairs','rb') as f:\n",
    "#     dep_train = pickle.load(f)\n",
    "#     f.close()\n",
    "\n",
    "# with open(r'/home/chengkun/jupyter_projects/Magic-NLPer-main/data/data_sets/dep_val_pairs','rb') as f:\n",
    "#     dep_val = pickle.load(f)\n",
    "#     f.close()\n",
    "    \n",
    "# # dep_test = torch.zeros(len(test_pairs), MAX_LENGTH+1, MAX_LENGTH+1)\n",
    "\n",
    "# print(dep_train.shape)\n",
    "\n",
    "\n",
    "\n",
    "# with open(r'/home/chengkun/jupyter_projects/Magic-NLPer-main/data/data_sets/dep_val_pairs','rb') as f:\n",
    "#     count  = 0\n",
    "#     while True:\n",
    "#         try:\n",
    "#             aa=pickle.load(f)\n",
    "#             count = count + 1\n",
    "#             if count%240 == 0:\n",
    "#                 print(count)\n",
    "#         except EOFError:\n",
    "#             break\n",
    "# print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** start training...\n",
      "\n",
      "================================2021-11_14 10:28:11\n",
      "***** [step = 500] loss: 6.96806, acc: 0.08206\n",
      "***** [step = 1000] loss: 6.27508, acc: 0.12009\n",
      "***** [step = 1500] loss: 5.73672, acc: 0.16330\n",
      "EPOCH = 1 loss: 5.35260, acc: 0.19684, val_loss: 3.95331, val_acc: 0.33765\n",
      "\n",
      "================================2021-11_14 10:50:35\n",
      "finishing training...\n",
      "Training complete in 22m 26s\n",
      "   epoch      loss       acc  val_loss   val_acc\n",
      "0    1.0  5.352603  0.196844  3.953307  0.337647\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "# df_history = train_model(transformer, EPOCHS, train_dataloader, val_dataloader, \n",
    "#                          dep_train_batch_path, dep_val_batch_path, print_trainstep_every)\n",
    "\n",
    "df_history = train_model(transformer, EPOCHS, train_dataloader, val_dataloader, \n",
    "                         dep_train_batch_path, dep_val, print_trainstep_every)\n",
    "\n",
    "# 保存df_history变量\n",
    "save_file = open(result_save + \"transformer_df_history.bin\", \"wb\")\n",
    "pickle.dump(df_history, save_file)\n",
    "save_file.close()\n",
    "print(df_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制训练曲线\n",
    "def plot_metric(df_history, metric):\n",
    "    plt.figure()\n",
    "\n",
    "    train_metrics = df_history[metric]\n",
    "    val_metrics = df_history['val_' + metric]  #\n",
    "\n",
    "    epochs = range(1, len(train_metrics) + 1)\n",
    "\n",
    "    plt.plot(epochs, train_metrics, 'bo--')\n",
    "    plt.plot(epochs, val_metrics, 'ro-')  #\n",
    "\n",
    "    plt.title('Training and validation ' + metric)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([\"train_\" + metric, 'val_' + metric])\n",
    "    plt.savefig(result_save + metric + '.png')  # 保存图片\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "plot_metric(df_history, 'loss')\n",
    "plot_metric(df_history, metric_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 17.评估\n",
    "这里的评估 没有额外的使用测试集测评，还是拿验证集测试的。另外可以对一个法语句子进行翻译，看看翻译的结果如何\n",
    "\n",
    "以下步骤用于评估：\n",
    "- 用法语分词器（tokenizer_pt）编码输入语句。此外，添加<start>和<end>标记，这样输入就与模型训练的内容相同。这是编码器输入。\n",
    "- 解码器输入为 <start> token id\n",
    "- 计算padding mask 和 look ahead mask\n",
    "- 解码器通过查看编码器输出和它自身的输出（自注意力）给出预测。\n",
    "- 选择最后一个词并计算它的 argmax。\n",
    "- 将预测的词concat到解码器输入，然后传递给解码器。\n",
    "- 在这种方法中，解码器根据它之前预测的words预测下一个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001_ckpt.tar\n",
      "checkpoint: /home/chengkun/jupyter_projects/Magic-NLPer-main/train_save/transformer_improved_decoder/001_ckpt.tar\n",
      "Loading model ...\n",
      "Model loaded ...\n"
     ]
    }
   ],
   "source": [
    "# 加载model\n",
    "a=os.listdir(train_model_save)\n",
    "a.sort()\n",
    "print(a[-1])\n",
    "checkpoint = train_model_save + a[-1]\n",
    "print('checkpoint:', checkpoint)\n",
    "\n",
    "# ckpt = torch.load(checkpoint, map_location=device)  # dict  save 在 CPU 加载到GPU\n",
    "ckpt = torch.load(checkpoint)  # dict  save 在 GPU 加载到 GPU\n",
    "# print('ckpt', ckpt)\n",
    "transformer_sd = ckpt['net']\n",
    "# optimizer_sd = ckpt['opt'] # 不重新训练的话不需要\n",
    "# lr_scheduler_sd = ckpt['lr_scheduler']\n",
    "\n",
    "reload_model = mn_Transformer(num_layers,\n",
    "                           d_model,\n",
    "                           num_heads,\n",
    "                           dff,\n",
    "                           input_vocab_size,\n",
    "                           target_vocab_size,\n",
    "                           pe_input=input_vocab_size,\n",
    "                           pe_target=target_vocab_size,\n",
    "                           rate=dropout_rate)\n",
    "\n",
    "reload_model = reload_model.to(device)\n",
    "# reload_model.load_state_dict(transformer_sd)\n",
    "\n",
    "if ngpu > 1:\n",
    "    reload_model = torch.nn.DataParallel(reload_model,  device_ids=list(range(ngpu))) # 设置并行执行  device_ids=[0,1]\n",
    "\n",
    "\n",
    "print('Loading model ...')\n",
    "if device.type == 'cuda' and ngpu > 1:\n",
    "   reload_model.module.load_state_dict(transformer_sd)\n",
    "else:\n",
    "   reload_model.load_state_dict(transformer_sd)\n",
    "print('Model loaded ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** final test...\n",
      "******** Test: loss: 3.96409, test_acc: 0.33827\n"
     ]
    }
   ],
   "source": [
    "def test(model, dataloader, dep):\n",
    "    # model.eval() # 设置为eval mode\n",
    "\n",
    "    test_loss_sum = 0.\n",
    "    test_metric_sum = 0.\n",
    "    for test_step, (inp, targ) in enumerate(dataloader, start=1):\n",
    "        # inp [64, 10] , targ [64, 10]\n",
    "        \n",
    "        st = (test_step-1) * inp.shape[0]\n",
    "        end = st + inp.shape[0]\n",
    "        dependency_matrix = dep[st:end,:,:]\n",
    "        \n",
    "#         print('inp.shape',inp.shape)\n",
    "#         print('targ.shape',targ.shape)\n",
    "#         print('dependency_matrix.shape',dependency_matrix.shape)\n",
    "        \n",
    "        loss, metric = validate_step(model, inp, targ, dependency_matrix)\n",
    "        # print('*'*8, loss, metric)\n",
    "\n",
    "        test_loss_sum += loss\n",
    "        test_metric_sum += metric\n",
    "    # 打印\n",
    "    print('*' * 8,\n",
    "          'Test: loss: {:.5f}, {}: {:.5f}'.format(test_loss_sum / test_step, 'test_acc', test_metric_sum / test_step))\n",
    "\n",
    "\n",
    "# 在测试集上测试指标，这里使用val_dataloader模拟测试集\n",
    "print('*' * 8, 'final test...')\n",
    "test(reload_model, test_dataloader, dep_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 916, 32, 5, 838, 262, 170, 242, 2675, 39, 30, 32, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "<end> ᠶ᠋ᠢᠨ ᠦ᠋ᠳ ᠤᠯᠤᠰ ᠶᠡᠬᠡ ᠪᠠᠨ ᠦ᠌ ᠬᠠᠤᠯᠢ <start> .\n",
      "<end> 的 局 业 限 司 是 上 <start> .\n"
     ]
    }
   ],
   "source": [
    "def tokenizer_encode(tokenize, sentence, vocab):\n",
    "    # print(type(vocab)) # torchtext.vocab.Vocab\n",
    "    # print(len(vocab))\n",
    "    sentence = normalizeString(sentence)\n",
    "    # print(type(sentence)) # str\n",
    "    sentence = tokenize(sentence)  # list\n",
    "#     print(sentence)\n",
    "    sentence = ['<start>'] + sentence + ['<end>']\n",
    "    if len(sentence) < MAX_LENGTH + 2:\n",
    "        sentence = sentence + (MAX_LENGTH + 2 - len(sentence)) * ['<pad>']\n",
    "#     print(sentence)\n",
    "    sentence_ids = [vocab.stoi[token] for token in sentence]\n",
    "    # print(sentence_ids, type(sentence_ids[0])) # int\n",
    "    return sentence_ids\n",
    "\n",
    "\n",
    "def tokenzier_decode(sentence_ids, vocab):\n",
    "    sentence = [vocab.itos[id] for id in sentence_ids if id<len(vocab)]\n",
    "    # print(sentence)\n",
    "    return \" \".join(sentence)\n",
    "\n",
    "# 只有一个句子，不需要加pad\n",
    "s = 'ᠲᠦᠩᠯᠢᠶᠣᠣ ᠬᠣᠲᠠ ᠶ᠋ᠢᠨ ᡁᠢ ᠶᠤᠸᠠᠨ ᠠᠱᠢᠨ ᠲᠡᠷᠭᠡ ᠲᠦᠷᠢᠶᠡᠰᠦᠯᠡᠭᠦᠯᠬᠦ ᠬᠢᠵᠠᠭᠠᠷᠲᠤ ᠺᠣᠮᠫᠠᠨᠢ ᠬᠣᠲᠠ'\n",
    "# print(len(s.split()))\n",
    "print(tokenizer_encode(tokenizer, s, SRC_TEXT.vocab))\n",
    "\n",
    "\n",
    "s_ids = [3, 5, 251, 17, 46, 35, 12, 36, 4, 2]\n",
    "print(tokenzier_decode(s_ids, SRC_TEXT.vocab))\n",
    "print(tokenzier_decode(s_ids, TARG_TEXT.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real target: 内 蒙 古 盛 和 汽 车 服 务 有 限 公 司\n",
      "pred_sentence: <start> 内 蒙 古 汽 车 服 务 有 限 公 司\n"
     ]
    }
   ],
   "source": [
    "# inp_sentence 一个法语句子，例如\"je pars en vacances pour quelques jours .\"\n",
    "def evaluate(model, inp_sentence, dependency_matrix):\n",
    "    model.eval()  # 设置eval mode\n",
    "\n",
    "    inp_sentence_ids = tokenizer_encode(tokenizer, inp_sentence, SRC_TEXT.vocab)  # 转化为索引\n",
    "#     print(tokenzier_decode(inp_sentence_ids, SRC_TEXT.vocab))\n",
    "    encoder_input = torch.tensor(inp_sentence_ids).unsqueeze(dim=0)  # =>[b=1, inp_seq_len=10]\n",
    "#     print('encoder_input.shape：', encoder_input.shape)\n",
    "\n",
    "    decoder_input = [TARG_TEXT.vocab.stoi['<start>']]\n",
    "#     print(decoder_input)\n",
    "    decoder_input = torch.tensor(decoder_input).unsqueeze(0)  # =>[b=1,seq_len=1]\n",
    "#     print('decoder_input.shape：', decoder_input.shape)\n",
    "#     print(MAX_LENGTH)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(MAX_LENGTH + 2):\n",
    "            enc_padding_mask, combined_mask, dec_padding_mask = create_mask(encoder_input.cpu(), decoder_input.cpu())\n",
    "            # [b,1,1,inp_seq_len], [b,1,targ_seq_len,inp_seq_len], [b,1,1,inp_seq_len]\n",
    "\n",
    "            encoder_input = encoder_input.to(device)\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            enc_padding_mask = enc_padding_mask.to(device)\n",
    "            combined_mask = combined_mask.to(device)\n",
    "            dec_padding_mask = dec_padding_mask.to(device)\n",
    "            dependency_matrix = dependency_matrix.to(device)\n",
    "\n",
    "            # forward\n",
    "            \n",
    "#             print('encoder_input.shape',encoder_input.shape)\n",
    "#             print('decoder_input.shape',decoder_input.shape)\n",
    "#             print('dependency_matrix.shape',dependency_matrix.shape)\n",
    "            \n",
    "            predictions, attention_weights = model(encoder_input,\n",
    "                                                   decoder_input,\n",
    "                                                   dependency_matrix,\n",
    "                                                   enc_padding_mask,\n",
    "                                                   combined_mask,\n",
    "                                                   dec_padding_mask)\n",
    "            # [b=1, targ_seq_len, target_vocab_size]\n",
    "            # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "            #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "\n",
    "            # 看最后一个词并计算它的 argmax\n",
    "            prediction = predictions[:, -1:, :]  # =>[b=1, 1, target_vocab_size]\n",
    "            prediction_id = torch.argmax(prediction, dim=-1)  # => [b=1, 1]\n",
    "            # print('prediction_id:', prediction_id, prediction_id.dtype) # torch.int64\n",
    "            if prediction_id.squeeze().item() == TARG_TEXT.vocab.stoi['<end>']:\n",
    "                return decoder_input.squeeze(dim=0), attention_weights\n",
    "\n",
    "            decoder_input = torch.cat([decoder_input, prediction_id],\n",
    "                                      dim=-1)  # [b=1,targ_seq_len=1]=>[b=1,targ_seq_len=2]\n",
    "            # decoder_input在逐渐变长\n",
    "\n",
    "    return decoder_input.squeeze(dim=0), attention_weights\n",
    "    # [targ_seq_len],\n",
    "    # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "    #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "\n",
    "\n",
    "\n",
    "# s = 'je pars en vacances pour quelques jours .'\n",
    "# evaluate(s)\n",
    "\n",
    "\n",
    "# s = 'I want to know now .'\n",
    "# s_targ = 'Je veux le savoir maintenant.'\n",
    "# 翻译时不提供外部知识，因为目标句子不参与翻译，因此dependency_matrix设置为0\n",
    "s = 'ᠥᠪᠥᠷ ᠣᠩᠭᠣᠯ ᠤ᠋ᠨ ᠱᠸᠩ ᠾᠧ ᠠᠱᠢᠨ ᠲᠡᠷᠭᠡᠨ ᠦ᠌ ᠦᠢᠯᠡᠴᠢᠯᠡᠭᠡ ᠶ᠋ᠢᠨ ᠬᠢᠵᠠᠭᠠᠷᠲᠤ ᠺᠣᠮᠫᠠᠨᠢ'\n",
    "s_targ = '内 蒙 古 盛 和 汽 车 服 务 有 限 公 司'\n",
    "dep = torch.zeros(1)\n",
    "\n",
    "pred_result, attention_weights = evaluate(reload_model, s, dep)\n",
    "pred_sentence = tokenzier_decode(pred_result, TARG_TEXT.vocab)\n",
    "print('real target:', s_targ)\n",
    "print('pred_sentence:', pred_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 批量翻译\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "dep = torch.zeros(1)\n",
    "sentence_pairs = test_pairs\n",
    "print('测试集句子数目：', len(sentence_pairs))\n",
    "\n",
    "# total_score = 0\n",
    "number = 0.0\n",
    "f1 = open(result_save + 'target1.txt','w', encoding='utf-8')\n",
    "f2 = open(result_save + 'pred1.txt','w', encoding='utf-8')\n",
    "for pair in sentence_pairs:\n",
    "    pred_result, _ = evaluate(reload_model, pair[0],dep)\n",
    "    pred_sentence = tokenzier_decode(pred_result, TARG_TEXT.vocab).replace('<start>','').replace('<end>','').replace('\\n','')\n",
    "\n",
    "    f1.write(pair[1] + '\\n')\n",
    "    f2.write(pred_sentence + '\\n')\n",
    "\n",
    "    number = number + 1\n",
    "    if number%1000 ==0:\n",
    "        print(number)\n",
    "\n",
    "f1.close()\n",
    "f2.close()\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ᠱᠢᠯᠢ ᠶᠢᠨ ᠭᠣᠣᠯ ᠠᠢᠮᠠᠭ ᠤ᠋ᠨ ᠷᠦᠩ ᠳᠡ ᠹᠠᠩ ᠹᠠ ᠬᠢᠵᠠᠭᠠᠷᠲᠤ ᠬᠠᠷᠢᠭᠤᠴᠠᠯᠭᠠᠲᠤ ᠺᠣᠮᠫᠠᠨᠢ\n",
      "target: 锡 林 郭 勒 盟 荣 达 房 发 有 限 责 任 公 司\n",
      "pred: <start> 阿 拉 善 盟 盟 盟 林 业 有 限 责 任 公 司\n",
      "\n",
      "input: ᠡᠦᠲᠦ ᠦᠶ᠎ᠡ ᠶ᠋ᠢᠨ ᠡᠨᠧᠷᠭᠢ ᠶ᠋ᠢᠨ ᠠᠵᠤ ᠠᠬᠤᠢ ᠬᠡᠮᠡᠬᠦ ᠡᠨᠡᠬᠦ ᠬᠠᠭᠤᠳᠠᠰᠤ ᠵᠣᠬᠢᠶᠠᠯ ᠢ᠋ ᠰᠠᠢᠲᠤᠷ ᠵᠣᠬᠢᠶᠠᠵᠤ ᠣᠯᠠᠨ ᠡᠱᠢᠲᠦ ᠬᠥᠭᠵᠢᠯᠲᠡ ᠣᠯᠠᠨ ᠲᠤᠢᠯᠱᠢᠷᠠᠯ ᠤ᠋ᠨ ᠲᠤᠯᠭᠠᠭᠤᠷᠢᠯᠠᠯ ᠲᠠᠢ ᠣᠳᠣ ᠦᠶ᠎ᠡ ᠶ᠋ᠢᠨ ᠠᠵᠤ ᠦᠢᠯᠡᠰ ᠤ᠋ᠨ ᠱᠢᠨ᠎ᠡ ᠲᠣᠭᠲᠠᠯᠴᠠᠭ᠎ᠠ ᠶ᠋ᠢ ᠴᠢᠷᠮᠠᠢᠨ ᠴᠣᠭᠴᠠᠯᠠᠶ᠎ᠠ\n",
      "target: 做 好 现 代 能 源 经 济 这 篇 文 章 . 努 力 构 建 多 元 发 展 . 多 极 支 撑 的 现 代 产 业 新 体 系\n",
      "pred: <start> 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现 现\n",
      "\n",
      "input: ᠲᠡᠷᠡ ᠤᠳᠠᠲᠠᠯ᠎ᠠ ᠬᠠᠷᠠᠭᠳᠠᠭᠰᠠᠨ ᠦᠭᠡᠢ ᠪᠣᠯᠬᠣᠷ ᠪᠢ ᠶᠡᠬᠡᠳᠡ ᠰᠠᠨᠠᠭ᠎ᠠ ᠵᠣᠪᠠᠵᠤ ᠪᠠᠢᠨ᠎ᠠ .\n",
      "target: 她 好 久 不 露 面 叫 我 担 心 死 了 .\n",
      "pred: <start> 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我\n",
      "\n",
      "input: ᠪᠠᠲᠤᠯ ᠴᠢᠩᠰᠠᠩ . ᠲᠣᠭᠣᠭᠠᠨ ᠲᠠᠢᠱᠢ . ᠡᠰᠡᠨ ᠲᠠᠢᠱᠢ ᠶ᠋ᠢᠨ ᠦᠶ᠎ᠡ ᠳ᠋ᠦ᠍\n",
      "target: 巴 图 拉 丞 相 . 托 欢 太 师 . 也 先 太 师 时 期 .\n",
      "pred: <start> 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔 喀 尔\n",
      "\n",
      "input: ᠹᠢᠯᠢᠮ ᠢ᠋ᠶ᠋ᠡᠨ ᠣᠷᠢᠶᠠᠵᠤ ᠠᠪᠳᠠᠭ ᠨᠢ ᠡᠢᠮᠦ ᠵᠢᠷᠤᠭ ᠤ᠋ᠨ ᠠᠱᠢᠨ ᠶ᠋ᠢᠨ ᠪᠠᠰᠠ ᠨᠢᠭᠡ ᠣᠨᠴᠠᠯᠢᠭ .\n",
      "target: 这 种 相 机 的 另 一 特 点 是 自 动 卷 片 .\n",
      "pred: <start> 这 个 一 个 特 色 列 车 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 随机选个句子翻译\n",
    "dep = torch.zeros(1)\n",
    "def evaluateRandomly(n=5):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('input:', pair[0])\n",
    "        print('target:', pair[1])\n",
    "        pred_result, attentions = evaluate(reload_model, pair[0],dep)\n",
    "        pred_sentence = tokenzier_decode(pred_result, TARG_TEXT.vocab)\n",
    "        print('pred:', pred_sentence)\n",
    "        print('')\n",
    "\n",
    "\n",
    "evaluateRandomly(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 18.attention 的可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 可视化attenton 这里我们只展示...block2的attention，即[b, num_heads, targ_seq_len, inp_seq_len]\n",
    "# attention: {'decoder_layer{i + 1}_block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "#             'decoder_layer{i + 1}_block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "# sentence: [seq_len]，例如：'je recherche un assistant .'\n",
    "# pred_result: [seq_len]，例如：'<start> i m looking for an assistant .'\n",
    "# layer: 字符串类型，表示模型decoder的N层decoder-layer的第几层的attention，形如'decoder_layer{i}_block1'或'decoder_layer{i}_block2'\n",
    "def plot_attention_weights(attention, sentence, pred_sentence, layer):\n",
    "    \n",
    "    # block2 attention[layer] => [b=1, num_heads, targ_seq_len, inp_seq_len]\n",
    "    attention = torch.squeeze(attention[layer], dim=0) # => [num_heads, targ_seq_len, inp_seq_len]\n",
    "#     print(attention.shape)\n",
    "\n",
    "    # print(matplotlib.matplotlib_fname())\n",
    "    plt.rcParams['font.sans-serif'] = ['Mongolian Baiti']\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    \n",
    "    sentence = sentence.split()\n",
    "    pred_sentence = pred_sentence.split()\n",
    "\n",
    "    fig = plt.figure(figsize=(len(pred_sentence), len(sentence)+2)) #figsize=(attention.shape[1], attention.shape[2])\n",
    "    \n",
    "    for head in range(attention.shape[0]):\n",
    "        ax = fig.add_subplot(2, 4, head + 1)  # 111是单个整数编码的子绘图网格参数。例如，“111”表示“1×1网格，第一子图”，“234”表示“2×3网格，第四子图”\n",
    "\n",
    "        cax = ax.matshow(attention[head].cpu(), cmap='viridis')  # 绘制网格热图？注意力权重\n",
    "        # fig.colorbar(cax)#给子图添加colorbar（颜色条或渐变色条）\n",
    "\n",
    "        fontdict = {'fontsize': 10}\n",
    "\n",
    "        # 设置轴刻度线\n",
    "        ax.set_xticks(range(len(sentence)+2))  # 算上start和end\n",
    "        ax.set_yticks(range(len(pred_sentence)))\n",
    "\n",
    "        ax.set_xlim(-0.5,len(sentence) + 0.5)  # 设定x座标轴的范围\n",
    "        ax.set_ylim(len(pred_sentence) - 1.5, -0.5)  # 设定y座标轴的范围\n",
    "\n",
    "        # 设置轴\n",
    "        ax.set_xticklabels(['<start>']+sentence+['<end>'], fontdict=fontdict, rotation=90)  # 顺时间旋转90度\n",
    "        ax.set_yticklabels(pred_sentence, fontdict=fontdict, family = 'SimHei')\n",
    "\n",
    "        ax.set_xlabel('Head {}'.format(head + 1))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(result_save+'attention_{}.pdf'.format(layer),format='pdf',bbox_inches='tight')\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "def translate(sentence_pair, dep, plot=None):\n",
    "    print('input:', sentence_pair[0])\n",
    "    print('target:', sentence_pair[1])\n",
    "    pred_result, attention_weights = evaluate(reload_model, sentence_pair[0],dep)\n",
    "    print('attention_weights:', attention_weights.keys())\n",
    "    pred_sentence = tokenzier_decode(pred_result, TARG_TEXT.vocab)\n",
    "    print('pred:', pred_sentence)\n",
    "    print('')\n",
    "\n",
    "    if plot:\n",
    "        plot_attention_weights(attention_weights, sentence_pair[0], pred_sentence, plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ᠵᠠᠷᠯᠠᠨ ᠨᠡᠢᠲᠡᠯᠡᠬᠦ ᠬᠤᠭᠤᠴᠠᠭ᠎ᠠ ᠨᠢ ᠬᠣᠷᠢᠨ ᠡᠳᠦᠷ ᠡᠴᠡ ᠳᠤᠲᠠᠭᠤ ᠪᠠᠢᠵᠤ ᠪᠣᠯᠬᠤ ᠦᠭᠡᠢ\n",
      "target: 公 示 时 间 不 得 少 于 二 十 日\n",
      "attention_weights: dict_keys(['decoder_layer1_block1', 'decoder_layer1_block2', 'decoder_layer2_block1', 'decoder_layer2_block2', 'decoder_layer3_block1', 'decoder_layer3_block2', 'decoder_layer4_block1', 'decoder_layer4_block2'])\n",
      "pred: <start> 第 二 十 二 条 不 得 . 不 得 期 期 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x7344 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dep = torch.zeros(1)\n",
    "# translate(sentence_pairs[481], dep, 'decoder_layer4_block2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# translate(sentence_pairs[2], plot='decoder_layer4_block2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = os.popen('/home/chengkun/moses/mosesdecoder/scripts/generic/multi-bleu.perl -lc /home/chengkun/jupyter_projects/Magic-NLPer-main/best_save/{}/target1.txt < /home/chengkun/jupyter_projects/Magic-NLPer-main/best_save/{}/pred1.txt'.format(begin_time,begin_time))\n",
    "print(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = []\n",
    "with open('/home/chengkun/jupyter_projects/Magic-NLPer-main/best_save/{}/target1.txt'.format(begin_time),'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        target.append(line.strip('\\n'))#去掉列表中每一个元素的换行符\n",
    "target = [target]\n",
    "f.close()\n",
    "\n",
    "pred = []\n",
    "with open('/home/chengkun/jupyter_projects/Magic-NLPer-main/best_save/{}/pred1.txt'.format(begin_time),'r', encoding='utf-8') as f1:\n",
    "    for line in f1.readlines():\n",
    "        pred.append(line.strip('\\n'))#去掉列表中每一个元素的换行符\n",
    "\n",
    "f1.close()\n",
    "\n",
    "\n",
    "bleu = sacrebleu.corpus_bleu(pred, target,smooth_method='none')\n",
    "bleu_score = format(bleu.score,'.3f')\n",
    "print(bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.system('cp  {} {}'.format(checkpoint,result_save))\n",
    "os.system('cp  {} {}'.format('/home/chengkun/jupyter_projects/Magic-NLPer-main/re/primer-ez/pytask-primer.log',result_save))\n",
    "os.system('cp  {} {}'.format('/home/chengkun/jupyter_projects/Magic-NLPer-main/train_save/transformer_improved_decoder/*',result_save))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep = torch.zeros(1)\n",
    "translate(sentence_pairs[481], dep, 'decoder_layer5_block2')\n",
    "translate(sentence_pairs[481], dep, 'decoder_layer5_block1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.rename(result_save[:-1], result_save[:-1]+'_'+str(bleu_score)+'_'+str(seed)+'_'+file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "# print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 19.总结\n",
    "\n",
    "代码还有以下待完善的地方：\n",
    "- 没有实现标签平滑\n",
    "- 在训练过程中使用了teacher-forcing，即总是会将target传递到下一时间步长。更好的做法是设置一个teacher_forcing_ration\n",
    "- 在evaluate阶段的解码使用的是greedy search decode，即对于每一步，我们只需从具有最高 softmax 值的 decoder_output 中选择单词。可以尝试使用更好的beam search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
