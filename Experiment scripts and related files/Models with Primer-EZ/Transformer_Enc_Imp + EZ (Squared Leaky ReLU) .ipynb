{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "transformer的核心：self-attentions\n",
    "\n",
    "transformer **优点**：\n",
    "- 无需对跨数据的时间/空间域的关系作出假设\n",
    "- 并行计算\n",
    "- distant item 互相影响彼此的输出\n",
    "- 可以学习到长距离依赖关系\n",
    "\n",
    "transformer **缺点**：\n",
    "- 对于每一个step的xt的输出，是由整个历史信息计算得出，而不再是当前输入和hidden，这可能效率较低\n",
    "- 如果输入具有时间/空间域的关系，则需要加入位置编码，否则整个model也只能看作是一个词袋模型\n",
    "\n",
    "目录\n",
    "* [1.加载数据 建立input pipeline](#)\n",
    "* [2.位置编码 positional encoding](#)\n",
    "* [3.掩码 masking](#3)\n",
    "* [4.scaled dot product attention](#)\n",
    "* [5.multi-head attention](#)\n",
    "* [6.point wise feed forward network](#)\n",
    "* [7.encoder layer](#)\n",
    "* [8.decoder layer](#)\n",
    "* [9.encoder](#)\n",
    "* [10.decoder](#)\n",
    "* [11.搭建transformer](#)\n",
    "* [12.设置超参](#)\n",
    "* [13.优化器](#)\n",
    "* [14.损失和评价准则](#)\n",
    "* [15.生成mask](#)\n",
    "* [16.训练和保存](#)\n",
    "* [17.评估](#)\n",
    "* [18.attention的可视化](#)\n",
    "* [19.总结](#)\n",
    "\n",
    "\n",
    "## 1.加载数据 建立input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mn_t1_xiugai\n",
      "2021-11-04_13:54:01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import random\n",
    "import re\n",
    "# from tqdm import tqdm  # 进度条\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import unicodedata\n",
    "import datetime\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import sacrebleu\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import pickle\n",
    "import torch_optimizer as optim\n",
    "# import adamod\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "from labml_helpers.module import Module\n",
    "from labml_nn.transformers import MultiHeadAttention\n",
    "\n",
    "# print(os.getcwd()) # /home/xijian\n",
    "file_name = 'transformer_improved_encoder1'\n",
    "print(file_name)\n",
    "\n",
    "train_model_save = '/home/chengkun/jupyter_projects/Magic-NLPer-main/train_save/transformer_improved_encoder/'\n",
    "shutil.rmtree(train_model_save)\n",
    "os.mkdir(train_model_save)\n",
    "\n",
    "#保存必要结果\n",
    "begin_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()).replace(' ','_')\n",
    "print(begin_time)\n",
    "result_save = '/home/chengkun/jupyter_projects/Magic-NLPer-main/best_save/' + begin_time +'/'\n",
    "os.mkdir(result_save)\n",
    "\n",
    "os.system('cp  {} {} {}'.format('/home/chengkun/jupyter_projects/Magic-NLPer-main/re/primer-ez/transformer_improved_encoder1.py','/home/chengkun/jupyter_projects/Magic-NLPer-main/re/primer-ez/transformer_improved_encoder1.ipynb',result_save))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquaredReLU(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.relu = torch.nn.LeakyReLU()\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.relu(x)\n",
    "        return x * x\n",
    "\n",
    "class SpatialDepthWiseConvolution(Module):\n",
    "    def __init__(self, d_k: int, kernel_size: int = 3):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = torch.nn.Conv1d(in_channels=d_k, out_channels=d_k,\n",
    "                              kernel_size=(kernel_size,), padding=(kernel_size - 1,), groups=d_k)\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x.unsqueeze(-1).permute(1, 0, 3, 2)\n",
    "        seq_len, batch_size, heads, d_k= x.shape\n",
    "        x = x.permute(1, 2, 3, 0)\n",
    "        x = x.view(batch_size * heads, d_k, seq_len)\n",
    "        x = self.conv(x)\n",
    "        x = x[:, :, :-(self.kernel_size - 1)]\n",
    "        x = x.view(batch_size, heads, d_k, seq_len)\n",
    "        x = x.permute(0, 3, 2, 1) # [batch_size, seq_len, heads, d_k]\n",
    "        x = x.view(batch_size, seq_len, heads, d_k)\n",
    "        x = torch.squeeze(x,2)\n",
    "        return x\n",
    "\n",
    "# class MultiDConvHeadAttention(MultiHeadAttention):\n",
    "#     def __init__(self, heads: int, d_model: int, dropout_prob: float = 0.1):\n",
    "#         super().__init__(heads, d_model, dropout_prob)\n",
    "        \n",
    "#         self.query = torch.nn.Sequential(self.query, SpatialDepthWiseConvolution(self.d_k))\n",
    "#         self.key = torch.nn.Sequential(self.key, SpatialDepthWiseConvolution(self.d_k))\n",
    "#         self.value = torch.nn.Sequential(self.value, SpatialDepthWiseConvolution(self.d_k))\n",
    "        \n",
    "# m = SquaredReLU()\n",
    "# n = torch.nn.LeakyReLU()\n",
    "# input = torch.randn(2)\n",
    "# print(m(input))\n",
    "# print(n(input)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu0 = int(sys.argv[1])\n",
    "gpu1 = int(sys.argv[2])\n",
    "seed = int(sys.argv[3])\n",
    "print('gpu0:',gpu0,'gpu1:',gpu1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngpu： 2\n",
      "batch： 120\n",
      "MAX_LENGTH： 100\n",
      "EPOCHS： 60\n",
      "warm_steps： 3000\n",
      "seed： 1234\n"
     ]
    }
   ],
   "source": [
    "# 设置超参数\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"{},{}\".format(gpu0, gpu1)\n",
    "ngpu = 2\n",
    "print('ngpu：', ngpu)\n",
    "\n",
    "batch = 120\n",
    "print('batch：', batch)\n",
    "\n",
    "# MAX_LENGTH = d_model//num_heads\n",
    "# 注意，MAX_LENGTH必须小于等于源语言词表的大小\n",
    "MAX_LENGTH = 100\n",
    "\n",
    "print('MAX_LENGTH：', MAX_LENGTH)\n",
    "EPOCHS = 35\n",
    "print('EPOCHS：', EPOCHS)\n",
    "warm_steps=3000\n",
    "print('warm_steps：', warm_steps)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()  #检测是否有可用的gpu\n",
    "device = torch.device(\"cuda:0\" if (use_cuda and ngpu > 0) else \"cpu\")\n",
    "\n",
    "\n",
    "# seed = 256\n",
    "print('seed：', seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "# torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 数据读取\n",
    "# 当你用read_csv读文件的时候，如果文本里包含英文双引号，直接读取会导致行数变少或是直接如下报错停止\n",
    "# 此时应该对read_csv设置参数控制csv中的引号常量，设定quoting=3或是quoting=csv.QUOTE_NONE”（注：用第二种要先导csv库）然后问题就解决了。\n",
    "\n",
    "data_dir = '/home/chengkun/jupyter_projects/Magic-NLPer-main/data/' \n",
    "\n",
    "data_df = pd.read_csv(data_dir + 'ch_mn_50_nodict.txt',  # 数据格式：英语\\t法语，注意我们的任务源语言是法语，目标语言是英语\n",
    "                      encoding='UTF-8', sep='\\t', header=None,quoting=3,\n",
    "                      names=['mn', 'ch'], index_col=False)\n",
    "\n",
    "# print(data_df.shape)\n",
    "# print(data_df.values.shape)\n",
    "# print(data_df.values[0])\n",
    "# print(data_df.values[0].shape)\n",
    "# data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "\n",
    "# 规范化字符串\n",
    "def normalizeString(s):\n",
    "    # print(s) # list  ['Go.']\n",
    "    # s = s[0]\n",
    "    s = s.lower().strip()\n",
    "    #s = unicodeToAscii(s)\n",
    "    #s = re.sub(r\"([.!?])\", r\" \\1\", s)  # \\1表示group(1)即第一个匹配到的 即匹配到'.'或者'!'或者'?'后，一律替换成'空格.'或者'空格!'或者'空格？'\n",
    "    #s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)  # 非字母以及非.!?的其他任何字符 一律被替换成空格\n",
    "    s = re.sub(r'[\\s]+', \" \", s)  # 将出现的多个空格，都使用一个空格代替。例如：w='abc  1   23  1' 处理后：w='abc 1 23 1'\n",
    "    return s\n",
    "\n",
    "\n",
    "# print(normalizeString('Va !'))\n",
    "# print(normalizeString('Go.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pairs = [[normalizeString(s) for s in line] for line in data_df.values]\n",
    "\n",
    "# print('pairs num=', len(pairs))\n",
    "# print(pairs[0])\n",
    "# print(pairs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "经过过滤后平行语料数目为： 498921\n",
      "['ᠡᠯ\\u180eᠡ ᠰᠠᠯᠪᠤᠷᠢ ᠳ᠋ᠠᠬᠢ ᠰᠣᠯᠢᠯᠴᠠᠭ\\u180eᠠ ᠬᠠᠮᠲᠤᠷᠠᠯᠴᠠᠭ\\u180eᠠ ᠲᠠᠰᠤᠷᠠᠯᠲᠠ ᠦᠭᠡᠢ ᠥᠷᠭᠡᠳᠪᠡ .', '各 领 域 交 流 合 作 不 断 扩 大 .']\n"
     ]
    }
   ],
   "source": [
    "# 文件是英译法，我们实现的是法译英，所以进行了reverse，所以pair[1]是英语\n",
    "# 为了快速训练，仅保留“我是”“你是”“他是”等简单句子，并且删除原始文本长度大于10个标记的样本\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH \n",
    "\n",
    "def filterPairs(pairs):\n",
    "    # 过滤，并交换句子顺序，得到法英句子对（之前是英法句子对）\n",
    "    return [[pair[1], pair[0]] for pair in pairs if filterPair(pair)]\n",
    "\n",
    "\n",
    "pairs = filterPairs(pairs)\n",
    "\n",
    "print('经过过滤后平行语料数目为：', len(pairs))\n",
    "print(pairs[0])\n",
    "# print(random.choice(pairs))\n",
    "# print(np.array(pairs).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集句子数目： 479163\n",
      "验证集句子数目： 9979\n",
      "测试集句子数目： 9779\n"
     ]
    }
   ],
   "source": [
    "# 划分数据集：训练集和验证集\n",
    "##0.0338 0.03485\n",
    "##50 0.020 0.020\n",
    "train_test, val_pairs = train_test_split(pairs, test_size=0.02, random_state=1234)\n",
    "train_pairs, test_pairs = train_test_split(train_test, test_size=0.02, random_state=1234)\n",
    "\n",
    "print('训练集句子数目：', len(train_pairs))\n",
    "print('验证集句子数目：', len(val_pairs))\n",
    "print('测试集句子数目：', len(test_pairs))\n",
    "# print(test_pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = lambda x: x.split() # 分词器\n",
    "\n",
    "SRC_TEXT = torchtext.data.Field(sequential=True,\n",
    "                                tokenize=tokenizer,\n",
    "                                # lower=True,\n",
    "                                fix_length=MAX_LENGTH + 2,\n",
    "                                preprocessing=lambda x: ['<start>'] + x + ['<end>'],\n",
    "                                # after tokenizing but before numericalizing\n",
    "                                # postprocessing # after numericalizing but before the numbers are turned into a Tensor\n",
    "                                )\n",
    "TARG_TEXT = torchtext.data.Field(sequential=True,\n",
    "                                 tokenize=tokenizer,\n",
    "                                 # lower=True,\n",
    "                                 fix_length=MAX_LENGTH + 2,\n",
    "                                 preprocessing=lambda x: ['<start>'] + x + ['<end>'],\n",
    "                                 )\n",
    "\n",
    "\n",
    "def get_dataset(pairs, src, targ):\n",
    "    fields = [('src', src), ('targ', targ)]  # filed信息 fields dict[str, Field])\n",
    "    examples = []  # list(Example)\n",
    "    for mn, ch in pairs: # 进度条\n",
    "        # 创建Example时会调用field.preprocess方法\n",
    "        examples.append(torchtext.data.Example.fromlist([mn, ch], fields))\n",
    "    return examples, fields\n",
    "\n",
    "\n",
    "# examples, fields = get_dataset(pairs, SRC_TEXT, TARG_TEXT)\n",
    "\n",
    "ds_train = torchtext.data.Dataset(*get_dataset(train_pairs, SRC_TEXT, TARG_TEXT))\n",
    "ds_val = torchtext.data.Dataset(*get_dataset(val_pairs, SRC_TEXT, TARG_TEXT))\n",
    "ds_test = torchtext.data.Dataset(*get_dataset(test_pairs, SRC_TEXT, TARG_TEXT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds_train\n",
      "20 ['<start>', 'ᠴᠢᠯᠠᠭᠤᠨ', 'ᠲᠣᠰᠣ', 'ᠭᠠᠳᠠᠭᠱᠢᠯᠠᠭᠤᠯᠬᠤ', 'ᠤᠯᠤᠰ', 'ᠤ᠋ᠨ', 'ᠵᠣᠬᠢᠶᠠᠨ', 'ᠪᠠᠢᠭᠤᠯᠤᠯᠲᠠ', 'ᠴᠢᠯᠠᠭᠤᠨ', 'ᠲᠣᠰᠣ', 'ᠭᠠᠳᠠᠭᠱᠢᠯᠠᠭᠤᠯᠬᠤ', 'ᠪᠠᠨ', 'ᠵᠣᠭᠰᠣᠭᠠᠨ', '.', 'ᠦᠨ\\u180eᠡ', 'ᠥᠰᠬᠦ', 'ᠶ᠋ᠢ', 'ᠬᠦᠯᠢᠶᠡᠨ\\u180eᠡ', '.', '<end>']\n",
      "22 ['<start>', '石', '油', '输', '出', '国', '家', '组', '织', '冻', '结', '石', '油', '输', '出', '.', '价', '格', '看', '涨', '.', '<end>']\n",
      "ds_val\n",
      "25 ['<start>', 'ᠴᠠᠭᠠᠨ', 'ᠰᠠᠷ\\u180eᠠ', 'ᠶ᠋ᠢᠨ', 'ᠦᠶ\\u180eᠡ', 'ᠪᠡᠷ', 'ᠬᠥᠮᠦᠰ', 'ᠬᠠᠭᠤᠴᠢᠨ', 'ᠶᠣᠰᠣ', 'ᠵᠠᠩᠱᠢᠯ', 'ᠢ᠋ᠶ᠋ᠠᠨ', 'ᠪᠠᠷᠢᠮᠲᠠᠯᠠᠵᠤ', 'ᠪᠠᠢᠭ\\u180eᠠ', 'ᠪᠥᠭᠡᠳ', 'ᠤᠯᠠᠮ', 'ᠢ᠋ᠶ᠋ᠠᠷ', 'ᠱᠢᠨ\\u180eᠡ', 'ᠵᠦᠢᠯ', 'ᠢ᠋', 'ᠡᠷᠢᠨ', 'ᠬᠠᠢᠬᠤ', 'ᠪᠣᠯᠵᠤ', 'ᠪᠠᠢᠨ\\u180eᠠ', '.', '<end>']\n",
      "35 ['<start>', '人', '们', '在', '固', '守', '原', '有', '过', '年', '消', '费', '方', '式', '的', '同', '时', '.', '也', '在', '欣', '赏', '.', '渴', '望', '着', '浪', '漫', '的', '时', '尚', '情', '趣', '.', '<end>']\n",
      "ds_test\n",
      "13 ['<start>', 'ᠵᠠᠷᠯᠠᠨ', 'ᠨᠡᠢᠲᠡᠯᠡᠬᠦ', 'ᠬᠤᠭᠤᠴᠠᠭ\\u180eᠠ', 'ᠨᠢ', 'ᠬᠣᠷᠢᠨ', 'ᠡᠳᠦᠷ', 'ᠡᠴᠡ', 'ᠳᠤᠲᠠᠭᠤ', 'ᠪᠠᠢᠵᠤ', 'ᠪᠣᠯᠬᠤ', 'ᠦᠭᠡᠢ', '<end>']\n",
      "13 ['<start>', '公', '示', '时', '间', '不', '得', '少', '于', '二', '十', '日', '<end>']\n"
     ]
    }
   ],
   "source": [
    "# # 查看1个样本的信息\n",
    "print('ds_train')\n",
    "print(len(ds_train[0].src), ds_train[0].src)\n",
    "print(len(ds_train[0].targ), ds_train[0].targ)\n",
    "print('ds_val')\n",
    "print(len(ds_val[0].src), ds_val[0].src)\n",
    "print(len(ds_val[0].targ), ds_val[0].targ)\n",
    "print('ds_test')\n",
    "print(len(ds_test[0].src), ds_test[0].src)\n",
    "print(len(ds_test[0].targ), ds_test[0].targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型大小与词表大小正相关，控制词表大小\n",
      "0-10：<unk> <pad> . <end> <start> ᠶ᠋ᠢᠨ ᠤ᠋ᠨ ᠤ᠋ ᠦ᠋ᠨ ᠶ᠋ᠢ\n",
      "\n",
      "input_vocab_size： 110560\n",
      "target_vocab_size： 6085\n"
     ]
    }
   ],
   "source": [
    "# 构建词典\n",
    "print('模型大小与词表大小正相关，控制词表大小')\n",
    "SRC_TEXT.build_vocab(ds_train)  # 建立词表 并建立token和ID的映射关系\n",
    "# print(len(SRC_TEXT.vocab))\n",
    "# print(SRC_TEXT.vocab.itos[0])\n",
    "# print(SRC_TEXT.vocab.itos[1])\n",
    "# print(SRC_TEXT.vocab.itos[2])\n",
    "# print(SRC_TEXT.vocab.itos[3])\n",
    "# print(SRC_TEXT.vocab.stoi['<start>'])\n",
    "# print(SRC_TEXT.vocab.stoi['<end>'])\n",
    "\n",
    "# 模拟decode\n",
    "res = []\n",
    "for id in range(10):\n",
    "    res.append(SRC_TEXT.vocab.itos[id])\n",
    "print('0-10：'+' '.join(res)+'\\n')\n",
    "\n",
    "TARG_TEXT.build_vocab(ds_train,min_freq=1)\n",
    "# print(len(TARG_TEXT.vocab))\n",
    "# print(TARG_TEXT.vocab.itos[0])\n",
    "# print(TARG_TEXT.vocab.itos[1])\n",
    "# print(TARG_TEXT.vocab.itos[2])\n",
    "# print(TARG_TEXT.vocab.itos[3])\n",
    "# print(TARG_TEXT.vocab.stoi['<start>'])\n",
    "# print(TARG_TEXT.vocab.stoi['<end>'])\n",
    "\n",
    "input_vocab_size = len(SRC_TEXT.vocab)\n",
    "target_vocab_size = len(TARG_TEXT.vocab)\n",
    "\n",
    "print('input_vocab_size：', input_vocab_size)\n",
    "print('target_vocab_size：', target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = batch * ngpu\n",
    "\n",
    "# 构建数据管道迭代器\n",
    "train_iter, val_iter, test_iter= torchtext.data.Iterator.splits(\n",
    "    (ds_train, ds_val, ds_test),\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x.src),\n",
    "    batch_sizes=(BATCH_SIZE, BATCH_SIZE, BATCH_SIZE)\n",
    ")\n",
    "\n",
    "\n",
    "# # 查看数据管道信息，此时会触发postprocessing，如果有的话\n",
    "# for batch in train_iter:\n",
    "#     # 注意，这里text第0维不是batch，而是seq_len\n",
    "#     print(batch.src[:,0])\n",
    "#     print(batch.src.shape, batch.targ.shape)  # [12,64], [12,64]\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 将数据管道组织成与torch.utils.data.DataLoader相似的inputs, targets的输出形式\n",
    "class DataLoader:\n",
    "    def __init__(self, data_iter):\n",
    "        self.data_iter = data_iter\n",
    "        self.length = len(data_iter)  # 一共有多少个batch？\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __iter__(self):\n",
    "        # 注意，在此处调整text的shape为batch first\n",
    "        for batch in self.data_iter:\n",
    "            yield (torch.transpose(batch.src, 0, 1), torch.transpose(batch.targ, 0, 1))\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_iter)\n",
    "val_dataloader = DataLoader(val_iter)\n",
    "test_dataloader = DataLoader(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_dataloader): 4\n",
      "torch.Size([200, 32]) torch.Size([200, 32])\n",
      "tensor([   3,  880,   55,   10,  162,    9, 1265,    8,  753, 1998,  434,  394,\n",
      "          61,  355,    4,  206,    6,  424,    8,  246,  567,    6,   53,  167,\n",
      "           8,  434,  394,   45,  569, 2608,    2,    1]) torch.int64\n",
      "tensor([   3,  285,  118,  202,   47,  345,   16, 1276,   10,   57,  122,  160,\n",
      "         167,   48,  186,   20,   98,   23,   57,  522,  419,   42,   33,   65,\n",
      "          48,  186,  123,   93,    2,    1,    1,    1]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# 查看数据管道\n",
    "print('len(train_dataloader):', len(train_dataloader))  # 句子总数/batch数\n",
    "print('len(val_dataloader):', len(val_dataloader))  # 句子总数/batch数\n",
    "print('len(test_dataloader):', len(test_dataloader))  # 句子总数/batch数\n",
    "# print('len(train_dataloader):', len(train_dataloader))  # 34 个step/batch\n",
    "# for batch_src, batch_targ in train_dataloader:\n",
    "#     print(batch_src.shape, batch_targ.shape)  # [256,12], [256,12]\n",
    "#     print(batch_src[0], batch_src.dtype)\n",
    "#     print(batch_targ[0], batch_targ.dtype)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.位置编码 positional encoding\n",
    "绝对位置编码\n",
    "\n",
    "由于model中不含有任何recurrence or convolution，所以句子中token的相对位置关系无法体现，\n",
    "所以就需要在embedding vector中加入position encoding vector（维度相同）。这样每个词的词向量\n",
    "在 $d_{model}$ 维的空间中，就可以基于meaning和position来计算相似度或相关性\n",
    "\n",
    "$$\\begin{array}{ll} & PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}}) \\\\ & PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})\\end{array}$$\n",
    "\n",
    "**特点：**\n",
    "- （1）后面位置是前面位置的线性组合，保证了即使位置不是相邻的，也可能有关系（[参考这里](#https://blog.csdn.net/zhulinniao/article/details/104462228/)）\n",
    "- （2）每个位置的编码又是独特的\n",
    "- （3）每两个位置的encoding互相做点积，位置越远，点积的值越小，自己和自己点积，值最大\n",
    "\n",
    "![jupyter-img1](./imgs/im1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 512])\n"
     ]
    }
   ],
   "source": [
    "# 计算角度：pos * 1/(10000^(2i/d))\n",
    "def get_angles(pos, i, d_model):\n",
    "    # 2*(i//2)保证了2i，这部分计算的是1/10000^(2i/d)\n",
    "    angle_rates = 1 / np.power(10000, 2 * (i // 2) / np.float32(d_model))  # => [1, 512]\n",
    "    return pos * angle_rates  # [50,1]*[1,512]=>[50, 512]\n",
    "\n",
    "\n",
    "# np.arange()函数返回一个有终点和起点的固定步长的排列，如[1,2,3,4,5]，起点是1，终点是5，步长为1\n",
    "# 注意：起点终点是左开右闭区间，即start=1,end=6，才会产生[1,2,3,4,5]\n",
    "# 只有一个参数时，参数值为终点，起点取默认值0，步长取默认值1。\n",
    "def positional_encoding(position, d_model):  # d_model是位置编码的长度，相当于position encoding的embedding_dim？\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],  # [50, 1]\n",
    "                            np.arange(d_model)[np.newaxis, :],  # [1, d_model=512]\n",
    "                            d_model)\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])  # 2i\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])  # 2i+2\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]  # [50,512]=>[1,50,512]\n",
    "    return torch.tensor(pos_encoding, dtype=torch.float32)\n",
    "\n",
    "pos_encoding = positional_encoding(50, 512)\n",
    "print(pos_encoding[:,:2,:].shape) # [1,50,512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def draw_pos_encoding(pos_encoding):\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(pos_encoding[0], cmap='RdBu') # 绘制分类图\n",
    "    plt.xlabel('Depth')\n",
    "    plt.xlim((0, 512))\n",
    "    plt.ylabel('Position')\n",
    "    plt.colorbar() # 条形bar颜色图例\n",
    "    plt.savefig(result_save+'pos_encoding.png')\n",
    "    #plt.show()\n",
    "\n",
    "draw_pos_encoding(pos_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.掩码 masking\n",
    "这里用到的mask有2种：\n",
    "- padding mask：mask pad，即句子中为pad(value=0)的位置处其mask值为1\n",
    "- look-ahead mask：mask future token，将当前token后面的词mask掉，只让看到前面的词，即future token位置的mask值为1\n",
    "\n",
    "**【注意】：** 因为我这里使用的是torchtext里的tokenizer,从前面可以看出它的词表里pad的index=1，而不是常规的0。\n",
    "这里要特别注意，不然容易出错！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "mask 表示掩码，它对某些值进行掩盖，使其在参数更新时不产生效果。\n",
    "Transformer 模型里面涉及两种 mask，分别是 padding mask 和 sequence mask。\n",
    "其中，padding mask 在所有的 scaled dot-product attention 里面都需要用到，\n",
    "而 sequence mask 只有在 decoder 的 self-attention 里面用到。\n",
    "'''\n",
    "# 需要mask的位置，替换为1，seq：sentence_len x d_model \n",
    "#返回值为：sentence_len x 1 x 1 x d_model 为什么要扩展维度\n",
    "\n",
    "pad = 1 # 重要！\n",
    "def create_padding_mask(seq):  # seq [b, seq_len]\n",
    "#     seq = torch.eq(seq, torch.tensor(0)).float() # pad=0的情况\n",
    "    seq = torch.eq(seq, torch.tensor(pad)).float()  # pad!=0\n",
    "    return seq[:, np.newaxis, np.newaxis, :]  # =>[b, 1, 1, seq_len]\n",
    "\n",
    "# x = torch.tensor([[7, 6, 0, 0, 1],\n",
    "#                   [1, 2, 3, 0, 0],\n",
    "#                   [0, 0, 0, 4, 5]])\n",
    "# print(x.shape) # [3,5]\n",
    "# print(x)\n",
    "# mask = create_padding_mask(x)\n",
    "# print(mask.shape, mask.dtype) # [3,1,1,5]\n",
    "# print(mask)\n",
    "\n",
    "# 用train_dataloader的第一个BATCH_SIZE中的input来测试\n",
    "# for step, (inp, targ) in enumerate(train_dataloader, start=1):\n",
    "#     print(inp,'\\n',inp.shape,targ.shape)\n",
    "#     break\n",
    "\n",
    "# input_mask = create_padding_mask(inp)\n",
    "# print(input_mask,input_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = []\n",
    "# b = []\n",
    "# for step, (inp, targ) in enumerate(test_dataloader):\n",
    "#     a.append(inp.shape[1])\n",
    "#     b.append(targ.shape[1])\n",
    "# #     print(inp,'\\n',inp.shape,targ.shape)\n",
    "# #     break\n",
    "# print(a)\n",
    "# print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pad = 1 # 重要！\n",
    "# def mn_create_padding_mask(seq):  # seq [b, seq_len]\n",
    "#     if seq.shape[1] < 10:\n",
    "#         d = torch.ones([seq.shape[0],10-seq.shape[1]])\n",
    "#         seq = torch.cat((seq, d), dim=1)\n",
    "#     print(seq)\n",
    "#     # seq = torch.eq(seq, torch.tensor(0)).float() # pad=0的情况\n",
    "#     seq = torch.eq(seq, torch.tensor(pad)).float()  # pad!=0\n",
    "#     return seq[:, np.newaxis, np.newaxis, :]  # =>[b, 1, 1, seq_len]\n",
    "\n",
    "# x = torch.tensor([[7, 6, 0, 0, 1],\n",
    "#                   [1, 2, 3, 0, 0],\n",
    "#                   [0, 0, 0, 4, 5]])\n",
    "# print(x.shape) # [3,5]\n",
    "# print(x)\n",
    "# mask = mn_create_padding_mask(x)\n",
    "# print(mask.shape, mask.dtype) # [3,1,1,5]\n",
    "# print(mask)\n",
    "\n",
    "# # create two sample vectors\n",
    "# # inps = torch.randn([64, 161])\n",
    "# d = torch.ones([3,4])\n",
    "\n",
    "# # bring d into the same format, and then concatenate tensors\n",
    "# # new_inps = torch.cat((x, d.unsqueeze(2)), dim=-1)\n",
    "# new_inps = torch.cat((x,d),dim=1)\n",
    "# print(new_inps.shape)  # [64, 161, 2]\n",
    "# print(new_inps)\n",
    "# print(x.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# torch.triu(tensor, diagonal=0) 求上三角矩阵，diagonal默认为0表示主对角线的上三角矩阵\n",
    "# diagonal>0，则主对角上面的第|diagonal|条次对角线的上三角矩阵\n",
    "# diagonal<0，则主对角下面的第|diagonal|条次对角线的上三角矩阵\n",
    "#look-ahead_mask 用于对未预测的token进行掩码，这意味着要预测第三个单词，只会使用第一个和第二个单词。 \n",
    "\n",
    "def create_look_ahead_mask(size):  # seq_len\n",
    "    mask = torch.triu(torch.ones((size, size)), diagonal=1)\n",
    "    # mask = mask.device() #\n",
    "    return mask  # [seq_len, seq_len]\n",
    "\n",
    "# x = torch.rand(1,3)\n",
    "# print(x.shape)\n",
    "# print(x)\n",
    "# mask = create_look_ahead_mask(x.shape[1])\n",
    "# print(mask.shape, mask.dtype)\n",
    "# print(mask)\n",
    "\n",
    "# 用train_dataloader的第一个BATCH_SIZE中的target来测试\n",
    "# for step, (inp, targ) in enumerate(train_dataloader, start=1):\n",
    "#     print(targ,'\\n',inp.shape,targ.shape)\n",
    "#     break\n",
    "\n",
    "# look_ahead_mask = create_look_ahead_mask(targ.shape[-1])\n",
    "# print(look_ahead_mask,look_ahead_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4.scaled dot product attention\n",
    "![jupyter-img2](./imgs/im2.jpg)\n",
    "\n",
    "$$Attention(Q,K,V)=softmax_{(k)}(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "注意：实现时对mask的处理\n",
    "\n",
    "mask=1的位置是pad或者future token，乘以-1e9（-1*10^9）成为负无穷，经过softmax后会趋于0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    \"\"\"\n",
    "    #计算注意力权重。\n",
    "    q, k, v 必须具有匹配的前置维度。 且dq=dk\n",
    "    k, v 必须有匹配的倒数第二个维度，例如：seq_len_k = seq_len_v。\n",
    "    #虽然 mask 根据其类型（填充或前瞻）有不同的形状，\n",
    "    #但是 mask 必须能进行广播转换以便求和。\n",
    "\n",
    "    #参数:\n",
    "        q: 请求的形状 == (..., seq_len_q, depth)\n",
    "        k: 主键的形状 == (..., seq_len_k, depth)\n",
    "        v: 数值的形状 == (..., seq_len_v, depth_v)  seq_len_k = seq_len_v\n",
    "        mask: Float 张量，其形状能转换成\n",
    "              (..., seq_len_q, seq_len_k)。默认为None。\n",
    "    \n",
    "    # self-attention中q=k=v这点和attention不同，需要先明确\n",
    "    #q和k相似度计算是为了获取到最合适的值，也就是值的给与注意力的值\n",
    "    #softmax是为了获取这一系列相似度值的占比（这也就是所谓的权重值）\n",
    "    #加权是和v也就是本身进行加权，求和是为了获取粒度单词和完整句子之间的关系值计算\n",
    "    #那么自然而然的，所谓的qkv这一系列操作的目的就是\n",
    "    #先通过对本身的各个向量值进行相似度计算，然后通过softmax获取本身向量的权重值，在和本身进行加权计算，\n",
    "    #最后在求和，这样子就获取了一个词和本身所有的词的权重值，然后将所有词的权重值作为输入向量，这也就是所谓的自注意力机制。\n",
    "\n",
    "    #返回值:\n",
    "        #输出，注意力权重\n",
    "    \"\"\"\n",
    "    # matmul(a,b)矩阵乘:a b的最后2个维度要能做乘法，即a的最后一个维度值==b的倒数第2个纬度值，\n",
    "    # 除此之外，其他维度值必须相等或为1（为1时会广播）\n",
    "    matmul_qk = torch.matmul(q, k.transpose(-1, -2))  # 矩阵乘 =>[..., seq_len_q, seq_len_k]\n",
    "\n",
    "    # 缩放matmul_qk\n",
    "    dk = torch.tensor(k.shape[-1], dtype=torch.float32)  # k的深度dk，或叫做depth_k\n",
    "    scaled_attention_logits = matmul_qk / torch.sqrt(dk)  # [..., seq_len_q, seq_len_k]\n",
    "#     print('scaled_attention_logits：', scaled_attention_logits.shape)\n",
    "\n",
    "    # 将 mask 加入到缩放的张量上(重要！)\n",
    "    if mask is not None:  # mask: [b, 1, 1, seq_len]\n",
    "#         print('mask.shape',mask.shape)\n",
    "        # mask=1的位置是pad，乘以-1e9（-1*10^9）成为负无穷，经过softmax后会趋于0\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax 在最后一个轴（seq_len_k）上归一化\n",
    "    attention_weights = torch.nn.functional.softmax(scaled_attention_logits, dim=-1)  # [..., seq_len_q, seq_len_k]\n",
    "\n",
    "    output = torch.matmul(attention_weights, v)  # =>[..., seq_len_q, depth_v]\n",
    "    return output, attention_weights  # [..., seq_len_q, depth_v], [..., seq_len_q, seq_len_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mn_scaled_dot_product_attention(q, k, v, m, mask=None):\n",
    "    \"\"\"\n",
    "    #计算注意力权重。\n",
    "    q, k, v 必须具有匹配的前置维度。 且dq=dk\n",
    "    k, v 必须有匹配的倒数第二个维度，例如：seq_len_k = seq_len_v。\n",
    "    #虽然 mask 根据其类型（填充或前瞻）有不同的形状，\n",
    "    #但是 mask 必须能进行广播转换以便求和。\n",
    "\n",
    "    #参数:\n",
    "        q: 请求的形状 == (..., seq_len_q, depth)\n",
    "        k: 主键的形状 == (..., seq_len_k, depth)\n",
    "        v: 数值的形状 == (..., seq_len_v, depth_v)  seq_len_k = seq_len_v\n",
    "        m: 数值的形状 == (..., seq_len_k, seq_len_v) m必须是个方阵\n",
    "        mask: Float 张量，其形状能转换成\n",
    "              (..., seq_len_q, seq_len_k)。默认为None。\n",
    "\n",
    "    #返回值:\n",
    "        #输出，注意力权重\n",
    "    \"\"\"\n",
    "    # matmul(a,b)矩阵乘:a b的最后2个维度要能做乘法，即a的最后一个维度值==b的倒数第2个纬度值，\n",
    "    # 除此之外，其他维度值必须相等或为1（为1时会广播）\n",
    "    matmul_qk = torch.matmul(q, k.transpose(-1, -2))  # 矩阵乘 =>[..., seq_len_q, seq_len_k]\n",
    "\n",
    "    # 缩放matmul_qk\n",
    "    dk = torch.tensor(k.shape[-1], dtype=torch.float32)  # k的深度dk，或叫做depth_k\n",
    "    scaled_attention_logits = matmul_qk / torch.sqrt(dk) # [..., seq_len_q, seq_len_k]\n",
    "#     print('scaled_attention_logits.shape',scaled_attention_logits.shape)\n",
    "#     print('m.shape',m.shape)\n",
    "#     print('q*k^t：', scaled_attention_logits.shape)\n",
    "    \n",
    "#     scaled_attention_logits = torch.matmul(scaled_attention_logits,m)\n",
    "    scaled_attention_logits = scaled_attention_logits * m # 按元素乘法\n",
    "#     print('q*k^t*m：', scaled_attention_logits.shape)\n",
    "    \n",
    "    # 将 mask 加入到缩放的张量上(重要！)\n",
    "    if mask is not None:  # mask: [b, 1, 1, seq_len]\n",
    "        # mask=1的位置是pad，乘以-1e9（-1*10^9）成为负无穷，经过softmax后会趋于0\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax 在最后一个轴（seq_len_k）上归一化\n",
    "    attention_weights = torch.nn.functional.softmax(scaled_attention_logits, dim=-1)  # [..., seq_len_q, seq_len_k]\n",
    "    \n",
    "    output = torch.matmul(attention_weights, v)  # =>[..., seq_len_q, depth_v]\n",
    "#     print('output：', output.shape)\n",
    "    return output, attention_weights  # [..., seq_len_q, depth_v], [..., seq_len_q, seq_len_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def print_out(q, k, v, m):\n",
    "#     temp_out, temp_attn = mn_scaled_dot_product_attention(q, k, v, m, None)\n",
    "#     print('Attention weights are:')\n",
    "#     print(temp_attn)\n",
    "#     print('Output is:')\n",
    "#     print(temp_out)\n",
    "\n",
    "# np.set_printoptions(suppress=True) # 设置不以科学计数法的形式显示数据\n",
    "\n",
    "# temp_k = torch.tensor([[10,0,0],\n",
    "#                        [0,10,0],\n",
    "#                        [0,0,10],\n",
    "#                        [0,0,10]], dtype=torch.float32) # [4,3]\n",
    "# temp_v = torch.tensor([[1,0],\n",
    "#                        [10,0],\n",
    "#                        [100,5],\n",
    "#                        [1000,6]], dtype=torch.float32) #[4,2]\n",
    "\n",
    "# temp_m = torch.tensor([[10,0,0,0],\n",
    "#                        [0,10,0,0],\n",
    "#                        [0,0,10,0],\n",
    "#                        [0,0,10,0]], dtype=torch.float32) # [4,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # query aligns with第2个key (key的第2列)，得到attention weights：[0. 1. 0. 0.]\n",
    "# # 所以第2行 的value值被返回\n",
    "# temp_q = torch.tensor([[0,10,0]], dtype=torch.float32) # [1,3]\n",
    "# print_out(temp_q, temp_k, temp_v,temp_m)\n",
    "# # Attention weights：[0. 1. 0. 0.]， Output：[[10.  0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # query aligns with 重复的key (key的第3列和第4列)，得到attention weights：[0. 0. 0.5 0.5]\n",
    "# # 所以第3行的value值与第4行的value值平均化后，被返回 【(100+1000)/2=550, (5+6)/2=5.5】\n",
    "# temp_q = torch.tensor([[0,0,10]], dtype=torch.float32) # [1,3]\n",
    "# print_out(temp_q, temp_k, temp_v,temp_m)\n",
    "# # Attention weights：[0. 0. 0.5 0.5]， Output：[[550.  5.5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # query aligns with 第1个key (key的第1列)和第2个key (key的第2列)，得到attention weights：[0.5 0.5 0. 0.]\n",
    "# # 所以第1行的value值与第2行的value值平均化后，被返回 【(1+10)/2=5.5, (0.+0.)/2=0.】\n",
    "# temp_q = torch.tensor([[10,10,0]], dtype=torch.float32) # [1,3]\n",
    "# print_out(temp_q, temp_k, temp_v,temp_m)\n",
    "# # Attention weights：[0.5 0.5 0. 0.]， Output：[[5.5.  0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # 传入所有的query\n",
    "# temp_q = torch.tensor([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=torch.float32)  # (3, 3)\n",
    "# print_out(temp_q, temp_k, temp_v,temp_m)\n",
    "# \"\"\"\n",
    "# # Attention weights：tensor(\n",
    "# [[0.  0.  0.5 0.5]\n",
    "#  [0.  1.  0.  0. ]\n",
    "#  [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)，\n",
    "# # Output：tensor(\n",
    "# [[550.    5.5]\n",
    "#  [ 10.    0. ]\n",
    "#  [  5.5   0. ]], shape=(3, 2), dtype=float32)\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.multi-head attention\n",
    "![jupyter-img3](./imgs/im3.jpg)\n",
    "\n",
    "多头注意允许模型在不同的位置联合处理来自不同表示子空间的信息。\n",
    "（Multi-head attention allows the model to jointly attend to information\n",
    "from different representation subspaces at different positions.）\n",
    "\n",
    "$$\\begin{array}{ll}  & MultiHead(Q,K,V)=Concat(head_1,...,head_h)W^O \\\\ & where\\; head_i=Attention(QW_i^Q,KW_i^K,VW_i^V)\\end{array}$$\n",
    "\n",
    "其中投影维参数矩阵$W_i^Q\\in R^{d_{model}\\text{x}\\,d_k}$，$W_i^K\\in R^{d_{model}\\text{x}\\,d_k}$， $W_i^V\\in R^{d_{model}\\text{x}\\,d_v}$，$W^O\\in R^{hd_v\\text{x}\\,d_{model}}$。\n",
    "\n",
    "$h=8,\\;d_k = d_v = d_{model}/h = 64$\n",
    "\n",
    "multi-head attention有4部分组成：\n",
    "- linear layer 将 linear layer的结果 split到不同的head\n",
    "- scaled dot-product attention\n",
    "- 将所有的head 进行拼接 concat\n",
    "- final linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0  # 因为输入要被（平均？）split到不同的head\n",
    "\n",
    "        self.depth = d_model // self.num_heads  # 512/8=64，所以在scaled dot-product atten中dq=dk=64,dv也是64\n",
    "\n",
    "        self.wq = torch.nn.Linear(d_model, d_model)\n",
    "        self.wk = torch.nn.Linear(d_model, d_model)\n",
    "        self.wv = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.final_linear = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):  # x [b, seq_len, d_model]\n",
    "        x = x.view(batch_size, -1, self.num_heads,\n",
    "                   self.depth)  # [b, seq_len, d_model=512]=>[b, seq_len, num_head=8, depth=64]\n",
    "        return x.transpose(1, 2)  # [b, seq_len, num_head=8, depth=64]=>[b, num_head=8, seq_len, depth=64]\n",
    "\n",
    "    def forward(self, q, k, v, mask):  # q=k=v=x [b, seq_len, embedding_dim] embedding_dim其实也=d_model\n",
    "        batch_size = q.shape[0]\n",
    "\n",
    "        q = self.wq(q)  # =>[b, seq_len, d_model]\n",
    "        k = self.wk(k)  # =>[b, seq_len, d_model]\n",
    "        v = self.wv(v)  # =>[b, seq_len, d_model]\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # =>[b, num_head=8, seq_len, depth=64]\n",
    "        k = self.split_heads(k, batch_size)  # =>[b, num_head=8, seq_len, depth=64]\n",
    "        v = self.split_heads(v, batch_size)  # =>[b, num_head=8, seq_len, depth=64]\n",
    "\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        # => [b, num_head=8, seq_len_q, depth=64], [b, num_head=8, seq_len_q, seq_len_k]\n",
    "\n",
    "        scaled_attention = scaled_attention.transpose(1, 2)  # =>[b, seq_len_q, num_head=8, depth=64]\n",
    "        # 转置操作让张量存储结构扭曲，直接使用view方法会失败，可以使用reshape方法\n",
    "        concat_attention = scaled_attention.reshape(batch_size, -1, self.d_model)  # =>[b, seq_len_q, d_model=512]\n",
    "\n",
    "        output = self.final_linear(concat_attention)  # =>[b, seq_len_q, d_model=512]\n",
    "        \n",
    "        return output, attention_weights  # [b, seq_len_q, d_model=512], [b, num_head=8, seq_len_q, seq_len_k]\n",
    "\n",
    "# temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "# x = torch.rand(batch, MAX_LENGTH+2, 512) # [b,seq_len,d_model,embedding_dim]\n",
    "# print(x.shape)\n",
    "# out, attn_weights = temp_mha(x, x, x, mask=None)\n",
    "# print(out.shape, attn_weights.shape) # [1, 60, 512], [1, 8, 60, 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class mn_MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads, seq_len):\n",
    "        super(mn_MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        assert d_model % self.num_heads == 0  # 因为输入要被（平均？）split到不同的head\n",
    "        \n",
    "        # 512/8=64，所以在mn_scaled dot-product atten中dq=dk=64,dv也是64\n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "\n",
    "        self.wq = torch.nn.Linear(d_model, d_model)\n",
    "        self.wk = torch.nn.Linear(d_model, d_model)\n",
    "        self.wv = torch.nn.Linear(d_model, d_model)\n",
    "        self.wm = torch.nn.Linear(d_model, seq_len*num_heads)\n",
    "\n",
    "        self.final_linear = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):  # x [b, seq_len, d_model]\n",
    "        x = x.view(batch_size, -1, self.num_heads,\n",
    "                   self.depth)  # [b, seq_len, d_model=512]=>[b, seq_len, num_head=8, depth=64]\n",
    "        return x.transpose(1, 2)  # [b, seq_len, num_head=8, depth=64]=>[b, num_head=8, seq_len, depth=64]\n",
    "    \n",
    "    def split_heads_m(self, x, batch_size):  # x [b, seq_len, d_model]\n",
    "        x = x.view(batch_size, -1, self.num_heads,\n",
    "                   self.seq_len)  # [b, seq_len, d_model=512]=>[b, seq_len, num_head=8, depth=64]\n",
    "        return x.transpose(1, 2)  # [b, seq_len, num_head=8, depth=64]=>[b, num_head=8, seq_len, depth=seq_len]\n",
    "    \n",
    "    def forward(self, q, k, v, m, mask):  # q=k=v=x [b, seq_len, embedding_dim] embedding_dim其实也=d_model\n",
    "        batch_size = q.shape[0]\n",
    "        \n",
    "        q = self.wq(q)  # =>[b, seq_len, d_model]\n",
    "#         print('q：', q.shape)\n",
    "        k = self.wk(k)  # =>[b, seq_len, d_model]\n",
    "#         print('k：', k.shape)\n",
    "        v = self.wv(v)  # =>[b, seq_len, d_model]\n",
    "#         print('v：', v.shape)\n",
    "        m = self.wm(m)  # =>[b, seq_len, seq_len]\n",
    "#         print('m：', m.shape)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # =>[b, num_head=8, seq_len, depth=64]\n",
    "#         print('q：', q.shape)\n",
    "        k = self.split_heads(k, batch_size)  # =>[b, num_head=8, seq_len, depth=64]\n",
    "#         print('k：', k.shape)\n",
    "        v = self.split_heads(v, batch_size)  # =>[b, num_head=8, seq_len, depth=64]\n",
    "#         print('v：', v.shape)\n",
    "        m = self.split_heads_m(m, batch_size)  # =>[b, num_head=8, seq_len, depth=64]\n",
    "#         print('m：', m.shape)\n",
    "        scaled_attention, attention_weights = mn_scaled_dot_product_attention(q, k, v, m, mask)\n",
    "        # => [b, num_head=8, seq_len_q, depth=64], [b, num_head=8, seq_len_q, seq_len_k]\n",
    "\n",
    "        scaled_attention = scaled_attention.transpose(1, 2)  # =>[b, seq_len_q, num_head=8, depth=64]\n",
    "        # 转置操作让张量存储结构扭曲，直接使用view方法会失败，可以使用reshape方法\n",
    "        concat_attention = scaled_attention.reshape(batch_size, -1, self.d_model)  # =>[b, seq_len_q, d_model=512]\n",
    "\n",
    "        output = self.final_linear(concat_attention)  # =>[b, seq_len_q, d_model=512]\n",
    "        return output, attention_weights  # [b, seq_len_q, d_model=512], [b, num_head=8, seq_len_q, seq_len_k]\n",
    "\n",
    "\n",
    "# temp_mha = mn_MultiHeadAttention(d_model=256, num_heads=8, seq_len=MAX_LENGTH+2)\n",
    "# x = torch.rand(batch, MAX_LENGTH+2, 256) # [b,seq_len,d_model,embedding_dim]\n",
    "# print('输入：', x.shape)\n",
    "# out, attn_weights = temp_mha(x, x, x, x, mask=None)\n",
    "# print(out.shape, attn_weights.shape) # [1, 60, 512], [1, 8, 60, 60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 6.point wise feed forward network\n",
    "2层线性变换和一个ReLU激活：\n",
    "$$FFN(x) = max(0, xW1 + b1)W2 + b2$$\n",
    "\n",
    "这一层的input和output的维度都是$d_{model}$，而内层的维度的是$d_{ff}=2048$\n",
    "\n",
    "其实就是(nn.relu(x w1+b1))w2+b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 点式前馈网络\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    feed_forward_net = torch.nn.Sequential(\n",
    "        torch.nn.Linear(d_model, dff),  # [b, seq_len, d_model]=>[b, seq_len, dff=2048]\n",
    "        #torch.nn.ReLU(),\n",
    "        \n",
    "        # 更换激活函数为SquaredReLU()\n",
    "#         torch.nn.LeakyReLU(),\n",
    "        SquaredReLU(), #primer_ez\n",
    "        torch.nn.Linear(dff, d_model),  # [b, seq_len, dff=2048]=>[b, seq_len, d_model=512]\n",
    "    )\n",
    "    return feed_forward_net\n",
    "\n",
    "# sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
    "# input = torch.rand(64, 50, 512) # [b, seq_len, d_model]\n",
    "# print(sample_ffn(input).shape) # [b=64, seq_len=50, d_model=512]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "transformer model:\n",
    "\n",
    "(1) input sentence 传入N个encoder layer，句子的每个token都会有一个输出\n",
    "\n",
    "(2) decoder attends on encoder 的输出（encoder-decoder attention） 和 它自己的输入（self-attention），然后预测出下一个词\n",
    "\n",
    "## 7.encoder layer\n",
    "每个编码器层包括以下2个子层：\n",
    "- 多头注意力（有padding mask）\n",
    "- 点式前馈网络（Point wise feed forward networks）\n",
    "\n",
    "注意：每个子层还伴随着一个残差连接，然后进行“层归一化”（LayerNorm）。残差连接有助于避免深度网络中的梯度消失问题。\n",
    "\n",
    "每个子层的输出是 LayerNorm(x + Sublayer(x))。归一化是在 d_model（最后一个）维度完成的。\n",
    "\n",
    "注意：实现时在每个sub layer 之后加入了dropout层，再才进行add the sub layer input 和 normalized，即\n",
    "LayerNorm(x + Dropout(Sublayer(x)))\n",
    "\n",
    "Transformer 中有 N 个编码器层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)  # 多头注意力（padding mask）(self-attention)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "        self.layernorm2 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "\n",
    "        self.dropout1 = torch.nn.Dropout(rate)\n",
    "        self.dropout2 = torch.nn.Dropout(rate)\n",
    "\n",
    "    # x [b, inp_seq_len, embedding_dim] embedding_dim其实也=d_model\n",
    "    # mask [b,1,1,inp_seq_len]\n",
    "    def forward(self, x, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # =>[b, seq_len, d_model]\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)  # 残差&层归一化 =>[b, seq_len, d_model]\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # =>[b, seq_len, d_model]\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # 残差&层归一化 =>[b, seq_len, d_model]\n",
    "\n",
    "        return out2  # [b, seq_len, d_model]\n",
    "\n",
    "# layernorm = torch.nn.LayerNorm(normalized_shape=512, eps=1e-6)\n",
    "# x = torch.rand(4, 64, 512)\n",
    "# print(layernorm(x).shape)\n",
    "\n",
    "# sample_encoder_layer = EncoderLayer(256, 8, 1024)\n",
    "# x = torch.rand(16, 14, 256) # [b, seq_len, d_model]\n",
    "# sample_encoder_layer_output = sample_encoder_layer(x, None)\n",
    "# print(sample_encoder_layer_output.shape) # [64, 50, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mn_EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff, seq_len, rate=0.1):\n",
    "        super(mn_EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = mn_MultiHeadAttention(d_model, num_heads, seq_len)  # 多头注意力（padding mask）(self-attention)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "        self.layernorm2 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "\n",
    "        self.dropout1 = torch.nn.Dropout(rate)\n",
    "        self.dropout2 = torch.nn.Dropout(rate)\n",
    "\n",
    "    # x [b, inp_seq_len, embedding_dim] embedding_dim其实也=d_model\n",
    "    # mask [b,1,1,inp_seq_len]\n",
    "    def forward(self, x, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, x, mask)  # =>[b, seq_len, d_model]\n",
    "#         print(attn_output.shape)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)  # 残差&层归一化 =>[b, seq_len, d_model]\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # =>[b, seq_len, d_model]\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # 残差&层归一化 =>[b, seq_len, d_model]\n",
    "\n",
    "        return out2  # [b, seq_len, d_model]\n",
    "\n",
    "# layernorm = torch.nn.LayerNorm(normalized_shape=512, eps=1e-6)\n",
    "# x = torch.rand(4, MAX_LENGTH, 512)\n",
    "# print(layernorm(x).shape)\n",
    "\n",
    "# sample_encoder_layer = mn_EncoderLayer(256, 8, 2048,MAX_LENGTH+2)\n",
    "# x = torch.rand(64, MAX_LENGTH+2, 256) # [b, seq_len, d_model]\n",
    "# sample_encoder_layer_output = sample_encoder_layer(x, None)\n",
    "# print(sample_encoder_layer_output.shape) # [64, 50, 512]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 8.decoder layer\n",
    "\n",
    "每个解码器层包括以下3个子层：\n",
    "- masked的多头注意力（look ahead mask 和 padding mask）(self-attention)\n",
    "- 多头注意力（padding mask）(encoder-decoder attention)。\n",
    "    V和 K接收encoder的输出作为输入。Q接收masked的多头注意力子层的输出。\n",
    "- 点式前馈网络\n",
    "\n",
    "每个子层还伴随着一个残差连接，然后进行“层归一化”（LayerNorm）。\n",
    "\n",
    "每个子层的输出是 LayerNorm(x + Sublayer(x))。归一化是在 d_model（最后一个）维度完成的。\n",
    "\n",
    "Transformer 中共有 N 个解码器层。\n",
    "\n",
    "当 Q 接收到decoder的第一个注意力块的输出，并且 K 接收到encoder的输出时，注意力权重表示根据encoder的输出赋予decoder输入的重要性。\n",
    "换一种说法，decoder通过查看encoder输出和对其自身输出的自注意力，预测下一个词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model,\n",
    "                                       num_heads)  # masked的多头注意力（look ahead mask 和 padding mask）(self-attention)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)  # 多头注意力（padding mask）(encoder-decoder attention)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "        self.layernorm2 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "        self.layernorm3 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "\n",
    "        self.dropout1 = torch.nn.Dropout(rate)\n",
    "        self.dropout2 = torch.nn.Dropout(rate)\n",
    "        self.dropout3 = torch.nn.Dropout(rate)\n",
    "\n",
    "    # x [b, targ_seq_len, embedding_dim] embedding_dim其实也=d_model=512\n",
    "    # look_ahead_mask [b, 1, targ_seq_len, targ_seq_len] 这里传入的look_ahead_mask应该是已经结合了look_ahead_mask和padding mask的mask\n",
    "    # enc_output [b, inp_seq_len, d_model]\n",
    "    # padding_mask [b, 1, 1, inp_seq_len]\n",
    "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x,\n",
    "                                               look_ahead_mask)  # =>[b, targ_seq_len, d_model], [b, num_heads, targ_seq_len, targ_seq_len]\n",
    "#         print('attn1',attn1.shape)\n",
    "        attn1 = self.dropout1(attn1)\n",
    "        out1 = self.layernorm1(x + attn1)  # 残差&层归一化 [b, targ_seq_len, d_model]\n",
    "\n",
    "        # Q: receives the output from decoder's first attention block，即 masked multi-head attention sublayer\n",
    "        # K V: V (value) and K (key) receive the encoder output as inputs\n",
    "        attn2, attn_weights_block2 = self.mha2(out1, enc_output, enc_output,\n",
    "                                               padding_mask)  # =>[b, targ_seq_len, d_model], [b, num_heads, targ_seq_len, inp_seq_len]\n",
    "#         print('attn2',attn2.shape)\n",
    "        attn2 = self.dropout2(attn2)\n",
    "        out2 = self.layernorm2(out1 + attn2)  # 残差&层归一化 [b, targ_seq_len, d_model]\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # =>[b, targ_seq_len, d_model]\n",
    "        ffn_output = self.dropout3(ffn_output)\n",
    "        out3 = self.layernorm3(out2 + ffn_output)  # 残差&层归一化 =>[b, targ_seq_len, d_model]\n",
    "\n",
    "#         print('out3',out3.shape)\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "        # [b, targ_seq_len, d_model], [b, num_heads, targ_seq_len, targ_seq_len], [b, num_heads, targ_seq_len, inp_seq_len]\n",
    "\n",
    "\n",
    "# sample_decoder_layer = DecoderLayer(512, 8, 2048) #\n",
    "# y = torch.rand(64, 40, 512) # [b, seq_len, d_model]\n",
    "# sample_decoder_layer_output,_,_ = sample_decoder_layer(y, sample_encoder_layer_output, None, None)\n",
    "# print(sample_decoder_layer_output.shape) # [64, 40, 512]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 9.encoder\n",
    "编码器 包括：\n",
    "- 输入嵌入（Input Embedding）\n",
    "- 位置编码（Positional Encoding）\n",
    "- N 个编码器层（encoder layers）\n",
    "\n",
    "输入经过嵌入（embedding）后，该嵌入与位置编码相加。该加法结果的输出是编码器层的输入。编码器的输出是解码器的输入\n",
    "\n",
    "注意：缩放 embedding\n",
    "\n",
    "原始论文的3.4节Embeddings and Softmax最后一句有提到： In the embedding layers, we multiply those weights by $\\sqrt{d_{model}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers,  # N个encoder layer\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dff,  # 点式前馈网络内层fn的维度\n",
    "                 input_vocab_size,  # 输入词表大小（源语言（法语））\n",
    "                 maximun_position_encoding,\n",
    "                 rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=input_vocab_size, embedding_dim=d_model, padding_idx=0)\n",
    "        self.pos_encoding = positional_encoding(maximun_position_encoding,\n",
    "                                                d_model)  # =>[1, max_pos_encoding, d_model=512]\n",
    "\n",
    "        # self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate).cuda() for _ in range(num_layers)] # 不行\n",
    "        self.enc_layers = torch.nn.ModuleList([EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)])\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(rate)\n",
    "\n",
    "    # x [b, inp_seq_len]\n",
    "    # mask [b, 1, 1, inp_sel_len]\n",
    "    def forward(self, x, mask):\n",
    "        inp_seq_len = x.shape[-1]\n",
    "        \n",
    "        # adding embedding and position encoding\n",
    "        x = self.embedding(x)  # [b, inp_seq_len]=>[b, inp_seq_len, d_model]\n",
    "\n",
    "        # 缩放 embedding 原始论文的3.4节有提到： In the embedding layers, we multiply those weights by \\sqrt{d_model}.\n",
    "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
    "        pos_encoding = self.pos_encoding[:, :inp_seq_len, :]\n",
    "        pos_encoding = pos_encoding.cuda()  # ###############\n",
    "        x += pos_encoding  # [b, inp_seq_len, d_model]\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, mask)  # [b, inp_seq_len, d_model]=>[b, inp_seq_len, d_model]\n",
    "        return x  # [b, inp_seq_len, d_model]\n",
    "\n",
    "\n",
    "# sample_encoder = Encoder(num_layers=2,\n",
    "#                          d_model=512,\n",
    "#                          num_heads=8,\n",
    "#                          dff=2048,\n",
    "#                          input_vocab_size=8500,\n",
    "#                          maximun_position_encoding=10000)\n",
    "# sample_encoder = sample_encoder.to(device)\n",
    "\n",
    "# x = torch.tensor(np.random.randint(low=0, high=200, size=(64, 42))) # [b, seq_len]\n",
    "# # print(x.shape)\n",
    "# sample_encoder_output = sample_encoder(x.cuda(), None)\n",
    "# print(sample_encoder_output.shape) # [b, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mn_Encoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers,  # N个encoder layer\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dff,  # 点式前馈网络内层fn的维度\n",
    "                 input_vocab_size,  # 输入词表大小（源语言（法语））\n",
    "                 maximun_position_encoding,\n",
    "                 seq_len,\n",
    "                 rate=0.1):\n",
    "        super(mn_Encoder, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=input_vocab_size, embedding_dim=d_model, padding_idx=0)\n",
    "        self.pos_encoding = positional_encoding(maximun_position_encoding,\n",
    "                                                d_model)  # =>[1, max_pos_encoding, d_model=512]\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate).cuda() for _ in range(num_layers)] # 不行\n",
    "        self.enc_layers = torch.nn.ModuleList([mn_EncoderLayer(d_model, num_heads, dff, seq_len, rate) for _ in range(num_layers)])\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(rate)\n",
    "\n",
    "    # x [b, inp_seq_len]\n",
    "    # mask [b, 1, 1, inp_sel_len]\n",
    "    def forward(self, x, mask):\n",
    "        inp_seq_len = x.shape[-1]\n",
    "#         print('inp_seq_len:',inp_seq_len)\n",
    "#         print('1')\n",
    "#         print('x.shape:',x.shape)\n",
    "        # adding embedding and position encoding\n",
    "        x = self.embedding(x)  # [b, inp_seq_len]=>[b, inp_seq_len, d_model]\n",
    "#         print('2')\n",
    "#         print('embedding(x):',x.shape)\n",
    "        # 缩放 embedding 原始论文的3.4节有提到： In the embedding layers, we multiply those weights by \\sqrt{d_model}.\n",
    "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
    "#         print('3')\n",
    "        pos_encoding = self.pos_encoding[:, :inp_seq_len, :]\n",
    "#         print('4')\n",
    "        pos_encoding = pos_encoding.cuda()  # ###############\n",
    "#         print('5')\n",
    "#         print('x.shape:',x.shape)\n",
    "#         print('pos_encoding.shape:',pos_encoding.shape)\n",
    "        x += pos_encoding  # [b, inp_seq_len, d_model]\n",
    "#         print('6')\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, mask)  # [b, inp_seq_len, d_model]=>[b, inp_seq_len, d_model]\n",
    "#         print('7')\n",
    "        return x  # [b, inp_seq_len, d_model]\n",
    "\n",
    "\n",
    "# sample_encoder = mn_Encoder(num_layers=2,\n",
    "#                          d_model=256,\n",
    "#                          num_heads=8,\n",
    "#                          dff=2048,\n",
    "#                          input_vocab_size=8500,\n",
    "#                          maximun_position_encoding=10000,\n",
    "#                          seq_len = MAX_LENGTH+2)\n",
    "# sample_encoder = sample_encoder.to(device)\n",
    "\n",
    "# temp_inp = torch.tensor(np.random.randint(low=0, high=200, size=(16, MAX_LENGTH+2))) # [b, inp_seq_len]\n",
    "# print(temp_inp.shape)\n",
    "# sample_encoder_output = sample_encoder(temp_inp.cuda(), None)\n",
    "# print(sample_encoder_output.shape) # [b, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 10.decoder\n",
    "解码器包括：\n",
    "- 输出嵌入（Output Embedding）\n",
    "- 位置编码（Positional Encoding）\n",
    "\n",
    "N 个解码器层（decoder layers）\n",
    "\n",
    "目标（target）经过一个嵌入后，该嵌入和位置编码相加。该加法结果是解码器层的输入。解码器的输出是最后的线性层的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers,  # N个encoder layer\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dff,  # 点式前馈网络内层fn的维度\n",
    "                 target_vocab_size,  # target词表大小（目标语言（英语））\n",
    "                 maximun_position_encoding,\n",
    "                 rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=target_vocab_size, embedding_dim=d_model, padding_idx=0)\n",
    "        self.pos_encoding = positional_encoding(maximun_position_encoding,\n",
    "                                                d_model)  # =>[1, max_pos_encoding, d_model=512]\n",
    "\n",
    "        # self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate).cuda() for _ in range(num_layers)] # 不行\n",
    "        self.dec_layers = torch.nn.ModuleList([DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)])\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(rate)\n",
    "\n",
    "    # x [b, targ_seq_len]\n",
    "    # look_ahead_mask [b, 1, targ_seq_len, targ_seq_len] 这里传入的look_ahead_mask应该是已经结合了look_ahead_mask和padding mask的mask\n",
    "    # enc_output [b, inp_seq_len, d_model]\n",
    "    # padding_mask [b, 1, 1, inp_seq_len]\n",
    "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        targ_seq_len = x.shape[-1]\n",
    "\n",
    "        attention_weights = {}\n",
    "\n",
    "        # adding embedding and position encoding\n",
    "        x = self.embedding(x)  # [b, targ_seq_len]=>[b, targ_seq_len, d_model]\n",
    "        # 缩放 embedding 原始论文的3.4节有提到： In the embedding layers, we multiply those weights by \\sqrt{d_model}.\n",
    "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
    "        # x += self.pos_encoding[:, :targ_seq_len, :]  # [b, targ_seq_len, d_model]\n",
    "        pos_encoding = self.pos_encoding[:, :targ_seq_len, :]  # [b, targ_seq_len, d_model]\n",
    "        pos_encoding = pos_encoding.cuda() # ###############\n",
    "        x += pos_encoding  # [b, inp_seq_len, d_model]\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, attn_block1, attn_block2 = self.dec_layers[i](x, enc_output, look_ahead_mask, padding_mask)\n",
    "            # => [b, targ_seq_len, d_model], [b, num_heads, targ_seq_len, targ_seq_len], [b, num_heads, targ_seq_len, inp_seq_len]\n",
    "\n",
    "            attention_weights[f'decoder_layer{i + 1}_block1'] = attn_block1\n",
    "            attention_weights[f'decoder_layer{i + 1}_block2'] = attn_block2\n",
    "\n",
    "#         print('Decoder_output',x.shape)\n",
    "        return x, attention_weights\n",
    "        # => [b, targ_seq_len, d_model],\n",
    "        # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "        #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "\n",
    "# sample_decoder = Decoder(num_layers=2,\n",
    "#                          d_model=512,\n",
    "#                          num_heads=8,\n",
    "#                          dff=2048,\n",
    "#                          target_vocab_size=8000,\n",
    "#                          maximun_position_encoding=5000)\n",
    "# sample_decoder = sample_decoder.to(device)\n",
    "\n",
    "# y = torch.tensor(np.random.randint(low=0, high=200, size=(64, 36))) # [b, seq_len]\n",
    "# # print(y.shape) # [64, 36]\n",
    "# output, attn = sample_decoder(y.cuda(),\n",
    "#                               enc_output=sample_encoder_output, # [64, 42, 512]\n",
    "#                               look_ahead_mask=None,\n",
    "#                               padding_mask=None)\n",
    "# print(output.shape) # [64, 36, 512]\n",
    "# print(attn['decoder_layer2_block1'].shape) # [64, 8, 36, 36]\n",
    "# print(attn['decoder_layer2_block2'].shape) # [64, 8, 36, 42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 11.搭建transformer\n",
    "transformer 含有3各部分\n",
    "- encoder\n",
    "- decoder\n",
    "- final linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# class Transformer(torch.nn.Module):\n",
    "#     def __init__(self,\n",
    "#                  num_layers,  # N个encoder layer\n",
    "#                  d_model,\n",
    "#                  num_heads,\n",
    "#                  dff,  # 点式前馈网络内层fn的维度\n",
    "#                  input_vocab_size,  # input此表大小（源语言（法语））\n",
    "#                  target_vocab_size,  # target词表大小（目标语言（英语））\n",
    "#                  pe_input,  # input max_pos_encoding\n",
    "#                  pe_target,  # input max_pos_encoding\n",
    "#                  rate=0.1):\n",
    "#         super(Transformer, self).__init__()\n",
    "\n",
    "#         self.encoder = Encoder(num_layers,\n",
    "#                                d_model,\n",
    "#                                num_heads,\n",
    "#                                dff,\n",
    "#                                input_vocab_size,\n",
    "#                                pe_input,\n",
    "#                                rate)\n",
    "#         self.decoder = Decoder(num_layers,\n",
    "#                                d_model,\n",
    "#                                num_heads,\n",
    "#                                dff,\n",
    "#                                target_vocab_size,\n",
    "#                                pe_target,\n",
    "#                                rate)\n",
    "#         self.final_layer = torch.nn.Linear(d_model, target_vocab_size)\n",
    "\n",
    "#     # inp [b, inp_seq_len]\n",
    "#     # targ [b, targ_seq_len]\n",
    "#     # enc_padding_mask [b, 1, 1, inp_seq_len]\n",
    "#     # look_ahead_mask [b, 1, targ_seq_len, targ_seq_len]\n",
    "#     # dec_padding_mask [b, 1, 1, inp_seq_len] # 注意这里的维度是inp_seq_len\n",
    "#     def forward(self, inp, targ, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "#         enc_output = self.encoder(inp, enc_padding_mask)  # =>[b, inp_seq_len, d_model]\n",
    "\n",
    "#         dec_output, attention_weights = self.decoder(targ, enc_output, look_ahead_mask, dec_padding_mask)\n",
    "#         # => [b, targ_seq_len, d_model],\n",
    "#         # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "#         #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "#         final_output = self.final_layer(dec_output)  # =>[b, targ_seq_len, target_vocab_size]\n",
    "\n",
    "#         return final_output, attention_weights\n",
    "#         # [b, targ_seq_len, target_vocab_size]\n",
    "#         # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "#         #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "\n",
    "\n",
    "# sample_transformer = Transformer(num_layers=2,\n",
    "#                                  d_model=512,\n",
    "#                                  num_heads=8,\n",
    "#                                  dff=2048,\n",
    "#                                  input_vocab_size=8500,\n",
    "#                                  target_vocab_size=8000,\n",
    "#                                  pe_input=10000,\n",
    "#                                  pe_target=6000)\n",
    "# sample_transformer = sample_transformer.to(device)\n",
    "\n",
    "# temp_inp = torch.tensor(np.random.randint(low=0, high=200, size=(64, 36))) # [b, inp_seq_len]\n",
    "# temp_targ = torch.tensor(np.random.randint(low=0, high=200, size=(64, 42))) # [b, targ_seq_len]\n",
    "\n",
    "# fn_out, attn = sample_transformer(temp_inp.cuda(), temp_targ.cuda(), None, None, None)\n",
    "# print(fn_out.shape) # [64, 36, 8000]\n",
    "# print(attn['decoder_layer2_block1'].shape) # [64, 8, 36, 36]\n",
    "# print(attn['decoder_layer2_block2'].shape) # [64, 8, 36, 42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mn_Transformer(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers,  # N个encoder layer\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dff,  # 点式前馈网络内层fn的维度\n",
    "                 input_vocab_size,  # input此表大小（源语言（法语））\n",
    "                 target_vocab_size,  # target词表大小（目标语言（英语））\n",
    "                 pe_input,  # input max_pos_encoding\n",
    "                 pe_target,  # input max_pos_encoding\n",
    "                 seq_len,\n",
    "                 rate=0.1):\n",
    "        super(mn_Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = mn_Encoder(num_layers,\n",
    "                               d_model,\n",
    "                               num_heads,\n",
    "                               dff,\n",
    "                               input_vocab_size,\n",
    "                               pe_input,\n",
    "                               seq_len,\n",
    "                               rate)\n",
    "        self.decoder = Decoder(num_layers,\n",
    "                               d_model,\n",
    "                               num_heads,\n",
    "                               dff,\n",
    "                               target_vocab_size,\n",
    "                               pe_target,\n",
    "                               rate)\n",
    "        self.final_layer = torch.nn.Linear(d_model, target_vocab_size)\n",
    "\n",
    "    # inp [b, inp_seq_len]\n",
    "    # targ [b, targ_seq_len]\n",
    "    # enc_padding_mask [b, 1, 1, inp_seq_len]\n",
    "    # look_ahead_mask [b, 1, targ_seq_len, targ_seq_len]\n",
    "    # dec_padding_mask [b, 1, 1, inp_seq_len] # 注意这里的维度是inp_seq_len\n",
    "    def forward(self, inp, targ, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        enc_output = self.encoder(inp, enc_padding_mask)  # =>[b, inp_seq_len, d_model]\n",
    "        dec_output, attention_weights = self.decoder(targ, enc_output, look_ahead_mask, dec_padding_mask)\n",
    "        # => [b, targ_seq_len, d_model],\n",
    "        # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "        #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "        final_output = self.final_layer(dec_output)  # =>[b, targ_seq_len, target_vocab_size]\n",
    "\n",
    "        return final_output, attention_weights\n",
    "        # [b, targ_seq_len, target_vocab_size]\n",
    "        # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "        #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "\n",
    "\n",
    "# sample_transformer = mn_Transformer(num_layers=2,\n",
    "#                                  d_model=128,\n",
    "#                                  num_heads=8,\n",
    "#                                  dff=1024,\n",
    "#                                  input_vocab_size=8500,\n",
    "#                                  target_vocab_size=8000,\n",
    "#                                  pe_input=10000,\n",
    "#                                  pe_target=6000,\n",
    "#                                  seq_len=MAX_LENGTH+2)\n",
    "# sample_transformer = sample_transformer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_inp = torch.tensor(np.random.randint(low=0, high=200, size=(512, MAX_LENGTH+2))) # [b, inp_seq_len]\n",
    "# temp_targ = torch.tensor(np.random.randint(low=0, high=200, size=(512, MAX_LENGTH+2))) # [b, targ_seq_len]\n",
    "# fn_out, attn = sample_transformer(temp_inp.cuda(), temp_targ.cuda(), None, None, None)\n",
    "\n",
    "\n",
    "# for step, (inp, targ) in enumerate(train_dataloader, start=1):\n",
    "#     print(inp,inp.shape,targ,targ.shape)\n",
    "#     fn_out, attn = sample_transformer(inp.cuda(), targ.cuda(), None, None, None)\n",
    "#     break\n",
    "\n",
    "\n",
    "\n",
    "# print(fn_out.shape) # [64, 36, 8000]\n",
    "# print(attn['decoder_layer2_block1'].shape) # [64, 8, 36, 36]\n",
    "# print(attn['decoder_layer2_block2'].shape) # [64, 8, 36, 42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 12.设置超参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_layers： 4\n",
      "d_model： 256\n",
      "dff： 2048\n",
      "dropout_rate： 0.5\n"
     ]
    }
   ],
   "source": [
    "#Transformer 的基础模型使用的数值为：num_layers=6，d_model = 512，dff = 2048\n",
    "#为了让本示例小且相对较快，已经减小了num_layers、 d_model 和 dff 的值。\n",
    "num_layers = 5\n",
    "d_model = 256\n",
    "dff = 2048\n",
    "num_heads = 8\n",
    "\n",
    "print('num_layers：', num_layers)\n",
    "print('d_model：', d_model)\n",
    "print('dff：', dff)\n",
    "print('num_heads：', num_heads)\n",
    "\n",
    "dropout_rate = 0.2\n",
    "print('dropout_rate：', dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 13.优化器\n",
    "根据论文中的公式，将 Adam 优化器与自定义的学习速率调度程序（scheduler）配合使用。\n",
    "\n",
    "$$lrate = d_{model}^{-0.5} * min(\\text{step_num}^{-0.5},\\; \\text{step_num}*\\text{warmup_steps}^{-1.5})$$\n",
    "\n",
    "关于pytorch optimizer，参考：\n",
    "\n",
    "https://www.cnblogs.com/wanghui-garcia/p/10895397.html\n",
    "\n",
    "https://www.jianshu.com/p/5d85a59f1bac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, d_model, warm_steps=4):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warm_steps\n",
    "\n",
    "        super(CustomSchedule, self).__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"\n",
    "        # rsqrt 函数用于计算 x 元素的平方根的倒数.  即= 1 / sqrt{x}\n",
    "        arg1 = torch.rsqrt(torch.tensor(self._step_count, dtype=torch.float32))\n",
    "        arg2 = torch.tensor(self._step_count * (self.warmup_steps ** -1.5), dtype=torch.float32)\n",
    "        dynamic_lr = torch.rsqrt(self.d_model) * torch.minimum(arg1, arg2)\n",
    "        \"\"\"\n",
    "        # print('*'*27, self._step_count)\n",
    "        arg1 = self._step_count ** (-0.5)\n",
    "        arg2 = self._step_count * (self.warmup_steps ** -1.5)\n",
    "        dynamic_lr = (self.d_model ** (-0.5)) * min(arg1, arg2)\n",
    "        # print('dynamic_lr:', dynamic_lr)\n",
    "        return [dynamic_lr for group in self.optimizer.param_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # 测试\n",
    "# model = sample_transformer\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "# learning_rate = CustomSchedule(optimizer, d_model, warm_steps=4000)\n",
    "\n",
    "# lr_list = []\n",
    "# for i in range(1, 20000):\n",
    "#     learning_rate.step()\n",
    "#     lr_list.append(learning_rate.get_lr()[0])\n",
    "# plt.figure()\n",
    "# plt.plot(np.arange(1, 20000), lr_list)\n",
    "# plt.legend(['warmup=4000 steps'])\n",
    "# plt.ylabel(\"Learning Rate\")\n",
    "# plt.xlabel(\"Train Step\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # 其他的学习率调度器测试，例如pytorch自带的StepLR\n",
    "# _model = sample_transformer\n",
    "# _optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# _learning_rate = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# lr_list = []\n",
    "# for i in range(1, 50):\n",
    "#     _learning_rate.step()\n",
    "#     lr_list.append(_learning_rate.get_lr()[0])\n",
    "# plt.figure()\n",
    "# plt.plot(np.arange(1, 50), lr_list)\n",
    "# plt.legend(['StepLR:gamma=0.5'])\n",
    "# plt.ylabel(\"Learning Rate\")\n",
    "# plt.xlabel(\"Train Step\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 14.损失和评价准则\n",
    "\n",
    "计算loss时要把mask=1的位置的去除掉\n",
    "\n",
    "### 【大坑！】\n",
    "【注意】，当输入是多维时交叉熵的参数维度，跟tf2不一样，tf2中pred是【b,seq_len,vocab_size】\n",
    "pytorch中pred应该调整为【b,vocab_size,seq_len】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 'none'表示直接返回b个样本的loss，默认求平均\n",
    "loss_object = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "# 【注意】，当输入是多维时交叉熵的参数维度，跟tf2不一样，tf2中pred是【b,seq_len,vocab_size】\n",
    "# pytorch中pred应该调整为【b,vocab_size,seq_len】\n",
    "\"\"\"\n",
    "- Input: :math:`(N, C)` where `C = number of classes`, or\n",
    "          :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1`\n",
    "          in the case of `K`-dimensional loss.\n",
    "\n",
    "- Target: :math:`(N)` where each value is :math:`0 \\leq \\text{targets}[i] \\leq C-1`, or\n",
    "          :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \\geq 1` in the case of\n",
    "          K-dimensional loss.\n",
    "\"\"\"\n",
    "\n",
    "# real [b, targ_seq_len]\n",
    "# pred [b, targ_seq_len, target_vocab_size]\n",
    "def mask_loss_func(real, pred):\n",
    "    # print(real.shape, pred.shape)\n",
    "    # _loss = loss_object(pred, real) # [b, targ_seq_len]\n",
    "    _loss = loss_object(pred.transpose(-1, -2), real)  # [b, targ_seq_len]\n",
    "\n",
    "    # logical_not  取非\n",
    "    # mask 每个元素为bool值，如果real中有pad，则mask相应位置就为False\n",
    "    # mask = torch.logical_not(real.eq(0)).type(_loss.dtype) # [b, targ_seq_len] pad=0的情况\n",
    "    mask = torch.logical_not(real.eq(pad)).type(_loss.dtype)  # [b, targ_seq_len] pad!=0的情况\n",
    "\n",
    "    # 对应位置相乘，token上的损失被保留了下来，pad的loss被置为0或False 去掉，不计算在内\n",
    "    _loss *= mask\n",
    "\n",
    "    return _loss.sum() / mask.sum().item()\n",
    "\n",
    "# 另一种实现方式\n",
    "def mask_loss_func2(real, pred):\n",
    "    # _loss = loss_object(pred, real) # [b, targ_seq_len]\n",
    "    _loss = loss_object(pred.transpose(-1, -2), real)  # [b, targ_seq_len]\n",
    "    # mask = torch.logical_not(real.eq(0)) # [b, targ_seq_len] bool值\n",
    "    mask = torch.logical_not(real.eq(pad)) # [b, targ_seq_len] bool值\n",
    "    _loss = _loss.masked_select(mask) # mask必须是BoolTensor或ByteTensor类型\n",
    "    return _loss.mean()\n",
    "\n",
    "\n",
    "# y_pred = torch.randn(3,3) # [3,3]\n",
    "# y_true = torch.tensor([1,2,0]) # [3]\n",
    "# # print(y_true.shape, y_pred.shape)\n",
    "# print(loss_object(y_pred, y_true))\n",
    "# print('计算loss时把mask的也算进去了,损失会偏大？：', loss_object(y_pred, y_true).mean())\n",
    "# print('计算loss时去除mask:', mask_loss_func(y_true, y_pred))\n",
    "# print('计算loss时去除mask:', mask_loss_func2(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "同样计算metric（如acc）时要把mask=1的位置的去除掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# real [b, targ_seq_len]\n",
    "# pred [b, targ_seq_len, target_vocab_size]\n",
    "def mask_accuracy_func(real, pred):\n",
    "    _pred = pred.argmax(dim=-1)  # [b, targ_seq_len, target_vocab_size]=>[b, targ_seq_len]\n",
    "    corrects = _pred.eq(real)  # [b, targ_seq_len] bool值\n",
    "\n",
    "    # logical_not  取非\n",
    "    # mask 每个元素为bool值，如果real中有pad，则mask相应位置就为False\n",
    "    # mask = torch.logical_not(real.eq(0)) # [b, targ_seq_len] bool值 pad=0的情况\n",
    "    mask = torch.logical_not(real.eq(pad))  # [b, targ_seq_len] bool值 pad!=0的情况\n",
    "\n",
    "    # 对应位置相乘，token上的值被保留了下来，pad上的值被置为0或False 去掉，不计算在内\n",
    "    corrects *= mask\n",
    "\n",
    "    return corrects.sum().float() / mask.sum().item()\n",
    "\n",
    "# 另一种实现方式\n",
    "def mask_accuracy_func2(real, pred):\n",
    "    _pred = pred.argmax(dim=-1) # [b, targ_seq_len, target_vocab_size]=>[b, targ_seq_len]\n",
    "    corrects = _pred.eq(real).type(torch.float32) # [b, targ_seq_len]\n",
    "    # mask = torch.logical_not(real.eq(0)) # [b, targ_seq_len] bool值\n",
    "    mask = torch.logical_not(real.eq(pad)) # [b, targ_seq_len] bool值\n",
    "    corrects = corrects.masked_select(mask) # [真正有token的个数] 平摊开成1维的\n",
    "\n",
    "    return corrects.mean()\n",
    "\n",
    "def mask_accuracy_func3(real, pred):\n",
    "    _pred = pred.argmax(dim=-1) # [b, targ_seq_len, target_vocab_size]=>[b, targ_seq_len]\n",
    "    corrects = _pred.eq(real) # [b, targ_seq_len] bool值\n",
    "    # mask = torch.logical_not(real.eq(0)) # [b, targ_seq_len] bool值\n",
    "    mask = torch.logical_not(real.eq(pad)) # [b, targ_seq_len] bool值\n",
    "    corrects = torch.logical_and(corrects, mask)\n",
    "    # print(corrects.dtype) # bool\n",
    "    # print(corrects.sum().dtype) #int64\n",
    "    return corrects.sum().float()/mask.sum().item()\n",
    "\n",
    "# y_pred = torch.randn(3,3) # [3,3]\n",
    "# y_true = torch.tensor([0,2,1]) # [3] 最后一个1表示pad噢~\n",
    "# print(y_true)\n",
    "# print(y_pred)\n",
    "# print('计算acc时去除mask:', mask_accuracy_func(y_true, y_pred))\n",
    "# print('计算acc时去除mask:', mask_accuracy_func2(y_true, y_pred))\n",
    "# print('计算acc时去除mask:', mask_accuracy_func3(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 15.生成mask\n",
    "\n",
    "$$Attention(Q,K,V)=softmax_{(k)}(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "注意：实现时对mask的处理\n",
    "\n",
    "mask=1的位置是pad或者future token，乘以-1e9（-1*10^9）成为负无穷，经过softmax后会趋于0\n",
    "\n",
    "【注意】：在计算decoder的第2个attention block时（encoder-decoder attention），\n",
    "从scaled dot-product的公式可以看出，此时Q是decoder的第一个attention block的输出，\n",
    "而K，V都来自encoder的输出。QK^T后得到[...,seq_len_q, seq_len_k]，而softmax是在seq_len_k维\n",
    "上进行的，softmax后seq_len_k维上pad的位置被置于0，乘以V（seq_len_k=seq_len_v=encoder output的seq_len）\n",
    "后那些pad的位置还是只会是0。所以仅从公式就可以看出decoder的第2个attention block的padding mask是基于\n",
    "encoder output 的seq_len即整个模型的inp_seq_len来设置的，而不是targ_seq_len。\n",
    "\n",
    "从注意力角度来看，可以理解成：当 Q 接收到decoder的第一个注意力块的输出，并且 K 接收到encoder的输出时，注意力权重表示根据encoder的输出赋予decoder输入的重要性。\n",
    "换一种说法，decoder通过查看encoder输出和对其自身输出的自注意`力，预测下一个词。\n",
    "\n",
    "另外 padding mask 的维度是[b,1,1,seq_len] 一般只关注最后一个seq_len维度上pad(无论是在\n",
    "self-attention 或者encoder-decoder attention上都比较容易理解)\n",
    "\n",
    "而 look-ahead mask 是decoder self-attention时用于mask future token的，因为是“自”注意力，\n",
    "所以维度是[...,seq_len, seq_len]，需要自己跟自己运算，所以在QK（此时Q=K=V）矩阵乘的结果的最后2个维度上都需要mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# inp [b, inp_seq_len] 序列已经加入pad填充\n",
    "# targ [b, targ_seq_len] 序列已经加入pad填充\n",
    "def create_mask(inp, targ):\n",
    "    # encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(inp)  # =>[b,1,1,inp_seq_len] mask=1的位置为pad\n",
    "\n",
    "    # decoder's first attention block(self-attention)\n",
    "    # 使用的padding create_mask & look-ahead create_mask\n",
    "    look_ahead_mask = create_look_ahead_mask(targ.shape[-1])  # =>[targ_seq_len,targ_seq_len] ##################\n",
    "    dec_targ_padding_mask = create_padding_mask(targ)  # =>[b,1,1,targ_seq_len]\n",
    "    combined_mask = torch.max(look_ahead_mask, dec_targ_padding_mask)  # 结合了2种mask =>[b,1,targ_seq_len,targ_seq_len]\n",
    "\n",
    "    # decoder's second attention block(encoder-decoder attention) 使用的padding create_mask\n",
    "    # 【注意】：这里的mask是用于遮挡encoder output的填充pad，而encoder的输出与其输入shape都是[b,inp_seq_len,d_model]\n",
    "    # 所以这里mask的长度是inp_seq_len而不是targ_mask_len\n",
    "    dec_padding_mask = create_padding_mask(inp)  # =>[b,1,1,inp_seq_len] mask=1的位置为pad\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
    "    # [b,1,1,inp_seq_len], [b,1,targ_seq_len,targ_seq_len], [b,1,1,inp_seq_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 16.训练和保存\n",
    "\n",
    "### 配置检查点 save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mn_Transformer(\n",
      "  (encoder): mn_Encoder(\n",
      "    (embedding): Embedding(3081, 256, padding_idx=0)\n",
      "    (enc_layers): ModuleList(\n",
      "      (0): mn_EncoderLayer(\n",
      "        (mha): mn_MultiHeadAttention(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wm): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (final_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (layernorm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (layernorm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.5, inplace=False)\n",
      "        (dropout2): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (1): mn_EncoderLayer(\n",
      "        (mha): mn_MultiHeadAttention(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wm): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (final_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (layernorm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (layernorm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.5, inplace=False)\n",
      "        (dropout2): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (2): mn_EncoderLayer(\n",
      "        (mha): mn_MultiHeadAttention(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wm): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (final_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (layernorm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (layernorm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.5, inplace=False)\n",
      "        (dropout2): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (3): mn_EncoderLayer(\n",
      "        (mha): mn_MultiHeadAttention(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wm): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (final_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (layernorm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (layernorm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.5, inplace=False)\n",
      "        (dropout2): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(1783, 256, padding_idx=0)\n",
      "    (dec_layers): ModuleList(\n",
      "      (0): DecoderLayer(\n",
      "        (mha1): MultiHeadAttention(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (final_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (mha2): MultiHeadAttention(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (final_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (layernorm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (layernorm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (layernorm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.5, inplace=False)\n",
      "        (dropout2): Dropout(p=0.5, inplace=False)\n",
      "        (dropout3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (1): DecoderLayer(\n",
      "        (mha1): MultiHeadAttention(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (final_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (mha2): MultiHeadAttention(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (final_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (layernorm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (layernorm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (layernorm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.5, inplace=False)\n",
      "        (dropout2): Dropout(p=0.5, inplace=False)\n",
      "        (dropout3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (2): DecoderLayer(\n",
      "        (mha1): MultiHeadAttention(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (final_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (mha2): MultiHeadAttention(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (final_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (layernorm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (layernorm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (layernorm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.5, inplace=False)\n",
      "        (dropout2): Dropout(p=0.5, inplace=False)\n",
      "        (dropout3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (3): DecoderLayer(\n",
      "        (mha1): MultiHeadAttention(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (final_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (mha2): MultiHeadAttention(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (final_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (layernorm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (layernorm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (layernorm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.5, inplace=False)\n",
      "        (dropout2): Dropout(p=0.5, inplace=False)\n",
      "        (dropout3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (final_layer): Linear(in_features=256, out_features=1783, bias=True)\n",
      ")\n",
      "The model has 13,541,879 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "transformer = mn_Transformer(num_layers,\n",
    "                          d_model,\n",
    "                          num_heads,\n",
    "                          dff,\n",
    "                          input_vocab_size,\n",
    "                          target_vocab_size,\n",
    "                          pe_input=input_vocab_size,\n",
    "                          pe_target=target_vocab_size,\n",
    "                          seq_len=MAX_LENGTH+2,\n",
    "                          rate=dropout_rate)\n",
    "\n",
    "# print(transformer) # 打印模型基本信息\n",
    "\n",
    "transformer = transformer.to(device)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {count_parameters(transformer):,} trainable parameters')\n",
    "\n",
    "\n",
    "if ngpu > 1: # 并行化\n",
    "    transformer = torch.nn.DataParallel(transformer,  device_ids=list(range(ngpu))) # 设置并行执行  device_ids=[0,1]\n",
    "\n",
    "# optimizer = torch.optim.Adam(transformer.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "# lr_scheduler = CustomSchedule(optimizer, d_model, warm_steps=4000)\n",
    "\n",
    "#optimizer = torch.optim.AdamW(transformer.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.025, amsgrad=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(transformer.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.02, amsgrad=True)\n",
    "\n",
    "lr_scheduler = CustomSchedule(optimizer, d_model, warm_steps)\n",
    "lr_scheduler2 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max',factor=0.5,verbose=True,min_lr= 0,patience=1)\n",
    "\n",
    "# lr_scheduler3 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max',factor=0.5,verbose=True,min_lr= 0,patience=1)\n",
    "#optimizer = adamod.AdaMod(transformer.parameters(), lr=1e-3, beta3=0.999)\n",
    "# print('optimizer：', optimizer)\n",
    "\n",
    "#lr_scheduler2 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max',factor=0.5,verbose=True,min_lr= 0,patience=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 关于position encoding的一点自己的理解：\n",
    "\"\"\"\n",
    "为什么这里传入的pe_input是input_vocab_size，也就是说position encoding传入的pos参数是词表大小\n",
    "而我一开始理解的max_seq_len的大小，所以才在encoder中获取位置编码是self.pos_encoding[:, :seq_len, :]\n",
    "但知道看到这里传入的是词表大小我才发现我理解错了\n",
    "\n",
    "其实position encoding 就跟embedding 一样，就是【vocab_size x d_model】的\n",
    "但此时另一个问题又出现了，那为什么在获取position encoding时 使用的是pos_encoding[:, :seq_len, :]而不是\n",
    "像embedding一样 去look-up 查询每个token的位置表示呢？而是直接取得前seq_len个表示作为位置表示？\n",
    "\n",
    "这就体现了这种位置编码的巧妙之处，他不需要训练，就能独特的表示每一个单独的token，而每个token之间又存在关系\n",
    "所以这并不需要位置编码和token是一一对应（固定死的），只需要位置编码能传达这2点信息（即token的独特性和相对依赖性）就够了\n",
    "即使是同一个token 在一个句子的不同的位置时 他的位置编码当然也是不一样的了 如果像embedding那样去查 那一个句子的\n",
    "不同位置的相同token  其表示就会是一样的  就体现不出位置关系\n",
    "\n",
    "这样一想的话，似乎把pe_input设置成max_seq_len也是可以的。但是这里实现position encoding方式（sin  cos）\n",
    "可以在测试阶段接受长度超过训练集实例的情况！所以干脆把pe_input设置成词表大小？\n",
    "\"\"\"\n",
    "\n",
    "# inp [b,inp_seq_len]\n",
    "# targ [b,targ_seq_len]\n",
    "\"\"\"\n",
    "拆分targ, 例如：sentence = \"SOS A lion in the jungle is sleeping EOS\"\n",
    "tar_inp = \"<start>> A lion in the jungle is sleeping\"\n",
    "tar_real = \"A lion in the jungle is sleeping <end>\"\n",
    "\"\"\"\n",
    "def train_step(model, inp, targ):\n",
    "    # 目标（target）被分成了 tar_inp 和 tar_real\n",
    "    # tar_inp 作为输入传递到解码器。\n",
    "    # tar_real 是位移了 1 的同一个输入：在 tar_inp 中的每个位置，tar_real 包含了应该被预测到的下一个标记（token）。\n",
    "    targ_inp = targ[:, :-1]\n",
    "    targ_real = targ[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_mask(inp, targ_inp)\n",
    "\n",
    "    inp = inp.to(device)\n",
    "    targ_inp = targ_inp.to(device)\n",
    "    targ_real = targ_real.to(device)\n",
    "    enc_padding_mask = enc_padding_mask.to(device)\n",
    "    combined_mask = combined_mask.to(device)\n",
    "    dec_padding_mask = dec_padding_mask.to(device)\n",
    "    # print('device:', inp.device, targ_inp)\n",
    "\n",
    "    model.train()  # 设置train mode\n",
    "\n",
    "    optimizer.zero_grad()  # 梯度清零\n",
    "\n",
    "    # forward\n",
    "    prediction, _ = transformer(inp, targ_inp, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "    # [b, targ_seq_len, target_vocab_size]\n",
    "    # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "    #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "\n",
    "    loss = mask_loss_func(targ_real, prediction)\n",
    "    metric = mask_accuracy_func(targ_real, prediction)\n",
    "\n",
    "    # backward\n",
    "    loss.backward()  # 反向传播计算梯度\n",
    "    optimizer.step()  # 更新参数\n",
    "\n",
    "    return loss.item(), metric.item()\n",
    "\n",
    "\n",
    "# # 检查train_step()的效果\n",
    "# batch_src, batch_targ = next(iter(train_dataloader)) # [64,10], [64,10]\n",
    "# print(train_step(transformer, batch_src, batch_targ))\n",
    "# \"\"\"\n",
    "# x += pos_encoding  # [b, inp_seq_len, d_model]\n",
    "# RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0!\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def validate_step(model, inp, targ):\n",
    "    targ_inp = targ[:, :-1]\n",
    "    targ_real = targ[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_mask(inp, targ_inp)\n",
    "\n",
    "    inp = inp.to(device)\n",
    "    targ_inp = targ_inp.to(device)\n",
    "    targ_real = targ_real.to(device)\n",
    "    enc_padding_mask = enc_padding_mask.to(device)\n",
    "    combined_mask = combined_mask.to(device)\n",
    "    dec_padding_mask = dec_padding_mask.to(device)\n",
    "\n",
    "    model.eval()  # 设置eval mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # forward\n",
    "        prediction, _ = model(inp, targ_inp, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "        # [b, targ_seq_len, target_vocab_size]\n",
    "        # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "        #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "\n",
    "        val_loss = mask_loss_func(targ_real, prediction)\n",
    "        val_metric = mask_accuracy_func(targ_real, prediction)\n",
    "\n",
    "    return val_loss.item(), val_metric.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print_trainstep_every = 500  # 每50个step做一次打印\n",
    "\n",
    "metric_name = 'acc'\n",
    "# df_history = pd.DataFrame(columns=['epoch', 'loss', metric_name]) # 记录训练历史信息\n",
    "df_history = pd.DataFrame(columns=['epoch', 'loss', metric_name, 'val_loss', 'val_' + metric_name])\n",
    "\n",
    "\n",
    "# 打印时间\n",
    "def printbar():\n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m_%d %H:%M:%S')\n",
    "    print('\\n' + \"====\"*8 + '%s'%nowtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, epochs, train_dataloader, val_dataloader, print_every):\n",
    "    starttime = time.time()\n",
    "    print('*' * 10, 'start training...')\n",
    "    printbar()\n",
    "\n",
    "    best_acc = 0.\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        loss_sum = 0.\n",
    "        metric_sum = 0.\n",
    "\n",
    "        for step, (inp, targ) in enumerate(train_dataloader, start=1):\n",
    "            # inp [64, 10] , targ [64, 10]\n",
    "            loss, metric = train_step(model, inp, targ)\n",
    "\n",
    "            loss_sum += loss\n",
    "            metric_sum += metric\n",
    "\n",
    "            # 打印batch级别日志\n",
    "            if step % print_every == 0:\n",
    "                print('*' * 5, f'[step = {step}] loss: {loss_sum / step:.5f}, {metric_name}: {metric_sum / step:.5f}')\n",
    "\n",
    "            lr_scheduler.step()  # 更新学习率\n",
    "\n",
    "        # 一个epoch的train结束，做一次验证\n",
    "        # test(model, train_dataloader)\n",
    "        val_loss_sum = 0.\n",
    "        val_metric_sum = 0.\n",
    "        for val_step, (inp, targ) in enumerate(val_dataloader, start=1):\n",
    "            # inp [64, 10] , targ [64, 10]\n",
    "            loss, metric = validate_step(model, inp, targ)\n",
    "\n",
    "            val_loss_sum += loss\n",
    "            val_metric_sum += metric\n",
    "\n",
    "        # 记录和收集1个epoch的训练（和验证）信息\n",
    "        # record = (epoch, loss_sum/step, metric_sum/step)\n",
    "        record = (epoch, loss_sum/step, metric_sum/step, val_loss_sum/val_step, val_metric_sum/val_step)\n",
    "        df_history.loc[epoch - 1] = record\n",
    "\n",
    "        # 打印epoch级别的日志\n",
    "        # print('*'*8, 'EPOCH = {} loss: {:.3f}, {}: {:.3f}'.format(\n",
    "        #        record[0], record[1], metric_name, record[2]))\n",
    "        print('EPOCH = {} loss: {:.5f}, {}: {:.5f}, val_loss: {:.5f}, val_{}: {:.5f}'.format(\n",
    "            record[0], record[1], metric_name, record[2], record[3], metric_name, record[4]))\n",
    "        printbar()\n",
    "        \n",
    "        # 监视loss的同时监视val_acc，防止loss下降而val_loss不下降\n",
    "        lr_scheduler2.step(record[4])\n",
    "        \n",
    "        # 保存模型\n",
    "        # current_acc_avg = metric_sum / step\n",
    "        current_acc_avg = val_metric_sum / val_step # 看验证集指标\n",
    "        \n",
    "        # 每隔几个epoch保存更好的模型，最后的几个epoch只要优于best_acc都保存\n",
    "        if (current_acc_avg > best_acc and epoch%6==0) or (current_acc_avg > best_acc and (epochs + 1 - epoch < 2)):\n",
    "            best_acc = current_acc_avg\n",
    "            checkpoint = train_model_save + '{:03d}_ckpt.tar'.format(epoch)\n",
    "            if device.type == 'cuda' and ngpu > 1:\n",
    "                # model_sd = model.module.state_dict()  ##################\n",
    "                model_sd = copy.deepcopy(model.module.state_dict())\n",
    "            else:\n",
    "                # model_sd = model.state_dict(),  ##################\n",
    "                model_sd = copy.deepcopy(model.state_dict())  ##################\n",
    "            torch.save({\n",
    "                'loss': loss_sum / step,\n",
    "                'epoch': epoch,\n",
    "                'net': model_sd,\n",
    "                'opt': optimizer.state_dict(),\n",
    "                'lr_scheduler': lr_scheduler.state_dict()\n",
    "            }, checkpoint)\n",
    "\n",
    "\n",
    "    print('finishing training...')\n",
    "    endtime = time.time()\n",
    "    time_elapsed = endtime - starttime\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    return df_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** start training...\n",
      "\n",
      "================================2021-10_08 17:23:11\n",
      "Decoder_output torch.Size([100, 31, 256])\n",
      "Decoder_output torch.Size([100, 31, 256])\n",
      "Decoder_output torch.Size([100, 31, 256])\n",
      "Decoder_output torch.Size([100, 31, 256])\n",
      "Decoder_output torch.Size([100, 31, 256])\n",
      "Decoder_output torch.Size([100, 31, 256])\n",
      "Decoder_output torch.Size([72, 31, 256])\n",
      "Decoder_output torch.Size([73, 31, 256])\n",
      "Decoder_output torch.Size([8, 31, 256])\n",
      "Decoder_output torch.Size([8, 31, 256])\n",
      "EPOCH = 1 loss: 7.63737, acc: 0.00061, val_loss: 7.54265, val_acc: 0.00000\n",
      "\n",
      "================================2021-10_08 17:23:12\n",
      "Decoder_output torch.Size([100, 31, 256])\n",
      "Decoder_output torch.Size([100, 31, 256])\n",
      "Decoder_output torch.Size([100, 31, 256])\n",
      "Decoder_output torch.Size([100, 31, 256])\n",
      "Decoder_output torch.Size([100, 31, 256])\n",
      "Decoder_output torch.Size([100, 31, 256])\n",
      "Decoder_output torch.Size([73, 31, 256])\n",
      "Decoder_output torch.Size([72, 31, 256])\n",
      "Decoder_output torch.Size([8, 31, 256])\n",
      "Decoder_output torch.Size([8, 31, 256])\n",
      "EPOCH = 2 loss: 7.62031, acc: 0.00054, val_loss: 7.51656, val_acc: 0.00000\n",
      "\n",
      "================================2021-10_08 17:23:14\n",
      "Decoder_output torch.Size([100, 31, 256])\n",
      "Decoder_output torch.Size([100, 31, 256])\n",
      "Decoder_output torch.Size([100, 31, 256])\n",
      "Decoder_output torch.Size([100, 31, 256])\n",
      "Decoder_output torch.Size([100, 31, 256])\n",
      "Decoder_output torch.Size([100, 31, 256])\n",
      "Decoder_output torch.Size([72, 31, 256])\n",
      "Decoder_output torch.Size([73, 31, 256])\n",
      "Decoder_output torch.Size([8, 31, 256])\n",
      "Decoder_output torch.Size([8, 31, 256])\n",
      "EPOCH = 3 loss: 7.60236, acc: 0.00076, val_loss: 7.47502, val_acc: 0.00000\n",
      "\n",
      "================================2021-10_08 17:23:15\n",
      "Epoch     3: reducing learning rate of group 0 to 2.4724e-06.\n",
      "finishing training...\n",
      "Training complete in 0m 5s\n",
      "   epoch      loss       acc  val_loss  val_acc\n",
      "0    1.0  7.637365  0.000614  7.542653      0.0\n",
      "1    2.0  7.620306  0.000538  7.516562      0.0\n",
      "2    3.0  7.602359  0.000761  7.475015      0.0\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "df_history = train_model(transformer, EPOCHS, train_dataloader, val_dataloader, print_trainstep_every)\n",
    "\n",
    "# 保存df_history变量\n",
    "save_file = open(result_save + \"transformer_df_history.bin\", \"wb\")\n",
    "pickle.dump(df_history, save_file)\n",
    "save_file.close()\n",
    "print(df_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制训练曲线\n",
    "def plot_metric(df_history, metric):\n",
    "    plt.figure()\n",
    "\n",
    "    train_metrics = df_history[metric]\n",
    "    val_metrics = df_history['val_' + metric]  #\n",
    "\n",
    "    epochs = range(1, len(train_metrics) + 1)\n",
    "\n",
    "    plt.plot(epochs, train_metrics, 'bo--')\n",
    "    plt.plot(epochs, val_metrics, 'ro-')  #\n",
    "\n",
    "    plt.title('Training and validation ' + metric)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([\"train_\" + metric, 'val_' + metric])\n",
    "    plt.savefig(result_save + metric + '.png')  # 保存图片\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "plot_metric(df_history, 'loss')\n",
    "plot_metric(df_history, metric_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 17.评估\n",
    "这里的评估 没有额外的使用测试集测评，还是拿验证集测试的。另外可以对一个法语句子进行翻译，看看翻译的结果如何\n",
    "\n",
    "以下步骤用于评估：\n",
    "- 用法语分词器（tokenizer_pt）编码输入语句。此外，添加<start>和<end>标记，这样输入就与模型训练的内容相同。这是编码器输入。\n",
    "- 解码器输入为 <start> token id\n",
    "- 计算padding mask 和 look ahead mask\n",
    "- 解码器通过查看编码器输出和它自身的输出（自注意力）给出预测。\n",
    "- 选择最后一个词并计算它的 argmax。\n",
    "- 将预测的词concat到解码器输入，然后传递给解码器。\n",
    "- 在这种方法中，解码器根据它之前预测的words预测下一个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-446-a36ac38d779b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_model_save\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model_save\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# checkpoint = \"/home/chengkun/jupyter_projects/Magic-NLPer-main/best_save/2021-09-19_21:00:40_34.270/059_ckpt.tar\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# 加载model\n",
    "a=os.listdir(train_model_save)\n",
    "a.sort()\n",
    "print(a[-1])\n",
    "checkpoint = train_model_save + a[-1]\n",
    "# checkpoint = \"/home/chengkun/jupyter_projects/Magic-NLPer-main/best_save/2021-09-19_21:00:40_34.270/059_ckpt.tar\"\n",
    "print('checkpoint:', checkpoint)\n",
    "\n",
    "# ckpt = torch.load(checkpoint, map_location=device)  # dict  save 在 CPU 加载到GPU\n",
    "ckpt = torch.load(checkpoint)  # dict  save 在 GPU 加载到 GPU\n",
    "# print('ckpt', ckpt)\n",
    "transformer_sd = ckpt['net']\n",
    "# optimizer_sd = ckpt['opt'] # 不重新训练的话不需要\n",
    "# lr_scheduler_sd = ckpt['lr_scheduler']\n",
    "\n",
    "reload_model = mn_Transformer(num_layers,\n",
    "                           d_model,\n",
    "                           num_heads,\n",
    "                           dff,\n",
    "                           input_vocab_size,\n",
    "                           target_vocab_size,\n",
    "                           pe_input=input_vocab_size,\n",
    "                           pe_target=target_vocab_size,\n",
    "                           seq_len=MAX_LENGTH+2,\n",
    "                           rate=dropout_rate)\n",
    "\n",
    "reload_model = reload_model.to(device)\n",
    "# reload_model.load_state_dict(transformer_sd)\n",
    "\n",
    "if ngpu > 1:\n",
    "    reload_model = torch.nn.DataParallel(reload_model,  device_ids=list(range(ngpu))) # 设置并行执行  device_ids=[0,1]\n",
    "\n",
    "\n",
    "print('Loading model ...')\n",
    "if device.type == 'cuda' and ngpu > 1:\n",
    "   reload_model.module.load_state_dict(transformer_sd)\n",
    "else:\n",
    "   reload_model.load_state_dict(transformer_sd)\n",
    "print('Model loaded ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test(model, dataloader):\n",
    "    # model.eval() # 设置为eval mode\n",
    "\n",
    "    test_loss_sum = 0.\n",
    "    test_metric_sum = 0.\n",
    "    for test_step, (inp, targ) in enumerate(dataloader, start=1):\n",
    "        # inp [64, 10] , targ [64, 10]\n",
    "        loss, metric = validate_step(model, inp, targ)\n",
    "        # print('*'*8, loss, metric)\n",
    "\n",
    "        test_loss_sum += loss\n",
    "        test_metric_sum += metric\n",
    "    # 打印\n",
    "    print('*' * 8,\n",
    "          'Test: loss: {:.5f}, {}: {:.5f}'.format(test_loss_sum / test_step, 'test_acc', test_metric_sum / test_step))\n",
    "\n",
    "\n",
    "# 在测试集上测试指标，这里使用val_dataloader模拟测试集\n",
    "print('*' * 8, 'final test...')\n",
    "test(reload_model, test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def tokenizer_encode(tokenize, sentence, vocab):\n",
    "    # print(type(vocab)) # torchtext.vocab.Vocab\n",
    "    # print(len(vocab))\n",
    "    sentence = normalizeString(sentence)\n",
    "    # print(type(sentence)) # str\n",
    "    sentence = tokenize(sentence)  # list\n",
    "#     print(sentence)\n",
    "    sentence = ['<start>'] + sentence + ['<end>']\n",
    "    if len(sentence) < MAX_LENGTH + 2:\n",
    "        sentence = sentence + (MAX_LENGTH + 2 - len(sentence))* ['<pad>']\n",
    "#     print(sentence)\n",
    "    sentence_ids = [vocab.stoi[token] for token in sentence]\n",
    "    # print(sentence_ids, type(sentence_ids[0])) # int\n",
    "    return sentence_ids\n",
    "\n",
    "\n",
    "def tokenzier_decode(sentence_ids, vocab):\n",
    "    sentence = [vocab.itos[id] for id in sentence_ids if id<len(vocab)]\n",
    "    # print(sentence)\n",
    "    return \" \".join(sentence)\n",
    "\n",
    "# 只有一个句子，不需要加pad\n",
    "s = 'ᠲᠦᠩᠯᠢᠶᠣᠣ ᠬᠣᠲᠠ ᠶ᠋ᠢᠨ ᡁᠢ ᠶᠤᠸᠠᠨ ᠠᠱᠢᠨ ᠲᠡᠷᠭᠡ ᠲᠦᠷᠢᠶᠡᠰᠦᠯᠡᠭᠦᠯᠬᠦ ᠬᠢᠵᠠᠭᠠᠷᠲᠤ ᠺᠣᠮᠫᠠᠨᠢ'\n",
    "print(tokenizer_encode(tokenizer, s, SRC_TEXT.vocab))\n",
    "\n",
    "\n",
    "s_ids = [3, 5, 251, 17, 46, 35, 12, 36, 4, 2]\n",
    "print(tokenzier_decode(s_ids, SRC_TEXT.vocab))\n",
    "print(tokenzier_decode(s_ids, TARG_TEXT.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# inp_sentence 一个法语句子，例如\"je pars en vacances pour quelques jours .\"\n",
    "def evaluate(model, inp_sentence):\n",
    "    model.eval()  # 设置eval mode\n",
    "\n",
    "    inp_sentence_ids = tokenizer_encode(tokenizer, inp_sentence, SRC_TEXT.vocab)  # 转化为索引\n",
    "#     print(tokenzier_decode(inp_sentence_ids, SRC_TEXT.vocab))\n",
    "    encoder_input = torch.tensor(inp_sentence_ids).unsqueeze(dim=0)  # =>[b=1, inp_seq_len=10]\n",
    "#     print('encoder_input.shape：', encoder_input.shape)\n",
    "\n",
    "    decoder_input = [TARG_TEXT.vocab.stoi['<start>']]\n",
    "    decoder_input = torch.tensor(decoder_input).unsqueeze(0)  # =>[b=1,seq_len=1]\n",
    "#     print('decoder_input.shape：', decoder_input.shape)\n",
    "#     print(MAX_LENGTH)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(MAX_LENGTH + 2):\n",
    "            enc_padding_mask, combined_mask, dec_padding_mask = create_mask(encoder_input.cpu(), decoder_input.cpu()) ################\n",
    "            # [b,1,1,inp_seq_len], [b,1,targ_seq_len,inp_seq_len], [b,1,1,inp_seq_len]\n",
    "\n",
    "            encoder_input = encoder_input.to(device)\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            enc_padding_mask = enc_padding_mask.to(device)\n",
    "            combined_mask = combined_mask.to(device)\n",
    "            dec_padding_mask = dec_padding_mask.to(device)\n",
    "\n",
    "            # forward\n",
    "            predictions, attention_weights = model(encoder_input,\n",
    "                                                   decoder_input,\n",
    "                                                   enc_padding_mask,\n",
    "                                                   combined_mask,\n",
    "                                                   dec_padding_mask)\n",
    "            # [b=1, targ_seq_len, target_vocab_size]\n",
    "            # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "            #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "\n",
    "            # 看最后一个词并计算它的 argmax\n",
    "            prediction = predictions[:, -1:, :]  # =>[b=1, 1, target_vocab_size]\n",
    "            prediction_id = torch.argmax(prediction, dim=-1)  # => [b=1, 1]\n",
    "            # print('prediction_id:', prediction_id, prediction_id.dtype) # torch.int64\n",
    "            if prediction_id.squeeze().item() == TARG_TEXT.vocab.stoi['<end>']:\n",
    "                return decoder_input.squeeze(dim=0), attention_weights\n",
    "\n",
    "            decoder_input = torch.cat([decoder_input, prediction_id],\n",
    "                                      dim=-1)  # [b=1,targ_seq_len=1]=>[b=1,targ_seq_len=2]\n",
    "            # decoder_input在逐渐变长\n",
    "\n",
    "    return decoder_input.squeeze(dim=0), attention_weights\n",
    "    # [targ_seq_len],\n",
    "    # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "    #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "\n",
    "\n",
    "\n",
    "# # s = 'je pars en vacances pour quelques jours .'\n",
    "# # evaluate(s)\n",
    "\n",
    "s = 'ᠥᠪᠥᠷ ᠣᠩᠭᠣᠯ ᠤ᠋ᠨ ᠱᠸᠩ ᠾᠧ ᠠᠱᠢᠨ ᠲᠡᠷᠭᠡᠨ ᠦ᠌ ᠦᠢᠯᠡᠴᠢᠯᠡᠭᠡ ᠶ᠋ᠢᠨ ᠬᠢᠵᠠᠭᠠᠷᠲᠤ ᠺᠣᠮᠫᠠᠨᠢ'\n",
    "\n",
    "s_targ = '内 蒙 古 盛 和 汽 车 服 务 有 限 公 司'\n",
    "pred_result, attention_weights = evaluate(reload_model, s)\n",
    "pred_sentence = tokenzier_decode(pred_result, TARG_TEXT.vocab)\n",
    "print('real target:', s_targ)\n",
    "print('pred_sentence:', pred_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 批量翻译\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "sentence_pairs = test_pairs\n",
    "print('测试集句子数目：', len(sentence_pairs))\n",
    "\n",
    "# total_score = 0\n",
    "number = 0.0\n",
    "f1 = open(result_save + 'target1.txt','w', encoding='utf-8')\n",
    "f2 = open(result_save + 'pred1.txt','w', encoding='utf-8')\n",
    "for pair in sentence_pairs:\n",
    "    pred_result, _ = evaluate(reload_model, pair[0])\n",
    "    pred_sentence = tokenzier_decode(pred_result, TARG_TEXT.vocab).replace('<start>','').replace('<end>','').replace('\\n','')\n",
    "\n",
    "    f1.write(pair[1] + '\\n')\n",
    "    f2.write(pred_sentence + '\\n')\n",
    "\n",
    "    number = number + 1\n",
    "    if number%1000 ==0:\n",
    "        print(number)\n",
    "\n",
    "f1.close()\n",
    "f2.close()\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 随机选个句子翻译\n",
    "def evaluateRandomly(n=5):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('input:', pair[0])\n",
    "        print('target:', pair[1])\n",
    "        pred_result, attentions = evaluate(reload_model, pair[0])\n",
    "        pred_sentence = tokenzier_decode(pred_result, TARG_TEXT.vocab)\n",
    "        print('pred:', pred_sentence)\n",
    "        print('')\n",
    "\n",
    "\n",
    "evaluateRandomly(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 18.attention 的可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 可视化attenton 这里我们只展示...block2的attention，即[b, num_heads, targ_seq_len, inp_seq_len]\n",
    "# attention: {'decoder_layer{i + 1}_block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "#             'decoder_layer{i + 1}_block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "# sentence: [seq_len]，例如：'je recherche un assistant .'\n",
    "# pred_result: [seq_len]，例如：'<start> i m looking for an assistant .'\n",
    "# layer: 字符串类型，表示模型decoder的N层decoder-layer的第几层的attention，形如'decoder_layer{i}_block1'或'decoder_layer{i}_block2'\n",
    "def plot_attention_weights(attention, sentence, pred_sentence, layer):\n",
    "    \n",
    "    # block2 attention[layer] => [b=1, num_heads, targ_seq_len, inp_seq_len]\n",
    "    attention = torch.squeeze(attention[layer], dim=0) # => [num_heads, targ_seq_len, inp_seq_len]\n",
    "#     print(attention.shape)\n",
    "\n",
    "    # print(matplotlib.matplotlib_fname())\n",
    "    plt.rcParams['font.sans-serif'] = ['Mongolian Baiti']\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    \n",
    "    sentence = sentence.split()\n",
    "    pred_sentence = pred_sentence.split()\n",
    "\n",
    "    fig = plt.figure(figsize=(len(pred_sentence), len(sentence)+2)) #figsize=(attention.shape[1], attention.shape[2])\n",
    "\n",
    "    for head in range(attention.shape[0]):\n",
    "        ax = fig.add_subplot(2, 4, head + 1)  # 111是单个整数编码的子绘图网格参数。例如，“111”表示“1×1网格，第一子图”，“234”表示“2×3网格，第四子图”\n",
    "\n",
    "        cax = ax.matshow(attention[head].cpu(), cmap='viridis')  # 绘制网格热图？注意力权重\n",
    "        # fig.colorbar(cax)#给子图添加colorbar（颜色条或渐变色条）\n",
    "\n",
    "        fontdict = {'fontsize': 10}\n",
    "\n",
    "        # 设置轴刻度线\n",
    "        ax.set_xticks(range(len(sentence)+2))  # 算上start和end\n",
    "        ax.set_yticks(range(len(pred_sentence)))\n",
    "        \n",
    "        ax.set_xlim(-0.5,len(sentence) + 0.5)  # 设定x座标轴的范围\n",
    "        ax.set_ylim(len(pred_sentence) -0.5, -0.5)  # 设定y座标轴的范围\n",
    "\n",
    "        # 设置轴\n",
    "        ax.set_xticklabels(['<start>']+sentence+['<end>'], fontdict=fontdict, rotation=90)  # 顺时间旋转90度\n",
    "        ax.set_yticklabels(pred_sentence, fontdict=fontdict, family = 'SimHei')\n",
    "\n",
    "        ax.set_xlabel('Head {}'.format(head + 1))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(result_save+'attention_{}.pdf'.format(layer),format='pdf',bbox_inches='tight')\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "def translate(sentence_pair, plot=None):\n",
    "    print('input:', sentence_pair[0])\n",
    "    print('target:', sentence_pair[1])\n",
    "    pred_result, attention_weights = evaluate(reload_model, sentence_pair[0])\n",
    "    print('attention_weights:', attention_weights.keys())\n",
    "    pred_sentence = tokenzier_decode(pred_result, TARG_TEXT.vocab)\n",
    "    print('pred:', pred_sentence)\n",
    "    print('')\n",
    "\n",
    "    if plot:\n",
    "        plot_attention_weights(attention_weights, sentence_pair[0], pred_sentence, plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# translate(sentence_pairs[481], 'decoder_layer5_block2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = os.popen('/home/chengkun/moses/mosesdecoder/scripts/generic/multi-bleu.perl -lc /home/chengkun/jupyter_projects/Magic-NLPer-main/best_save/{}/target1.txt < /home/chengkun/jupyter_projects/Magic-NLPer-main/best_save/{}/pred1.txt'.format(begin_time,begin_time))\n",
    "print(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = []\n",
    "with open('/home/chengkun/jupyter_projects/Magic-NLPer-main/best_save/{}/target1.txt'.format(begin_time),'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        target.append(line.strip('\\n'))#去掉列表中每一个元素的换行符\n",
    "target = [target]\n",
    "f.close()\n",
    "\n",
    "pred = []\n",
    "with open('/home/chengkun/jupyter_projects/Magic-NLPer-main/best_save/{}/pred1.txt'.format(begin_time),'r', encoding='utf-8') as f1:\n",
    "    for line in f1.readlines():\n",
    "        pred.append(line.strip('\\n'))#去掉列表中每一个元素的换行符\n",
    "\n",
    "f1.close()\n",
    "\n",
    "\n",
    "bleu = sacrebleu.corpus_bleu(pred, target,smooth_method='none')\n",
    "bleu_score = format(bleu.score,'.3f')\n",
    "print(bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.system('cp  {} {}'.format(checkpoint,result_save))\n",
    "os.system('cp  {} {}'.format('/home/chengkun/jupyter_projects/Magic-NLPer-main/re/primer-ez/pytask-primer.log',result_save))\n",
    "os.system('cp  {} {}'.format('/home/chengkun/jupyter_projects/Magic-NLPer-main/train_save/transformer_improved_encoder/*',result_save))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(sentence_pairs[481], 'decoder_layer5_block2')\n",
    "translate(sentence_pairs[481], 'decoder_layer5_block1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.rename(result_save[:-1], result_save[:-1]+'_'+str(bleu_score)+'_'+str(seed)+'_'+file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 19.总结\n",
    "\n",
    "代码还有以下待完善的地方：\n",
    "- 没有实现标签平滑\n",
    "- 在训练过程中使用了teacher-forcing，即总是会将target传递到下一时间步长。更好的做法是设置一个teacher_forcing_ration\n",
    "- 在evaluate阶段的解码使用的是greedy search decode，即对于每一步，我们只需从具有最高 softmax 值的 decoder_output 中选择单词。可以尝试使用更好的beam search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
