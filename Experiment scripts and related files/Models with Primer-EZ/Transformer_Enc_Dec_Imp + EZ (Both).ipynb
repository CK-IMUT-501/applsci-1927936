{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "transformer的核心：self-attentions\n",
    "\n",
    "transformer **优点**：\n",
    "- 无需对跨数据的时间/空间域的关系作出假设\n",
    "- 并行计算\n",
    "- distant item 互相影响彼此的输出\n",
    "- 可以学习到长距离依赖关系\n",
    "\n",
    "transformer **缺点**：\n",
    "- 对于每一个step的xt的输出，是由整个历史信息计算得出，而不再是当前输入和hidden，这可能效率较低\n",
    "- 如果输入具有时间/空间域的关系，则需要加入位置编码，否则整个model也只能看作是一个词袋模型\n",
    "\n",
    "目录\n",
    "* [1.加载数据 建立input pipeline](#)\n",
    "* [2.位置编码 positional encoding](#)\n",
    "* [3.掩码 masking](#3)\n",
    "* [4.scaled dot product attention](#)\n",
    "* [5.multi-head attention](#)\n",
    "* [6.point wise feed forward network](#)\n",
    "* [7.encoder layer](#)\n",
    "* [8.decoder layer](#)\n",
    "* [9.encoder](#)\n",
    "* [10.decoder](#)\n",
    "* [11.搭建transformer](#)\n",
    "* [12.设置超参](#)\n",
    "* [13.优化器](#)\n",
    "* [14.损失和评价准则](#)\n",
    "* [15.生成mask](#)\n",
    "* [16.训练和保存](#)\n",
    "* [17.评估](#)\n",
    "* [18.attention的可视化](#)\n",
    "* [19.总结](#)\n",
    "\n",
    "\n",
    "## 1.加载数据 建立input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer_improved_encoder_decoder_zero1\n",
      "2021-12-15_17:27:45\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchtext\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import random\n",
    "import re\n",
    "# from tqdm import tqdm  # 进度条\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import unicodedata\n",
    "import datetime\n",
    "import time\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import sacrebleu\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import pickle\n",
    "import torch_optimizer as optim\n",
    "# import adamod\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "from labml_helpers.module import Module\n",
    "from labml_nn.transformers import MultiHeadAttention\n",
    "\n",
    "# print(os.getcwd()) # /home/xijian\n",
    "file_name = 'transformer_improved_encoder_decoder_zero1'\n",
    "print(file_name)\n",
    "\n",
    "train_model_save = '/home/chengkun/jupyter_projects/Magic-NLPer-main/train_save/transformer_improved_encoder_decoder_zero/'\n",
    "shutil.rmtree(train_model_save)\n",
    "os.mkdir(train_model_save)\n",
    "\n",
    "#保存必要结果\n",
    "begin_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()).replace(' ','_')\n",
    "# begin_time = '2021-12-15_17:27:45'\n",
    "print(begin_time)\n",
    "result_save = '/home/chengkun/jupyter_projects/Magic-NLPer-main/best_save/' + begin_time +'/'\n",
    "os.mkdir(result_save)\n",
    "\n",
    "os.system('cp  {} {} {}'.format('/home/chengkun/jupyter_projects/Magic-NLPer-main/re/primer-ez/transformer_improved_encoder_decoder_zero1.py','/home/chengkun/jupyter_projects/Magic-NLPer-main/re/primer-ez/transformer_improved_encoder_decoder_zero1.ipynb',result_save))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquaredReLU(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.relu = torch.nn.LeakyReLU()\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.relu(x)\n",
    "        return x * x\n",
    "\n",
    "class SpatialDepthWiseConvolution(Module):\n",
    "    def __init__(self, d_k: int, kernel_size: int = 3):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = torch.nn.Conv1d(in_channels=d_k, out_channels=d_k,\n",
    "                              kernel_size=(kernel_size,), padding=(kernel_size - 1,), groups=d_k)\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x.unsqueeze(-1).permute(1, 0, 3, 2)\n",
    "        seq_len, batch_size, heads, d_k= x.shape\n",
    "        x = x.permute(1, 2, 3, 0)\n",
    "        x = x.view(batch_size * heads, d_k, seq_len)\n",
    "        x = self.conv(x)\n",
    "        x = x[:, :, :-(self.kernel_size - 1)]\n",
    "        x = x.view(batch_size, heads, d_k, seq_len)\n",
    "        x = x.permute(0, 3, 2, 1) # [batch_size, seq_len, heads, d_k]\n",
    "        x = x.view(batch_size, seq_len, heads, d_k)\n",
    "        x = torch.squeeze(x,2)\n",
    "        return x\n",
    "\n",
    "# class MultiDConvHeadAttention(MultiHeadAttention):\n",
    "#     def __init__(self, heads: int, d_model: int, dropout_prob: float = 0.1):\n",
    "#         super().__init__(heads, d_model, dropout_prob)\n",
    "        \n",
    "#         self.query = torch.nn.Sequential(self.query, SpatialDepthWiseConvolution(self.d_k))\n",
    "#         self.key = torch.nn.Sequential(self.key, SpatialDepthWiseConvolution(self.d_k))\n",
    "#         self.value = torch.nn.Sequential(self.value, SpatialDepthWiseConvolution(self.d_k))\n",
    "        \n",
    "# m = SquaredReLU()\n",
    "# n = torch.nn.LeakyReLU()\n",
    "# input = torch.randn(2)\n",
    "# print(m(input))\n",
    "# print(n(input)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class dependency_parsing_Attention(torch.nn.Module):\n",
    "#     def __init__(self, d_model):\n",
    "#         super(dependency_parsing_Attention, self).__init__()\n",
    "\n",
    "#         self.d_model = d_model\n",
    "\n",
    "#         self.wq = torch.nn.Linear(d_model, d_model)\n",
    "#         self.wk = torch.nn.Linear(d_model, d_model)\n",
    "#         self.wv = torch.nn.Linear(d_model, d_model)\n",
    "#         self.SDWC = SpatialDepthWiseConvolution(d_k=d_model,kernel_size=3)\n",
    "#         #primer_ez\n",
    "#         self.final_linear = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "#     def forward(self, q, k, v,dependency_parsing_matrix,mask):  # q=k=v=x [b, seq_len, embedding_dim] embedding_dim其实也=d_model\n",
    "#         batch_size = q.shape[0]\n",
    "        \n",
    "#         q = self.wq(q)  # =>[b, seq_len, d_model]\n",
    "#         print('q',q.shape)\n",
    "#         q = self.SDWC(q)\n",
    "#         k = self.wk(k)  # =>[b, seq_len, d_model]\n",
    "#         k = self.SDWC(k)\n",
    "#         v = self.wv(v)  # =>[b, seq_len, d_model]\n",
    "#         v = self.SDWC(v)\n",
    "# #         print('q:',q.shape)\n",
    "# #         print('k:',q.shape)\n",
    "# #         print('v:',q.shape)\n",
    "\n",
    "\n",
    "#         scaled_attention, attention_weights = dependency_parsing_scaled_dot_product_attention(q, k, v, dependency_parsing_matrix, mask)\n",
    "#         # => [b, seq_len_q, d_model=512], [b, seq_len_q, seq_len_k]\n",
    "\n",
    "#         output = self.final_linear(scaled_attention)  # =>[b, seq_len_q, d_model=512]\n",
    "#         return output, attention_weights  # [b, seq_len_q, d_model=512], [b, seq_len_q, seq_len_k]\n",
    "\n",
    "# b = 120\n",
    "# s = 100\n",
    "# d = 256\n",
    "# x = torch.rand(b, s, d) # [b,seq_len,d_model]\n",
    "# dependency_parsing_matrix = torch.rand(b,s,s)\n",
    "# temp_mha = dependency_parsing_Attention(d_model=d)\n",
    "# out, attn_weights = temp_mha(x, x, x, dependency_parsing_matrix, mask=None)\n",
    "# print(out.shape, attn_weights.shape) # [1, 60, 512], [1, 8, 60, 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.rand(120, 100, 256).unsqueeze(-1) # [b,seq_len,d_model]\n",
    "# print(x.shape)\n",
    "# x = x.permute(1, 0, 3, 2)\n",
    "# print(x.shape)\n",
    "# x = x.squeeze()\n",
    "# print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu0: 3 gpu1: 0\n"
     ]
    }
   ],
   "source": [
    "gpu0 = int(sys.argv[1])\n",
    "gpu1 = int(sys.argv[2])\n",
    "seed = int(sys.argv[3])\n",
    "\n",
    "# gpu0 = 3\n",
    "# gpu1 = 0\n",
    "# seed = 100\n",
    "print('gpu0:',gpu0,'gpu1:',gpu1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngpu： 2\n",
      "batch： 120\n",
      "MAX_LENGTH： 100\n",
      "EPOCHS： 35\n",
      "warm_steps： 3000\n",
      "seed： 100\n"
     ]
    }
   ],
   "source": [
    "# 设置超参数\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"{},{}\".format(gpu0, gpu1)\n",
    "ngpu = 2\n",
    "print('ngpu：', ngpu)\n",
    "\n",
    "batch = 120\n",
    "print('batch：', batch)\n",
    "\n",
    "# MAX_LENGTH = d_model//num_heads\n",
    "MAX_LENGTH = 100\n",
    "print('MAX_LENGTH：', MAX_LENGTH)\n",
    "EPOCHS = 35\n",
    "print('EPOCHS：', EPOCHS)\n",
    "warm_steps=3000\n",
    "print('warm_steps：', warm_steps)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()  #检测是否有可用的gpu\n",
    "device = torch.device(\"cuda:0\" if (use_cuda and ngpu > 0) else \"cpu\")\n",
    "\n",
    "# seed = 256\n",
    "print('seed：', seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "# torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 数据读取\n",
    "# 当你用read_csv读文件的时候，如果文本里包含英文双引号，直接读取会导致行数变少或是直接如下报错停止\n",
    "# 此时应该对read_csv设置参数控制csv中的引号常量，设定quoting=3或是quoting=csv.QUOTE_NONE”（注：用第二种要先导csv库）然后问题就解决了。\n",
    "\n",
    "data_dir = '/home/chengkun/jupyter_projects/Magic-NLPer-main/data/' \n",
    "\n",
    "data_df = pd.read_csv(data_dir + 'ch_mn_50_nodict.txt',  # 数据格式：英语\\t法语，注意我们的任务源语言是法语，目标语言是英语\n",
    "                      encoding='UTF-8', sep='\\t', header=None,quoting=3,\n",
    "                      names=['mn', 'ch'], index_col=False)\n",
    "\n",
    "# print(data_df.shape)\n",
    "# print(data_df.values.shape)\n",
    "# print(data_df.values[0])\n",
    "# print(data_df.values[0].shape)\n",
    "# data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "\n",
    "# 规范化字符串\n",
    "def normalizeString(s):\n",
    "    # print(s) # list  ['Go.']\n",
    "    # s = s[0]\n",
    "    s = s.lower().strip()\n",
    "    #s = unicodeToAscii(s)\n",
    "    #s = re.sub(r\"([.!?])\", r\" \\1\", s)  # \\1表示group(1)即第一个匹配到的 即匹配到'.'或者'!'或者'?'后，一律替换成'空格.'或者'空格!'或者'空格？'\n",
    "    #s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)  # 非字母以及非.!?的其他任何字符 一律被替换成空格\n",
    "    s = re.sub(r'[\\s]+', \" \", s)  # 将出现的多个空格，都使用一个空格代替。例如：w='abc  1   23  1' 处理后：w='abc 1 23 1'\n",
    "    return s\n",
    "\n",
    "\n",
    "# print(normalizeString('Va !'))\n",
    "# print(normalizeString('Go.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairs num= 500000\n"
     ]
    }
   ],
   "source": [
    "pairs = [[normalizeString(s) for s in line] for line in data_df.values]\n",
    "\n",
    "print('pairs num=', len(pairs))\n",
    "# print(pairs[0])\n",
    "# print(pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "经过过滤后平行语料数目为： 498921\n"
     ]
    }
   ],
   "source": [
    "# 文件是英译法，我们实现的是法译英，所以进行了reverse，所以pair[1]是英语\n",
    "# 为了快速训练，仅保留“我是”“你是”“他是”等简单句子，并且删除原始文本长度大于10个标记的样本\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH \n",
    "\n",
    "def filterPairs(pairs):\n",
    "    # 过滤，并交换句子顺序，得到法英句子对（之前是英法句子对）\n",
    "    return [[pair[1], pair[0]] for pair in pairs if filterPair(pair)]\n",
    "\n",
    "\n",
    "pairs = filterPairs(pairs)\n",
    "\n",
    "print('经过过滤后平行语料数目为：', len(pairs))\n",
    "# print(pairs[0])\n",
    "# print(random.choice(pairs))\n",
    "# print(np.array(pairs).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集句子数目： 479163\n",
      "验证集句子数目： 9979\n",
      "测试集句子数目： 9779\n"
     ]
    }
   ],
   "source": [
    "# 划分数据集：训练集和验证集\n",
    "##0.0338 0.03485\n",
    "##50 0.020 0.020\n",
    "train_test, val_pairs = train_test_split(pairs, test_size=0.020, random_state=1234)\n",
    "train_pairs, test_pairs = train_test_split(train_test, test_size=0.020, random_state=1234)\n",
    "\n",
    "print('训练集句子数目：', len(train_pairs))\n",
    "print('验证集句子数目：', len(val_pairs))\n",
    "print('测试集句子数目：', len(test_pairs))\n",
    "# print(test_pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(r'/home/chengkun/jupyter_projects/Magic-NLPer-main/data/data_sets/train_pairs','wb') as f:\n",
    "#     pickle.dump(train_pairs, f)\n",
    "#     f.close()\n",
    "\n",
    "# with open(r'/home/chengkun/jupyter_projects/Magic-NLPer-main/data/data_sets/val_pairs','wb') as f:\n",
    "#     pickle.dump(val_pairs, f)\n",
    "#     f.close()\n",
    "\n",
    "# with open(r'/home/chengkun/jupyter_projects/Magic-NLPer-main/data/data_sets/test_pairs','wb') as f:\n",
    "#     pickle.dump(test_pairs, f)\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(r'/home/chengkun/jupyter_projects/Magic-NLPer-main/data/data_sets/train_pairs','rb') as f:\n",
    "#     train_pairs = pickle.load(f)\n",
    "#     f.close()\n",
    "\n",
    "# with open(r'/home/chengkun/jupyter_projects/Magic-NLPer-main/data/data_sets/val_pairs','rb') as f:\n",
    "#     val_pairs = pickle.load(f)\n",
    "#     f.close()\n",
    "\n",
    "# with open(r'/home/chengkun/jupyter_projects/Magic-NLPer-main/data/data_sets/test_pairs','rb') as f:\n",
    "#     test_pairs = pickle.load(f)\n",
    "#     f.close()\n",
    "\n",
    "# print('训练集句子数目：', len(train_pairs))\n",
    "# print('验证集句子数目：', len(val_pairs))\n",
    "# print('测试集句子数目：', len(test_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = lambda x: x.split() # 分词器\n",
    "\n",
    "SRC_TEXT = torchtext.data.Field(sequential=True,\n",
    "                                tokenize=tokenizer,\n",
    "                                # lower=True,\n",
    "                                fix_length=MAX_LENGTH + 2,\n",
    "                                preprocessing=lambda x: ['<start>'] + x + ['<end>'],\n",
    "                                # after tokenizing but before numericalizing\n",
    "                                # postprocessing # after numericalizing but before the numbers are turned into a Tensor\n",
    "                                )\n",
    "TARG_TEXT = torchtext.data.Field(sequential=True,\n",
    "                                 tokenize=tokenizer,\n",
    "                                 # lower=True,\n",
    "                                 fix_length=MAX_LENGTH + 2,\n",
    "                                 preprocessing=lambda x: ['<start>'] + x + ['<end>'],\n",
    "                                 )\n",
    "\n",
    "\n",
    "def get_dataset(pairs, src, targ):\n",
    "    fields = [('src', src), ('targ', targ)]  # filed信息 fields dict[str, Field])\n",
    "    examples = []  # list(Example)\n",
    "    for mn, ch in pairs: # 进度条\n",
    "        # 创建Example时会调用field.preprocess方法\n",
    "        examples.append(torchtext.data.Example.fromlist([mn, ch], fields))\n",
    "    return examples, fields\n",
    "\n",
    "\n",
    "# examples, fields = get_dataset(pairs, SRC_TEXT, TARG_TEXT)\n",
    "\n",
    "ds_train = torchtext.data.Dataset(*get_dataset(train_pairs, SRC_TEXT, TARG_TEXT))\n",
    "ds_val = torchtext.data.Dataset(*get_dataset(val_pairs, SRC_TEXT, TARG_TEXT))\n",
    "ds_test = torchtext.data.Dataset(*get_dataset(test_pairs, SRC_TEXT, TARG_TEXT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds_train\n",
      "20 ['<start>', 'ᠴᠢᠯᠠᠭᠤᠨ', 'ᠲᠣᠰᠣ', 'ᠭᠠᠳᠠᠭᠱᠢᠯᠠᠭᠤᠯᠬᠤ', 'ᠤᠯᠤᠰ', 'ᠤ᠋ᠨ', 'ᠵᠣᠬᠢᠶᠠᠨ', 'ᠪᠠᠢᠭᠤᠯᠤᠯᠲᠠ', 'ᠴᠢᠯᠠᠭᠤᠨ', 'ᠲᠣᠰᠣ', 'ᠭᠠᠳᠠᠭᠱᠢᠯᠠᠭᠤᠯᠬᠤ', 'ᠪᠠᠨ', 'ᠵᠣᠭᠰᠣᠭᠠᠨ', '.', 'ᠦᠨ\\u180eᠡ', 'ᠥᠰᠬᠦ', 'ᠶ᠋ᠢ', 'ᠬᠦᠯᠢᠶᠡᠨ\\u180eᠡ', '.', '<end>']\n",
      "22 ['<start>', '石', '油', '输', '出', '国', '家', '组', '织', '冻', '结', '石', '油', '输', '出', '.', '价', '格', '看', '涨', '.', '<end>']\n",
      "ds_val\n",
      "25 ['<start>', 'ᠴᠠᠭᠠᠨ', 'ᠰᠠᠷ\\u180eᠠ', 'ᠶ᠋ᠢᠨ', 'ᠦᠶ\\u180eᠡ', 'ᠪᠡᠷ', 'ᠬᠥᠮᠦᠰ', 'ᠬᠠᠭᠤᠴᠢᠨ', 'ᠶᠣᠰᠣ', 'ᠵᠠᠩᠱᠢᠯ', 'ᠢ᠋ᠶ᠋ᠠᠨ', 'ᠪᠠᠷᠢᠮᠲᠠᠯᠠᠵᠤ', 'ᠪᠠᠢᠭ\\u180eᠠ', 'ᠪᠥᠭᠡᠳ', 'ᠤᠯᠠᠮ', 'ᠢ᠋ᠶ᠋ᠠᠷ', 'ᠱᠢᠨ\\u180eᠡ', 'ᠵᠦᠢᠯ', 'ᠢ᠋', 'ᠡᠷᠢᠨ', 'ᠬᠠᠢᠬᠤ', 'ᠪᠣᠯᠵᠤ', 'ᠪᠠᠢᠨ\\u180eᠠ', '.', '<end>']\n",
      "35 ['<start>', '人', '们', '在', '固', '守', '原', '有', '过', '年', '消', '费', '方', '式', '的', '同', '时', '.', '也', '在', '欣', '赏', '.', '渴', '望', '着', '浪', '漫', '的', '时', '尚', '情', '趣', '.', '<end>']\n",
      "ds_test\n",
      "13 ['<start>', 'ᠵᠠᠷᠯᠠᠨ', 'ᠨᠡᠢᠲᠡᠯᠡᠬᠦ', 'ᠬᠤᠭᠤᠴᠠᠭ\\u180eᠠ', 'ᠨᠢ', 'ᠬᠣᠷᠢᠨ', 'ᠡᠳᠦᠷ', 'ᠡᠴᠡ', 'ᠳᠤᠲᠠᠭᠤ', 'ᠪᠠᠢᠵᠤ', 'ᠪᠣᠯᠬᠤ', 'ᠦᠭᠡᠢ', '<end>']\n",
      "13 ['<start>', '公', '示', '时', '间', '不', '得', '少', '于', '二', '十', '日', '<end>']\n"
     ]
    }
   ],
   "source": [
    "# # 查看1个样本的信息\n",
    "print('ds_train')\n",
    "print(len(ds_train[0].src), ds_train[0].src)\n",
    "print(len(ds_train[0].targ), ds_train[0].targ)\n",
    "print('ds_val')\n",
    "print(len(ds_val[0].src), ds_val[0].src)\n",
    "print(len(ds_val[0].targ), ds_val[0].targ)\n",
    "print('ds_test')\n",
    "print(len(ds_test[0].src), ds_test[0].src)\n",
    "print(len(ds_test[0].targ), ds_test[0].targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型大小与词表大小正相关，控制词表大小\n",
      "0-20：<unk> <pad> . <end> <start> ᠶ᠋ᠢᠨ ᠤ᠋ᠨ ᠤ᠋ ᠦ᠋ᠨ ᠶ᠋ᠢ ᠢ᠋ ᠳ᠋ᠤ᠌ ᠦ᠌ ᠨᠢ ᠦᠭᠡᠢ ᠳ᠋ᠦ᠍ ᠠ᠋ᠴᠠ ᠤᠯᠤᠰ ᠬᠡᠷᠡᠭᠲᠡᠢ ᠭᠠᠵᠠᠷ\n",
      "\n",
      "input_vocab_size： 110560\n",
      "target_vocab_size： 6085\n"
     ]
    }
   ],
   "source": [
    "# 构建词典\n",
    "print('模型大小与词表大小正相关，控制词表大小')\n",
    "SRC_TEXT.build_vocab(ds_train,min_freq=1)  # 建立词表 并建立token和ID的映射关系\n",
    "# print(len(SRC_TEXT.vocab))\n",
    "# print(SRC_TEXT.vocab.itos[0])\n",
    "# print(SRC_TEXT.vocab.itos[1])\n",
    "# print(SRC_TEXT.vocab.itos[2])\n",
    "# print(SRC_TEXT.vocab.itos[3])\n",
    "# print(SRC_TEXT.vocab.stoi['<start>'])\n",
    "# print(SRC_TEXT.vocab.stoi['<end>'])\n",
    "\n",
    "# 模拟decode\n",
    "res = []\n",
    "for id in range(20):\n",
    "    res.append(SRC_TEXT.vocab.itos[id])\n",
    "print('0-20：'+' '.join(res)+'\\n')\n",
    "\n",
    "TARG_TEXT.build_vocab(ds_train,min_freq=1)\n",
    "\n",
    "# print(len(TARG_TEXT.vocab))\n",
    "# print(TARG_TEXT.vocab.itos[0])\n",
    "# print(TARG_TEXT.vocab.itos[1])\n",
    "# print(TARG_TEXT.vocab.itos[2])\n",
    "# print(TARG_TEXT.vocab.itos[3])\n",
    "# print(TARG_TEXT.vocab.stoi['<start>'])\n",
    "# print(TARG_TEXT.vocab.stoi['<end>'])\n",
    "\n",
    "input_vocab_size = len(SRC_TEXT.vocab)\n",
    "target_vocab_size = len(TARG_TEXT.vocab)\n",
    "\n",
    "print('input_vocab_size：', input_vocab_size)\n",
    "print('target_vocab_size：', target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = batch * ngpu\n",
    "\n",
    "# 构建数据管道迭代器\n",
    "train_iter, val_iter, test_iter= torchtext.data.Iterator.splits(\n",
    "    (ds_train, ds_val, ds_test),\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x.src),\n",
    "    batch_sizes=(BATCH_SIZE, BATCH_SIZE, BATCH_SIZE)\n",
    ")\n",
    "\n",
    "\n",
    "# # 查看数据管道信息，此时会触发postprocessing，如果有的话\n",
    "# for BATCH in train_iter:\n",
    "#     # 注意，这里text第0维不是batch，而是seq_len\n",
    "#     print(BATCH.src.shape, BATCH.targ.shape)  # [12,64], [12,64]\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 将数据管道组织成与torch.utils.data.DataLoader相似的inputs, targets的输出形式\n",
    "class DataLoader:\n",
    "    def __init__(self, data_iter):\n",
    "        self.data_iter = data_iter\n",
    "        self.length = len(data_iter)  # 一共有多少个batch？\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __iter__(self):\n",
    "        # 注意，在此处调整text的shape为batch first\n",
    "        for batch in self.data_iter:\n",
    "            yield (torch.transpose(batch.src, 0, 1), torch.transpose(batch.targ, 0, 1))\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_iter)\n",
    "val_dataloader = DataLoader(val_iter)\n",
    "test_dataloader = DataLoader(test_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_dataloader): 1997\n",
      "len(val_dataloader): 42\n",
      "len(test_dataloader): 41\n"
     ]
    }
   ],
   "source": [
    "# 查看数据管道\n",
    "print('len(train_dataloader):', len(train_dataloader))  # 句子总数/batch数\n",
    "print('len(val_dataloader):', len(val_dataloader))  # 句子总数/batch数\n",
    "print('len(test_dataloader):', len(test_dataloader))  # 句子总数/batch数\n",
    "\n",
    "# for batch_src, batch_targ in train_dataloader:\n",
    "#     print('batch_src.shape:',batch_src.shape,'\\n','batch_targ.shape:',batch_targ.shape)  # [256,12], [256,12]\n",
    "#     print(batch_src, batch_src.dtype)\n",
    "#     print(batch_targ, batch_targ.dtype)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dep_train = torch.randn(len(train_pairs),MAX_LENGTH+2,MAX_LENGTH+2)\n",
    "# dep_val = torch.randn(len(val_pairs),MAX_LENGTH+2,MAX_LENGTH+2)\n",
    "# dep_test = torch.randn(len(test_pairs),MAX_LENGTH+2,MAX_LENGTH+2)\n",
    "\n",
    "# for step, (inp, targ) in enumerate(train_dataloader, start=1):\n",
    "#     print(step)\n",
    "#     st = (step-1) * inp.shape[0]\n",
    "#     end = st + inp.shape[0]\n",
    "#     dependency_matrix = dep_train[st:end,:,:]\n",
    "#     print(inp.shape)\n",
    "#     print(targ.shape)\n",
    "#     print(dependency_matrix.shape)\n",
    "    \n",
    "# for step, (inp, targ) in enumerate(test_dataloader, start=1):\n",
    "#     print(step)\n",
    "#     st = (step-1) * inp.shape[0]\n",
    "#     end = st + inp.shape[0]\n",
    "#     dependency_matrix = dep_test[st:end,:,:]\n",
    "#     print(inp.shape)\n",
    "#     print(targ.shape)\n",
    "#     print(dependency_matrix.shape)\n",
    "\n",
    "# for step, (inp, targ) in enumerate(val_dataloader, start=1):\n",
    "#     print(step)\n",
    "#     st = (step-1) * inp.shape[0]\n",
    "#     end = st + inp.shape[0]\n",
    "#     dependency_matrix = dep_val[st:end,:,:]\n",
    "#     print(inp.shape)\n",
    "#     print(targ.shape)\n",
    "#     print(dependency_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.位置编码 positional encoding\n",
    "绝对位置编码\n",
    "\n",
    "由于model中不含有任何recurrence or convolution，所以句子中token的相对位置关系无法体现，\n",
    "所以就需要在embedding vector中加入position encoding vector（维度相同）。这样每个词的词向量\n",
    "在 $d_{model}$ 维的空间中，就可以基于meaning和position来计算相似度或相关性\n",
    "\n",
    "$$\\begin{array}{ll} & PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}}) \\\\ & PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})\\end{array}$$\n",
    "\n",
    "**特点：**\n",
    "- （1）后面位置是前面位置的线性组合，保证了即使位置不是相邻的，也可能有关系（[参考这里](#https://blog.csdn.net/zhulinniao/article/details/104462228/)）\n",
    "- （2）每个位置的编码又是独特的\n",
    "- （3）每两个位置的encoding互相做点积，位置越远，点积的值越小，自己和自己点积，值最大\n",
    "\n",
    "![jupyter-img1](./imgs/im1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 计算角度：pos * 1/(10000^(2i/d))\n",
    "def get_angles(pos, i, d_model):\n",
    "    # 2*(i//2)保证了2i，这部分计算的是1/10000^(2i/d)\n",
    "    angle_rates = 1 / np.power(10000, 2 * (i // 2) / np.float32(d_model))  # => [1, 512]\n",
    "    return pos * angle_rates  # [50,1]*[1,512]=>[50, 512]\n",
    "\n",
    "\n",
    "# np.arange()函数返回一个有终点和起点的固定步长的排列，如[1,2,3,4,5]，起点是1，终点是5，步长为1\n",
    "# 注意：起点终点是左开右闭区间，即start=1,end=6，才会产生[1,2,3,4,5]\n",
    "# 只有一个参数时，参数值为终点，起点取默认值0，步长取默认值1。\n",
    "def positional_encoding(position, d_model):  # d_model是位置编码的长度，相当于position encoding的embedding_dim？\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],  # [50, 1]\n",
    "                            np.arange(d_model)[np.newaxis, :],  # [1, d_model=512]\n",
    "                            d_model)\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])  # 2i\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])  # 2i+2\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]  # [50,512]=>[1,50,512]\n",
    "    return torch.tensor(pos_encoding, dtype=torch.float32)\n",
    "\n",
    "pos_encoding = positional_encoding(50, 512)\n",
    "# print(pos_encoding.shape) # [1,50,512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def draw_pos_encoding(pos_encoding):\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(pos_encoding[0], cmap='RdBu') # 绘制分类图\n",
    "    plt.xlabel('Depth')\n",
    "    plt.xlim((0, 512))\n",
    "    plt.ylabel('Position')\n",
    "    plt.colorbar() # 条形bar颜色图例\n",
    "    plt.savefig(result_save+'pos_encoding.png')\n",
    "    #plt.show()\n",
    "\n",
    "draw_pos_encoding(pos_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.掩码 masking\n",
    "这里用到的mask有2种：\n",
    "- padding mask：mask pad，即句子中为pad(value=0)的位置处其mask值为1\n",
    "- look-ahead mask：mask future token，将当前token后面的词mask掉，只让看到前面的词，即future token位置的mask值为1\n",
    "\n",
    "**【注意】：** 因为我这里使用的是torchtext里的tokenizer,从前面可以看出它的词表里pad的index=1，而不是常规的0。\n",
    "这里要特别注意，不然容易出错！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "mask 表示掩码，它对某些值进行掩盖，使其在参数更新时不产生效果。\n",
    "Transformer 模型里面涉及两种 mask，分别是 padding mask 和 sequence mask。\n",
    "其中，padding mask 在所有的 scaled dot-product attention 里面都需要用到，\n",
    "而 sequence mask 只有在 decoder 的 self-attention 里面用到。\n",
    "'''\n",
    "# 需要mask的位置，替换为1，seq：sentence_len x d_model \n",
    "#返回值为：sentence_len x 1 x 1 x d_model 为什么要扩展维度\n",
    "\n",
    "pad = 1 # 重要！\n",
    "def create_padding_mask(seq):  # seq [b, seq_len]\n",
    "    # seq = torch.eq(seq, torch.tensor(0)).float() # pad=0的情况\n",
    "    seq = torch.eq(seq, torch.tensor(pad)).float()  # pad!=0\n",
    "    return seq[:, np.newaxis, np.newaxis, :]  # =>[b, 1, 1, seq_len]\n",
    "\n",
    "# x = torch.tensor([[7, 6, 0, 0, 1],\n",
    "#                   [1, 2, 3, 0, 0],\n",
    "#                   [0, 0, 0, 4, 5]])\n",
    "# print(x.shape) # [3,5]\n",
    "# print(x)\n",
    "# mask = create_padding_mask(x)\n",
    "# print(mask.shape, mask.dtype) # [3,1,1,5]\n",
    "# print(mask)\n",
    "\n",
    "# 用train_dataloader的第一个BATCH_SIZE中的input来测试\n",
    "# for step, (inp, targ) in enumerate(train_dataloader, start=1):\n",
    "#     print(inp.shape,'\\t',targ.shape)\n",
    "#     print(inp,'\\n',targ)\n",
    "#     break\n",
    "\n",
    "# input_mask = create_padding_mask(inp)\n",
    "# targ_mask = create_padding_mask(targ)\n",
    "# print(input_mask,input_mask.shape)\n",
    "# print(targ_mask,targ_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = []\n",
    "# b = []\n",
    "# for step, (inp, targ) in enumerate(test_dataloader):\n",
    "#     a.append(inp.shape[1])\n",
    "#     b.append(targ.shape[1])\n",
    "# #     print(inp,'\\n',inp.shape,targ.shape)\n",
    "# #     break\n",
    "# print(a)\n",
    "# print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pad = 1 # 重要！\n",
    "# def mn_create_padding_mask(seq):  # seq [b, seq_len]\n",
    "#     if seq.shape[1] < 10:\n",
    "#         d = torch.ones([seq.shape[0],10-seq.shape[1]])\n",
    "#         seq = torch.cat((seq, d), dim=1)\n",
    "#     print(seq)\n",
    "#     # seq = torch.eq(seq, torch.tensor(0)).float() # pad=0的情况\n",
    "#     seq = torch.eq(seq, torch.tensor(pad)).float()  # pad!=0\n",
    "#     return seq[:, np.newaxis, np.newaxis, :]  # =>[b, 1, 1, seq_len]\n",
    "\n",
    "# x = torch.tensor([[7, 6, 0, 0, 1],\n",
    "#                   [1, 2, 3, 0, 0],\n",
    "#                   [0, 0, 0, 4, 5]])\n",
    "# print(x.shape) # [3,5]\n",
    "# print(x)\n",
    "# mask = mn_create_padding_mask(x)\n",
    "# print(mask.shape, mask.dtype) # [3,1,1,5]\n",
    "# print(mask)\n",
    "\n",
    "# # create two sample vectors\n",
    "# # inps = torch.randn([64, 161])\n",
    "# d = torch.ones([3,4])\n",
    "\n",
    "# # bring d into the same format, and then concatenate tensors\n",
    "# # new_inps = torch.cat((x, d.unsqueeze(2)), dim=-1)\n",
    "# new_inps = torch.cat((x,d),dim=1)\n",
    "# print(new_inps.shape)  # [64, 161, 2]\n",
    "# print(new_inps)\n",
    "# print(x.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# torch.triu(tensor, diagonal=0) 求上三角矩阵，diagonal默认为0表示主对角线的上三角矩阵\n",
    "# diagonal>0，则主对角上面的第|diagonal|条次对角线的上三角矩阵\n",
    "# diagonal<0，则主对角下面的第|diagonal|条次对角线的上三角矩阵\n",
    "#look-ahead_mask 用于对未预测的token进行掩码，这意味着要预测第三个单词，只会使用第一个和第二个单词。 \n",
    "\n",
    "def create_look_ahead_mask(size):  # seq_len\n",
    "    mask = torch.triu(torch.ones((size, size)), diagonal=1)\n",
    "    # mask = mask.device() #\n",
    "    return mask  # [seq_len, seq_len]\n",
    "\n",
    "# x = torch.rand(1,3)\n",
    "# print(x.shape)\n",
    "# print(x)\n",
    "# mask = create_look_ahead_mask(x.shape[1])\n",
    "# print(mask.shape, mask.dtype)\n",
    "# print(mask)\n",
    "\n",
    "# 用train_dataloader的第一个BATCH_SIZE中的target来测试\n",
    "# for step, (inp, targ) in enumerate(train_dataloader, start=1):\n",
    "#     print(targ,'\\n',inp.shape,targ.shape)\n",
    "#     break\n",
    "\n",
    "# look_ahead_mask = create_look_ahead_mask(targ.shape[-1])\n",
    "# print(look_ahead_mask,look_ahead_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4.scaled dot product attention\n",
    "![jupyter-img2](./imgs/im2.jpg)\n",
    "\n",
    "$$Attention(Q,K,V)=softmax_{(k)}(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "注意：实现时对mask的处理\n",
    "\n",
    "mask=1的位置是pad或者future token，乘以-1e9（-1*10^9）成为负无穷，经过softmax后会趋于0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    \"\"\"\n",
    "    #计算注意力权重。\n",
    "    q, k, v 必须具有匹配的前置维度。 且dq=dk\n",
    "    k, v 必须有匹配的倒数第二个维度，例如：seq_len_k = seq_len_v。\n",
    "    #虽然 mask 根据其类型（填充或前瞻）有不同的形状，\n",
    "    #但是 mask 必须能进行广播转换以便求和。\n",
    "\n",
    "    #参数:\n",
    "        q: 请求的形状 == (..., seq_len_q, depth)\n",
    "        k: 主键的形状 == (..., seq_len_k, depth)\n",
    "        v: 数值的形状 == (..., seq_len_v, depth_v)  seq_len_k = seq_len_v\n",
    "        mask: Float 张量，其形状能转换成\n",
    "              (..., seq_len_q, seq_len_k)。默认为None。\n",
    "    \n",
    "    # self-attention中q=k=v这点和attention不同，需要先明确\n",
    "    #q和k相似度计算是为了获取到最合适的值，也就是值的给与注意力的值\n",
    "    #softmax是为了获取这一系列相似度值的占比（这也就是所谓的权重值）\n",
    "    #加权是和v也就是本身进行加权，求和是为了获取粒度单词和完整句子之间的关系值计算\n",
    "    #那么自然而然的，所谓的qkv这一系列操作的目的就是\n",
    "    #先通过对本身的各个向量值进行相似度计算，然后通过softmax获取本身向量的权重值，在和本身进行加权计算，\n",
    "    #最后在求和，这样子就获取了一个词和本身所有的词的权重值，然后将所有词的权重值作为输入向量，这也就是所谓的自注意力机制。\n",
    "\n",
    "    #返回值:\n",
    "        #输出，注意力权重\n",
    "    \"\"\"\n",
    "    # matmul(a,b)矩阵乘:a b的最后2个维度要能做乘法，即a的最后一个维度值==b的倒数第2个纬度值，\n",
    "    # 除此之外，其他维度值必须相等或为1（为1时会广播）\n",
    "    matmul_qk = torch.matmul(q, k.transpose(-1, -2))  # 矩阵乘 =>[..., seq_len_q, seq_len_k]\n",
    "\n",
    "    # 缩放matmul_qk\n",
    "    dk = torch.tensor(k.shape[-1], dtype=torch.float32)  # k的深度dk，或叫做depth_k\n",
    "    scaled_attention_logits = matmul_qk / torch.sqrt(dk)  # [..., seq_len_q, seq_len_k]\n",
    "\n",
    "    # 将 mask 加入到缩放的张量上(重要！)\n",
    "    if mask is not None:  # mask: [b, 1, 1, seq_len]\n",
    "        # mask=1的位置是pad，乘以-1e9（-1*10^9）成为负无穷，经过softmax后会趋于0\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax 在最后一个轴（seq_len_k）上归一化\n",
    "    attention_weights = torch.nn.functional.softmax(scaled_attention_logits, dim=-1)  # [..., seq_len_q, seq_len_k]\n",
    "\n",
    "    output = torch.matmul(attention_weights, v)  # =>[..., seq_len_q, depth_v]\n",
    "    return output, attention_weights  # [..., seq_len_q, depth_v], [..., seq_len_q, seq_len_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mn_scaled_dot_product_attention(q, k, v, m, mask=None):\n",
    "    \"\"\"\n",
    "    #计算注意力权重。\n",
    "    q, k, v 必须具有匹配的前置维度。 且dq=dk\n",
    "    k, v 必须有匹配的倒数第二个维度，例如：seq_len_k = seq_len_v。\n",
    "    #虽然 mask 根据其类型（填充或前瞻）有不同的形状，\n",
    "    #但是 mask 必须能进行广播转换以便求和。\n",
    "\n",
    "    #参数:\n",
    "        q: 请求的形状 == (..., seq_len_q, depth)\n",
    "        k: 主键的形状 == (..., seq_len_k, depth)\n",
    "        v: 数值的形状 == (..., seq_len_v, depth_v)  seq_len_k = seq_len_v\n",
    "        m: 数值的形状 == (..., seq_len_k, seq_len_v) m必须是个方阵\n",
    "        mask: Float 张量，其形状能转换成\n",
    "              (..., seq_len_q, seq_len_k)。默认为None。\n",
    "\n",
    "    #返回值:\n",
    "        #输出，注意力权重\n",
    "    \"\"\"\n",
    "    # matmul(a,b)矩阵乘:a b的最后2个维度要能做乘法，即a的最后一个维度值==b的倒数第2个纬度值，\n",
    "    # 除此之外，其他维度值必须相等或为1（为1时会广播）\n",
    "    matmul_qk = torch.matmul(q, k.transpose(-1, -2))  # 矩阵乘 =>[..., seq_len_q, seq_len_k]\n",
    "\n",
    "    # 缩放matmul_qk\n",
    "    dk = torch.tensor(k.shape[-1], dtype=torch.float32)  # k的深度dk，或叫做depth_k\n",
    "    scaled_attention_logits = matmul_qk / torch.sqrt(dk) # [..., seq_len_q, seq_len_k]\n",
    "#     print('scaled_attention_logits.shape',scaled_attention_logits.shape)\n",
    "#     print('m.shape',m.shape)\n",
    "#     print('q*k^t：', scaled_attention_logits.shape)\n",
    "    \n",
    "#     scaled_attention_logits = torch.matmul(scaled_attention_logits,m)\n",
    "    scaled_attention_logits = scaled_attention_logits * m\n",
    "#     print('q*k^t*m：', scaled_attention_logits.shape)\n",
    "    \n",
    "    # 将 mask 加入到缩放的张量上(重要！)\n",
    "    if mask is not None:  # mask: [b, 1, 1, seq_len]\n",
    "        # mask=1的位置是pad，乘以-1e9（-1*10^9）成为负无穷，经过softmax后会趋于0\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax 在最后一个轴（seq_len_k）上归一化\n",
    "    attention_weights = torch.nn.functional.softmax(scaled_attention_logits, dim=-1)  # [..., seq_len_q, seq_len_k]\n",
    "    \n",
    "    output = torch.matmul(attention_weights, v)  # =>[..., seq_len_q, depth_v]\n",
    "#     print('output：', output.shape)\n",
    "    return output, attention_weights  # [..., seq_len_q, depth_v], [..., seq_len_q, seq_len_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dependency_parsing_scaled_dot_product_attention(q, k, v, dependency_parsing_matrix, mask=None):\n",
    "    \"\"\"\n",
    "    #计算注意力权重。\n",
    "    q, k, v 必须具有匹配的前置维度。 且dq=dk\n",
    "    k, v 必须有匹配的倒数第二个维度，例如：seq_len_k = seq_len_v。\n",
    "    #虽然 mask 根据其类型（填充或前瞻）有不同的形状，\n",
    "    #但是 mask 必须能进行广播转换以便求和。\n",
    "\n",
    "    #参数:\n",
    "        q: 请求的形状 == (..., seq_len_q, depth)\n",
    "        k: 主键的形状 == (..., seq_len_k, depth)\n",
    "        v: 数值的形状 == (..., seq_len_v, depth_v)  seq_len_k = seq_len_v\n",
    "        mask: Float 张量，其形状能转换成\n",
    "              (..., seq_len_q, seq_len_k)。默认为None。\n",
    "    \n",
    "    # self-attention中q=k=v这点和attention不同，需要先明确\n",
    "    #q和k相似度计算是为了获取到最合适的值，也就是值的给与注意力的值\n",
    "    #softmax是为了获取这一系列相似度值的占比（这也就是所谓的权重值）\n",
    "    #加权是和v也就是本身进行加权，求和是为了获取粒度单词和完整句子之间的关系值计算\n",
    "    #那么自然而然的，所谓的qkv这一系列操作的目的就是\n",
    "    #先通过对本身的各个向量值进行相似度计算，然后通过softmax获取本身向量的权重值，在和本身进行加权计算，\n",
    "    #最后在求和，这样子就获取了一个词和本身所有的词的权重值，然后将所有词的权重值作为输入向量，这也就是所谓的自注意力机制。\n",
    "\n",
    "    #返回值:\n",
    "        #输出，注意力权重\n",
    "    \"\"\"\n",
    "    # matmul(a,b)矩阵乘:a b的最后2个维度要能做乘法，即a的最后一个维度值==b的倒数第2个纬度值，\n",
    "    # 除此之外，其他维度值必须相等或为1（为1时会广播）\n",
    "    matmul_qk = torch.matmul(q, k.transpose(-1, -2))  # 矩阵乘 =>[..., seq_len_q, seq_len_k]\n",
    "#     print('matmul_qk:', matmul_qk.size())\n",
    "#     print('dependency_parsing_matrix:', dependency_parsing_matrix.size())\n",
    "    matmul_qk = matmul_qk + dependency_parsing_matrix\n",
    "\n",
    "    # 缩放matmul_qk\n",
    "    dk = torch.tensor(k.shape[-1], dtype=torch.float32)  # k的深度dk，或叫做depth_k\n",
    "    scaled_attention_logits = matmul_qk / torch.sqrt(dk)  # [..., seq_len_q, seq_len_k]\n",
    "#     print('scaled_attention_logits.shape:', scaled_attention_logits.shape)\n",
    "    # 将 mask 加入到缩放的张量上(重要！)\n",
    "    if mask is not None:  # mask: [b, 1, 1, seq_len]\n",
    "#         print('mask.shape', mask.shape)\n",
    "        mask = mask.reshape(-1,mask.shape[-1],mask.shape[-1])\n",
    "        # mask=1的位置是pad，乘以-1e9（-1*10^9）成为负无穷，经过softmax后会趋于0\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "    \n",
    "    # softmax 在最后一个轴（seq_len_k）上归一化\n",
    "    attention_weights = torch.nn.functional.softmax(scaled_attention_logits, dim=-1)  # [..., seq_len_q, seq_len_k]\n",
    "\n",
    "    output = torch.matmul(attention_weights, v)  # =>[..., seq_len_q, depth_v]\n",
    "#     print('output.shape',output.shape)\n",
    "    return output, attention_weights  # [..., seq_len_q, depth_v], [..., seq_len_q, seq_len_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 原来的\n",
    "# class dependency_parsing_Attention(torch.nn.Module):\n",
    "#     def __init__(self, d_model):\n",
    "#         super(dependency_parsing_Attention, self).__init__()\n",
    "\n",
    "#         self.d_model = d_model\n",
    "\n",
    "#         self.wq = torch.nn.Linear(d_model, d_model)\n",
    "#         self.wk = torch.nn.Linear(d_model, d_model)\n",
    "#         self.wv = torch.nn.Linear(d_model, d_model)\n",
    "#         #primer_ez\n",
    "#         self.final_linear = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "#     def forward(self, q, k, v,dependency_parsing_matrix,mask):  # q=k=v=x [b, seq_len, embedding_dim] embedding_dim其实也=d_model\n",
    "#         batch_size = q.shape[0]\n",
    "\n",
    "#         q = self.wq(q)  # =>[b, seq_len, d_model]\n",
    "#         k = self.wk(k)  # =>[b, seq_len, d_model]\n",
    "#         v = self.wv(v)  # =>[b, seq_len, d_model]\n",
    "# #         print('q:',q.shape)\n",
    "# #         print('k:',q.shape)\n",
    "# #         print('v:',q.shape)\n",
    "\n",
    "#         scaled_attention, attention_weights = dependency_parsing_scaled_dot_product_attention(q, k, v, dependency_parsing_matrix, mask)\n",
    "#         # => [b, seq_len_q, d_model=512], [b, seq_len_q, seq_len_k]\n",
    "\n",
    "#         output = self.final_linear(scaled_attention)  # =>[b, seq_len_q, d_model=512]\n",
    "#         return output, attention_weights  # [b, seq_len_q, d_model=512], [b, seq_len_q, seq_len_k]\n",
    "\n",
    "\n",
    "# # x = torch.rand(2, 5, 6) # [b,seq_len,d_model,]\n",
    "# # dependency_parsing_matrix = torch.rand(2,5,5)\n",
    "\n",
    "# # temp_mha = dependency_parsing_Attention(d_model=6)\n",
    "\n",
    "# # out, attn_weights = temp_mha(x, x, x, dependency_parsing_matrix, mask=None)\n",
    "# # print(out.shape, attn_weights.shape) # [1, 60, 512], [1, 8, 60, 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加入premer-ez的\n",
    "class dependency_parsing_Attention(torch.nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(dependency_parsing_Attention, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.wq = torch.nn.Linear(d_model, d_model)\n",
    "        self.wk = torch.nn.Linear(d_model, d_model)\n",
    "        self.wv = torch.nn.Linear(d_model, d_model)\n",
    "        self.SDWC = SpatialDepthWiseConvolution(d_k=d_model,kernel_size=3)\n",
    "        #primer_ez\n",
    "        self.final_linear = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v,dependency_parsing_matrix,mask):  # q=k=v=x [b, seq_len, embedding_dim] embedding_dim其实也=d_model\n",
    "        batch_size = q.shape[0]\n",
    "        \n",
    "        q = self.wq(q)  # =>[b, seq_len, d_model]\n",
    "#         print('q',q.shape)\n",
    "        q = self.SDWC(q)\n",
    "        k = self.wk(k)  # =>[b, seq_len, d_model]\n",
    "        k = self.SDWC(k)\n",
    "        v = self.wv(v)  # =>[b, seq_len, d_model]\n",
    "        v = self.SDWC(v)\n",
    "#         print('q:',q.shape)\n",
    "#         print('k:',q.shape)\n",
    "#         print('v:',q.shape)\n",
    "\n",
    "\n",
    "        scaled_attention, attention_weights = dependency_parsing_scaled_dot_product_attention(q, k, v, dependency_parsing_matrix, mask)\n",
    "        # => [b, seq_len_q, d_model=512], [b, seq_len_q, seq_len_k]\n",
    "\n",
    "        output = self.final_linear(scaled_attention)  # =>[b, seq_len_q, d_model=512]\n",
    "        return output, attention_weights  # [b, seq_len_q, d_model=512], [b, seq_len_q, seq_len_k]\n",
    "\n",
    "# b = 120\n",
    "# s = 100\n",
    "# d = 256\n",
    "# x = torch.rand(b, s, d) # [b,seq_len,d_model]\n",
    "# dependency_parsing_matrix = torch.rand(b,s,s)\n",
    "# temp_mha = dependency_parsing_Attention(d_model=d)\n",
    "# out, attn_weights = temp_mha(x, x, x, dependency_parsing_matrix, mask=None)\n",
    "# print(out.shape, attn_weights.shape) # [1, 60, 512], [1, 8, 60, 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def print_out(q, k, v, m):\n",
    "#     temp_out, temp_attn = mn_scaled_dot_product_attention(q, k, v, m, None)\n",
    "#     print('Attention weights are:')\n",
    "#     print(temp_attn)\n",
    "#     print('Output is:')\n",
    "#     print(temp_out)\n",
    "\n",
    "# np.set_printoptions(suppress=True) # 设置不以科学计数法的形式显示数据\n",
    "\n",
    "# temp_k = torch.tensor([[10,0,0],\n",
    "#                        [0,10,0],\n",
    "#                        [0,0,10],\n",
    "#                        [0,0,10]], dtype=torch.float32) # [4,3]\n",
    "# temp_v = torch.tensor([[1,0],\n",
    "#                        [10,0],\n",
    "#                        [100,5],\n",
    "#                        [1000,6]], dtype=torch.float32) #[4,2]\n",
    "\n",
    "# temp_m = torch.tensor([[10,0,0,0],\n",
    "#                        [0,10,0,0],\n",
    "#                        [0,0,10,0],\n",
    "#                        [0,0,10,0]], dtype=torch.float32) # [4,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # query aligns with第2个key (key的第2列)，得到attention weights：[0. 1. 0. 0.]\n",
    "# # 所以第2行 的value值被返回\n",
    "# temp_q = torch.tensor([[0,10,0]], dtype=torch.float32) # [1,3]\n",
    "# print_out(temp_q, temp_k, temp_v,temp_m)\n",
    "# # Attention weights：[0. 1. 0. 0.]， Output：[[10.  0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # query aligns with 重复的key (key的第3列和第4列)，得到attention weights：[0. 0. 0.5 0.5]\n",
    "# # 所以第3行的value值与第4行的value值平均化后，被返回 【(100+1000)/2=550, (5+6)/2=5.5】\n",
    "# temp_q = torch.tensor([[0,0,10]], dtype=torch.float32) # [1,3]\n",
    "# print_out(temp_q, temp_k, temp_v,temp_m)\n",
    "# # Attention weights：[0. 0. 0.5 0.5]， Output：[[550.  5.5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # query aligns with 第1个key (key的第1列)和第2个key (key的第2列)，得到attention weights：[0.5 0.5 0. 0.]\n",
    "# # 所以第1行的value值与第2行的value值平均化后，被返回 【(1+10)/2=5.5, (0.+0.)/2=0.】\n",
    "# temp_q = torch.tensor([[10,10,0]], dtype=torch.float32) # [1,3]\n",
    "# print_out(temp_q, temp_k, temp_v,temp_m)\n",
    "# # Attention weights：[0.5 0.5 0. 0.]， Output：[[5.5.  0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # 传入所有的query\n",
    "# temp_q = torch.tensor([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=torch.float32)  # (3, 3)\n",
    "# print_out(temp_q, temp_k, temp_v,temp_m)\n",
    "# \"\"\"\n",
    "# # Attention weights：tensor(\n",
    "# [[0.  0.  0.5 0.5]\n",
    "#  [0.  1.  0.  0. ]\n",
    "#  [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)，\n",
    "# # Output：tensor(\n",
    "# [[550.    5.5]\n",
    "#  [ 10.    0. ]\n",
    "#  [  5.5   0. ]], shape=(3, 2), dtype=float32)\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # relative_positions\n",
    "# def relative_positions(seq_len):\n",
    "#     result = []\n",
    "#     for i in range(seq_len):\n",
    "#         front = list(range(-i, 0))\n",
    "#         end = list(range(seq_len - i))\n",
    "#         result.append(front + end)\n",
    "#     return result\n",
    "# relative_positions(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import math\n",
    "# # import torch\n",
    "# # from torch import nn\n",
    "\n",
    "# class RelativeGlobalAttention(torch.nn.Module):\n",
    "#     def __init__(self, d_model, num_heads, max_len=100, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.d_model = d_model\n",
    "#         self.num_heads = num_heads\n",
    "        \n",
    "#         assert d_model % self.num_heads == 0\n",
    "#         self.d_head = d_model // self.num_heads\n",
    "# #         d_head, remainder = divmod(d_model, num_heads)\n",
    "# #         if remainder:\n",
    "# #             raise ValueError(\n",
    "# #                 \"incompatible `d_model` and `num_heads`\"\n",
    "# #             )\n",
    "\n",
    "#         self.max_len = max_len\n",
    "\n",
    "#         self.key = torch.nn.Linear(d_model, d_model)\n",
    "#         self.value = torch.nn.Linear(d_model, d_model)\n",
    "#         self.query = torch.nn.Linear(d_model, d_model)\n",
    "#         self.dropout = torch.nn.Dropout(dropout)\n",
    "#         self.Er = torch.nn.Parameter(torch.randn(max_len, self.d_head))\n",
    "        \n",
    "#         self.register_buffer(\n",
    "#             \"mask\", \n",
    "#             torch.tril(torch.randn(max_len, max_len))\n",
    "#             .unsqueeze(0).unsqueeze(0)\n",
    "#         )\n",
    "# #         print('self.mask.shape：', self.mask.shape)\n",
    "#         # self.mask.shape = (1, 1, max_len, max_len)\n",
    "\n",
    "#     def skew(self, QEr):\n",
    "#         # QEr.shape = (batch_size, num_heads, seq_len, seq_len)\n",
    "# #         print('QEr.shape:',QEr.shape)\n",
    "#         padded = torch.nn.functional.pad(QEr, (1, 0), 'constant', 1)\n",
    "# #         print('padded.shape:',padded.shape)\n",
    "# #         print(padded)\n",
    "#         # padded.shape = (batch_size, num_heads, seq_len, 1 + seq_len)\n",
    "#         batch_size, num_heads, num_rows, num_cols = padded.shape\n",
    "#         reshaped = padded.reshape(batch_size, num_heads, num_cols, num_rows)\n",
    "#         # reshaped.size = (batch_size, num_heads, 1 + seq_len, seq_len)\n",
    "#         Srel = reshaped[:, :, 1:, :]\n",
    "#         # Srel.shape = (batch_size, num_heads, seq_len, seq_len)\n",
    "#         return Srel\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         # x.shape == (batch_size, seq_len, d_model)\n",
    "#         batch_size, seq_len, _ = x.shape\n",
    "#         if seq_len > self.max_len:\n",
    "#             raise ValueError(\n",
    "#                 \"sequence length exceeds model capacity\"\n",
    "#             )\n",
    "        \n",
    "#         k_t = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
    "#         # k_t.shape = (batch_size, num_heads, d_head, seq_len)\n",
    "#         v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
    "#         q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
    "#         # shape = (batch_size, num_heads, seq_len, d_head)\n",
    "        \n",
    "#         start = self.max_len - seq_len\n",
    "#         Er_t = self.Er[start:, :].transpose(0, 1)\n",
    "#         print(Er_t)\n",
    "#         # Er_t.shape = (d_head, seq_len)\n",
    "#         QEr = torch.matmul(q, Er_t)\n",
    "#         # QEr.shape = (batch_size, num_heads, seq_len, seq_len)\n",
    "#         Srel = self.skew(QEr)\n",
    "#         # Srel.shape = (batch_size, num_heads, seq_len, seq_len)\n",
    "        \n",
    "#         QK_t = torch.matmul(q, k_t)\n",
    "#         # QK_t.shape = (batch_size, num_heads, seq_len, seq_len)\n",
    "#         # （Q*K^T+Srel）\n",
    "#         print('QK_t.shape：',QK_t.shape)\n",
    "#         print('Srel.shape：',Srel.shape)\n",
    "#         attn = (QK_t + Srel) / math.sqrt(q.size(-1))\n",
    "#         mask = self.mask[:, :, :seq_len, :seq_len]\n",
    "#         # mask.shape = (1, 1, seq_len, seq_len)\n",
    "#         attn = attn.masked_fill(mask == 1, float(\"-inf\"))\n",
    "#         # attn.shape = (batch_size, num_heads, seq_len, seq_len)\n",
    "#         attn = torch.nn.functional.softmax(attn, dim=-1)\n",
    "#         out = torch.matmul(attn, v)\n",
    "#         # out.shape = (batch_size, num_heads, seq_len, d_head)\n",
    "#         out = out.transpose(1, 2)\n",
    "#         # out.shape == (batch_size, seq_len, num_heads, d_head)\n",
    "#         out = out.reshape(batch_size, seq_len, -1)\n",
    "#         # out.shape == (batch_size, seq_len, d_model)\n",
    "#         return self.dropout(out)\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# test_in = torch.zeros(2, 5, 4) # batch_size, seq_len, d_model\n",
    "# l = RelativeGlobalAttention(d_model=4, num_heads=2,max_len=5)\n",
    "\n",
    "# l(test_in).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = 1\n",
    "# channal = 2\n",
    "# width = 5\n",
    "# height = 5\n",
    "# data = torch.randn(batch, channal, width, height) #　batch, channal, width, height\n",
    "# # 当pad只有两个参数时，如pad=(1,2)代表对最后一个维度改变——左边扩充一列，右边扩充2列\n",
    "# # 当pad有四个参数，代表对最后两个维度扩充，pad = (左边填充数， 右边填充数， 上边填充数， 下边填充数)\n",
    "# # 当pad有六个参数时，代表队最后三个维度扩充，\n",
    "# # pad = (左边填充数， 右边填充数， 上边填充数， 下边填充数， 前边填充数，后边填充数)\n",
    "# pad = (1,0)\n",
    "# data = torch.nn.functional.pad(data, pad, 'constant', 0)\n",
    "# print(data)\n",
    "# print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import torch\n",
    "# # import torch.nn as nn\n",
    "# # import math\n",
    "\n",
    "# # def d(tensor=None):\n",
    "# #     if tensor is None:\n",
    "# #         return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# #     return 'cuda' if tensor.is_cuda else 'cpu'\n",
    "\n",
    "# # class RelativeAttentionError(Exception):\n",
    "# #     pass\n",
    "\n",
    "# class RelativeMultiheadedAttention(torch.nn.Module):\n",
    "#     \"\"\"\n",
    "#     Narrow multiheaded attention. Each attention head inspects a \n",
    "#     fraction of the embedding space and expresses attention vectors for each sequence position as a weighted average of all (earlier) positions.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, d_model, heads=8, dropout=0.1, max_length=102, relative_pos=False):\n",
    "\n",
    "#         super().__init__()\n",
    "        \n",
    "# #         if d_model % heads != 0:\n",
    "# #             raise RelativeAttentionError(\"Number of heads does not divide model dimension\")\n",
    "            \n",
    "#         assert d_model % heads == 0\n",
    "#         self.d_model = d_model\n",
    "#         self.heads = heads\n",
    "#         s = d_model // heads\n",
    "        \n",
    "#         self.linears = torch.nn.ModuleList([torch.nn.Linear(s, s, bias=False) for i in range(3)])\n",
    "#         self.recombine_heads = torch.nn.Linear(heads * s, d_model)\n",
    "#         self.dropout = torch.nn.Dropout(p=dropout)\n",
    "#         self.max_length = max_length\n",
    "#         #relative positional embeddings\n",
    "#         self.relative_pos = relative_pos\n",
    "#         if relative_pos:\n",
    "#             self.Er = torch.randn([heads, self.max_length, s])\n",
    "#         else:\n",
    "#             self.Er = None\n",
    "\n",
    "#     def forward(self, x, mask):\n",
    "#         #batch size, sequence length, embedding dimension\n",
    "#         b, t, e = x.size()\n",
    "#         h = self.heads\n",
    "#         #each head inspects a fraction of the embedded space\n",
    "#         #head dimension\n",
    "#         s = e // h\n",
    "#         #start index of position embedding\n",
    "#         embedding_start = self.max_length - t\n",
    "#         x = x.view(b,t,h,s)\n",
    "#         queries, keys, values = [w(x).transpose(1,2)\n",
    "#                 for w, x in zip(self.linears, (x,x,x))]\n",
    "#         if self.relative_pos:\n",
    "#             #apply same position embeddings across the batch\n",
    "#             #Is it possible to apply positional self-attention over\n",
    "#             #only half of all relative distances?\n",
    "#             Er  = self.Er[:, embedding_start:, :].unsqueeze(0)\n",
    "#             print('Er:',Er.shape)\n",
    "#             QEr = torch.matmul(queries, Er.transpose(-1,-2))\n",
    "#             QEr = self.mask_positions(QEr)\n",
    "#             #Get relative position attention scores\n",
    "#             #combine batch with head dimension\n",
    "#             SRel = self.skew(QEr).contiguous().view(b*h, t, t)\n",
    "#         else:\n",
    "#             SRel = torch.zeros([b*h, t, t])\n",
    "#         print('SRel:',SRel.shape)\n",
    "#         queries, keys, values = map(lambda x: x.contiguous()\\\n",
    "#                 .view(b*h, t, s), (queries, keys, values))\n",
    "#         print('queries:',queries.shape)\n",
    "#         print('keys:',keys.shape)\n",
    "#         print('values:',values.shape)\n",
    "#         #Compute scaled dot-product self-attention\n",
    "#         #scale pre-matrix multiplication   \n",
    "#         queries = queries / (e ** (1/4))\n",
    "#         keys    = keys / (e ** (1/4))\n",
    "#         print('queries:',queries.shape)\n",
    "#         print('keys:',keys.shape)\n",
    "#         print('keys.transpose(1, 2):',keys.transpose(1, 2).shape)\n",
    "\n",
    "#         scores = torch.bmm(queries, keys.transpose(1, 2))\n",
    "#         print('scores:',scores.shape)\n",
    "#         scores = scores + SRel\n",
    "#         #(b*h, t, t)\n",
    "\n",
    "#         subsequent_mask = torch.triu(torch.ones(1, t, t),\n",
    "#                 1)\n",
    "#         scores = scores.masked_fill(subsequent_mask == 1, -1e9)\n",
    "#         if mask is not None:\n",
    "#             mask = mask.repeat_interleave(h, 0)\n",
    "#             wtf = (mask == 0).nonzero().transpose(0,1)\n",
    "#             scores[wtf[0], wtf[1], :] = -1e9\n",
    "\n",
    "        \n",
    "#         #Convert scores to probabilities\n",
    "#         attn_probs = torch.nn.functional.softmax(scores, dim=2)\n",
    "#         attn_probs = self.dropout(attn_probs)\n",
    "#         #use attention to get a weighted average of values\n",
    "#         out = torch.bmm(attn_probs, values).view(b, h, t, s)\n",
    "#         #transpose and recombine attention heads\n",
    "#         out = out.transpose(1, 2).contiguous().view(b, t, s * h)\n",
    "#         #last linear layer of weights\n",
    "#         return self.recombine_heads(out)\n",
    "\n",
    "\n",
    "#     def mask_positions(self, qe):\n",
    "#         #QEr is a matrix of queries (absolute position) dot distance embeddings (relative pos).\n",
    "#         #Mask out invalid relative positions: e.g. if sequence length is L, the query at\n",
    "#         #L-1 can only attend to distance r = 0 (no looking backward).\n",
    "#         L = qe.shape[-1]\n",
    "#         mask = torch.triu(torch.ones(L, L), 1).flip(1)\n",
    "#         return qe.masked_fill((mask == 1), 0)\n",
    "\n",
    "#     def skew(self, qe):\n",
    "#         #pad a column of zeros on the left\n",
    "#         padded_qe = torch.nn.functional.pad(qe, [1,0])\n",
    "#         s = padded_qe.shape\n",
    "#         padded_qe = padded_qe.view(s[0], s[1], s[3], s[2])\n",
    "#         #take out first (padded) row\n",
    "#         return padded_qe[:,:,1:,:]\n",
    "\n",
    "# test_in = torch.zeros(2, 5, 4) # batch_size, seq_len, d_model\n",
    "# # print(test_in)\n",
    "# test = RelativeMultiheadedAttention(d_model=4, heads=2, dropout=0.1, max_length=102, relative_pos=True)\n",
    "# result = test(test_in, mask = None)\n",
    "# print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.multi-head attention\n",
    "![jupyter-img3](./imgs/im3.jpg)\n",
    "\n",
    "多头注意允许模型在不同的位置联合处理来自不同表示子空间的信息。\n",
    "（Multi-head attention allows the model to jointly attend to information\n",
    "from different representation subspaces at different positions.）\n",
    "\n",
    "$$\\begin{array}{ll}  & MultiHead(Q,K,V)=Concat(head_1,...,head_h)W^O \\\\ & where\\; head_i=Attention(QW_i^Q,KW_i^K,VW_i^V)\\end{array}$$\n",
    "\n",
    "其中投影维参数矩阵$W_i^Q\\in R^{d_{model}\\text{x}\\,d_k}$，$W_i^K\\in R^{d_{model}\\text{x}\\,d_k}$， $W_i^V\\in R^{d_{model}\\text{x}\\,d_v}$，$W^O\\in R^{hd_v\\text{x}\\,d_{model}}$。\n",
    "\n",
    "$h=8,\\;d_k = d_v = d_{model}/h = 64$\n",
    "\n",
    "multi-head attention有4部分组成：\n",
    "- linear layer 将 linear layer的结果 split到不同的head\n",
    "- scaled dot-product attention\n",
    "- 将所有的head 进行拼接 concat\n",
    "- final linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0  # 因为输入要被（平均？）split到不同的head\n",
    "\n",
    "        self.depth = d_model // self.num_heads  # 512/8=64，所以在scaled dot-product atten中dq=dk=64,dv也是64\n",
    "\n",
    "        self.wq = torch.nn.Linear(d_model, d_model)\n",
    "        self.wk = torch.nn.Linear(d_model, d_model)\n",
    "        self.wv = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.final_linear = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):  # x [b, seq_len, d_model]\n",
    "        x = x.view(batch_size, -1, self.num_heads,\n",
    "                   self.depth)  # [b, seq_len, d_model=512]=>[b, seq_len, num_head=8, depth=64]\n",
    "        return x.transpose(1, 2)  # [b, seq_len, num_head=8, depth=64]=>[b, num_head=8, seq_len, depth=64]\n",
    "\n",
    "    def forward(self, q, k, v, mask):  # q=k=v=x [b, seq_len, embedding_dim] embedding_dim其实也=d_model\n",
    "        batch_size = q.shape[0]\n",
    "\n",
    "        q = self.wq(q)  # =>[b, seq_len, d_model]\n",
    "        k = self.wk(k)  # =>[b, seq_len, d_model]\n",
    "        v = self.wv(v)  # =>[b, seq_len, d_model]\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # =>[b, num_head=8, seq_len, depth=64]\n",
    "        k = self.split_heads(k, batch_size)  # =>[b, num_head=8, seq_len, depth=64]\n",
    "        v = self.split_heads(v, batch_size)  # =>[b, num_head=8, seq_len, depth=64]\n",
    "\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        # => [b, num_head=8, seq_len_q, depth=64], [b, num_head=8, seq_len_q, seq_len_k]\n",
    "\n",
    "        scaled_attention = scaled_attention.transpose(1, 2)  # =>[b, seq_len_q, num_head=8, depth=64]\n",
    "        # 转置操作让张量存储结构扭曲，直接使用view方法会失败，可以使用reshape方法\n",
    "        concat_attention = scaled_attention.reshape(batch_size, -1, self.d_model)  # =>[b, seq_len_q, d_model=512]\n",
    "\n",
    "        output = self.final_linear(concat_attention)  # =>[b, seq_len_q, d_model=512]\n",
    "        return output, attention_weights  # [b, seq_len_q, d_model=512], [b, num_head=8, seq_len_q, seq_len_k]\n",
    "\n",
    "\n",
    "# temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "# x = torch.rand(1, 64, 512) # [b,seq_len,d_model,embedding_dim]\n",
    "# print(x.shape)\n",
    "# out, attn_weights = temp_mha(x, x, x, mask=None)\n",
    "# print(out.shape, attn_weights.shape) # [1, 60, 512], [1, 8, 60, 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class mn_MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads, seq_len):\n",
    "        super(mn_MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        assert d_model % self.num_heads == 0  # 因为输入要被（平均？）split到不同的head\n",
    "        \n",
    "        # 512/8=64，所以在mn_scaled dot-product atten中dq=dk=64,dv也是64\n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "\n",
    "        self.wq = torch.nn.Linear(d_model, d_model)\n",
    "        self.wk = torch.nn.Linear(d_model, d_model)\n",
    "        self.wv = torch.nn.Linear(d_model, d_model)\n",
    "        self.wm = torch.nn.Linear(d_model, seq_len*num_heads)\n",
    "\n",
    "        self.final_linear = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):  # x [b, seq_len, d_model]\n",
    "        x = x.view(batch_size, -1, self.num_heads,\n",
    "                   self.depth)  # [b, seq_len, d_model=512]=>[b, seq_len, num_head=8, depth=64]\n",
    "        return x.transpose(1, 2)  # [b, seq_len, num_head=8, depth=64]=>[b, num_head=8, seq_len, depth=64]\n",
    "    \n",
    "    def split_heads_m(self, x, batch_size):  # x [b, seq_len, d_model]\n",
    "        x = x.view(batch_size, -1, self.num_heads,\n",
    "                   self.seq_len)  # [b, seq_len, d_model=512]=>[b, seq_len, num_head=8, depth=64]\n",
    "        return x.transpose(1, 2)  # [b, seq_len, num_head=8, depth=64]=>[b, num_head=8, seq_len, depth=seq_len]\n",
    "    \n",
    "    def forward(self, q, k, v, m, mask):  # q=k=v=x [b, seq_len, embedding_dim] embedding_dim其实也=d_model\n",
    "        batch_size = q.shape[0]\n",
    "        \n",
    "        q = self.wq(q)  # =>[b, seq_len, d_model]\n",
    "#         print('q：', q.shape)\n",
    "        k = self.wk(k)  # =>[b, seq_len, d_model]\n",
    "#         print('k：', k.shape)\n",
    "        v = self.wv(v)  # =>[b, seq_len, d_model]\n",
    "#         print('v：', v.shape)\n",
    "        m = self.wm(m)  # =>[b, seq_len, seq_len]\n",
    "#         print('m：', m.shape)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # =>[b, num_head=8, seq_len, depth=64]\n",
    "#         print('q：', q.shape)\n",
    "        k = self.split_heads(k, batch_size)  # =>[b, num_head=8, seq_len, depth=64]\n",
    "#         print('k：', k.shape)\n",
    "        v = self.split_heads(v, batch_size)  # =>[b, num_head=8, seq_len, depth=64]\n",
    "#         print('v：', v.shape)\n",
    "        m = self.split_heads_m(m, batch_size)  # =>[b, num_head=8, seq_len, depth=64]\n",
    "#         print('m：', m.shape)\n",
    "        scaled_attention, attention_weights = mn_scaled_dot_product_attention(q, k, v, m, mask)\n",
    "        # => [b, num_head=8, seq_len_q, depth=64], [b, num_head=8, seq_len_q, seq_len_k]\n",
    "\n",
    "        scaled_attention = scaled_attention.transpose(1, 2)  # =>[b, seq_len_q, num_head=8, depth=64]\n",
    "        # 转置操作让张量存储结构扭曲，直接使用view方法会失败，可以使用reshape方法\n",
    "        concat_attention = scaled_attention.reshape(batch_size, -1, self.d_model)  # =>[b, seq_len_q, d_model=512]\n",
    "\n",
    "        output = self.final_linear(concat_attention)  # =>[b, seq_len_q, d_model=512]\n",
    "        return output, attention_weights  # [b, seq_len_q, d_model=512], [b, num_head=8, seq_len_q, seq_len_k]\n",
    "\n",
    "\n",
    "# temp_mha = mn_MultiHeadAttention(d_model=256, num_heads=4, seq_len=MAX_LENGTH)\n",
    "# x = torch.rand(128, MAX_LENGTH, 256) # [b,seq_len,d_model,embedding_dim]\n",
    "# print('输入：', x.shape)\n",
    "# out, attn_weights = temp_mha(x, x, x, x, mask=None)\n",
    "# print(out.shape, attn_weights.shape) # [1, 60, 512], [1, 8, 60, 60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 6.point wise feed forward network\n",
    "2层线性变换和一个ReLU激活：\n",
    "$$FFN(x) = max(0, xW1 + b1)W2 + b2$$\n",
    "\n",
    "这一层的input和output的维度都是$d_{model}$，而内层的维度的是$d_{ff}=2048$\n",
    "\n",
    "其实就是(nn.relu(x w1+b1))w2+b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 点式前馈网络\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    feed_forward_net = torch.nn.Sequential(\n",
    "        torch.nn.Linear(d_model, dff),  # [b, seq_len, d_model]=>[b, seq_len, dff=2048]\n",
    "        #torch.nn.ReLU(),\n",
    "        \n",
    "        # 更换激活函数为SquaredReLU()\n",
    "#         torch.nn.LeakyReLU(),\n",
    "        SquaredReLU(), #primer_ez\n",
    "        torch.nn.Linear(dff, d_model),  # [b, seq_len, dff=2048]=>[b, seq_len, d_model=512]\n",
    "    )\n",
    "    return feed_forward_net\n",
    "\n",
    "# sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
    "# input = torch.rand(64, 50, 512) # [b, seq_len, d_model]\n",
    "# print(sample_ffn(input).shape) # [b=64, seq_len=50, d_model=512]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "transformer model:\n",
    "\n",
    "(1) input sentence 传入N个encoder layer，句子的每个token都会有一个输出\n",
    "\n",
    "(2) decoder attends on encoder 的输出（encoder-decoder attention） 和 它自己的输入（self-attention），然后预测出下一个词\n",
    "\n",
    "## 7.encoder layer\n",
    "每个编码器层包括以下2个子层：\n",
    "- 多头注意力（有padding mask）\n",
    "- 点式前馈网络（Point wise feed forward networks）\n",
    "\n",
    "注意：每个子层还伴随着一个残差连接，然后进行“层归一化”（LayerNorm）。残差连接有助于避免深度网络中的梯度消失问题。\n",
    "\n",
    "每个子层的输出是 LayerNorm(x + Sublayer(x))。归一化是在 d_model（最后一个）维度完成的。\n",
    "\n",
    "注意：实现时在每个sub layer 之后加入了dropout层，再才进行add the sub layer input 和 normalized，即\n",
    "LayerNorm(x + Dropout(Sublayer(x)))\n",
    "\n",
    "Transformer 中有 N 个编码器层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)  # 多头注意力（padding mask）(self-attention)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "        self.layernorm2 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "\n",
    "        self.dropout1 = torch.nn.Dropout(rate)\n",
    "        self.dropout2 = torch.nn.Dropout(rate)\n",
    "\n",
    "    # x [b, inp_seq_len, embedding_dim] embedding_dim其实也=d_model\n",
    "    # mask [b,1,1,inp_seq_len]\n",
    "    def forward(self, x, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # =>[b, seq_len, d_model]\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)  # 残差&层归一化 =>[b, seq_len, d_model]\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # =>[b, seq_len, d_model]\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # 残差&层归一化 =>[b, seq_len, d_model]\n",
    "\n",
    "        return out2  # [b, seq_len, d_model]\n",
    "\n",
    "# layernorm = torch.nn.LayerNorm(normalized_shape=512, eps=1e-6)\n",
    "# x = torch.rand(4, 64, 512)\n",
    "# print(layernorm(x).shape)\n",
    "\n",
    "# sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "# x = torch.rand(64, 64, 512) # [b, seq_len, d_model]\n",
    "# sample_encoder_layer_output = sample_encoder_layer(x, None)\n",
    "# print(sample_encoder_layer_output.shape) # [64, 50, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mn_EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff, seq_len, rate=0.1):\n",
    "        super(mn_EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = mn_MultiHeadAttention(d_model, num_heads, seq_len)  # 多头注意力（padding mask）(self-attention)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "        self.layernorm2 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "\n",
    "        self.dropout1 = torch.nn.Dropout(rate)\n",
    "        self.dropout2 = torch.nn.Dropout(rate)\n",
    "\n",
    "    # x [b, inp_seq_len, embedding_dim] embedding_dim其实也=d_model\n",
    "    # mask [b,1,1,inp_seq_len]\n",
    "    def forward(self, x, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, x, mask)  # =>[b, seq_len, d_model]\n",
    "#         print(attn_output.shape)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)  # 残差&层归一化 =>[b, seq_len, d_model]\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # =>[b, seq_len, d_model]\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # 残差&层归一化 =>[b, seq_len, d_model]\n",
    "\n",
    "        return out2  # [b, seq_len, d_model]\n",
    "\n",
    "# layernorm = torch.nn.LayerNorm(normalized_shape=512, eps=1e-6)\n",
    "# x = torch.rand(4, MAX_LENGTH, 512)\n",
    "# print(layernorm(x).shape)\n",
    "\n",
    "# sample_encoder_layer = mn_EncoderLayer(512, 8, 2048,MAX_LENGTH)\n",
    "# x = torch.rand(64, MAX_LENGTH, 512) # [b, seq_len, d_model]\n",
    "# sample_encoder_layer_output = sample_encoder_layer(x, None)\n",
    "# print(sample_encoder_layer_output.shape) # [64, 50, 512]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 8.decoder layer\n",
    "\n",
    "每个解码器层包括以下3个子层：\n",
    "- masked的多头注意力（look ahead mask 和 padding mask）(self-attention)\n",
    "- 多头注意力（padding mask）(encoder-decoder attention)。\n",
    "    V和 K接收encoder的输出作为输入。Q接收masked的多头注意力子层的输出。\n",
    "- 点式前馈网络\n",
    "\n",
    "每个子层还伴随着一个残差连接，然后进行“层归一化”（LayerNorm）。\n",
    "\n",
    "每个子层的输出是 LayerNorm(x + Sublayer(x))。归一化是在 d_model（最后一个）维度完成的。\n",
    "\n",
    "Transformer 中共有 N 个解码器层。\n",
    "\n",
    "当 Q 接收到decoder的第一个注意力块的输出，并且 K 接收到encoder的输出时，注意力权重表示根据encoder的输出赋予decoder输入的重要性。\n",
    "换一种说法，decoder通过查看encoder输出和对其自身输出的自注意力，预测下一个词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model,\n",
    "                                       num_heads)  # masked的多头注意力（look ahead mask 和 padding mask）(self-attention)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)  # 多头注意力（padding mask）(encoder-decoder attention)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "        self.layernorm2 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "        self.layernorm3 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "\n",
    "        self.dropout1 = torch.nn.Dropout(rate)\n",
    "        self.dropout2 = torch.nn.Dropout(rate)\n",
    "        self.dropout3 = torch.nn.Dropout(rate)\n",
    "\n",
    "    # x [b, targ_seq_len, embedding_dim] embedding_dim其实也=d_model=512\n",
    "    # look_ahead_mask [b, 1, targ_seq_len, targ_seq_len] 这里传入的look_ahead_mask应该是已经结合了look_ahead_mask和padding mask的mask\n",
    "    # enc_output [b, inp_seq_len, d_model]\n",
    "    # padding_mask [b, 1, 1, inp_seq_len]\n",
    "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x,\n",
    "                                               look_ahead_mask)  # =>[b, targ_seq_len, d_model], [b, num_heads, targ_seq_len, targ_seq_len]\n",
    "        attn1 = self.dropout1(attn1)\n",
    "        out1 = self.layernorm1(x + attn1)  # 残差&层归一化 [b, targ_seq_len, d_model]\n",
    "\n",
    "        # Q: receives the output from decoder's first attention block，即 masked multi-head attention sublayer\n",
    "        # K V: V (value) and K (key) receive the encoder output as inputs\n",
    "        attn2, attn_weights_block2 = self.mha2(out1, enc_output, enc_output,\n",
    "                                               padding_mask)  # =>[b, targ_seq_len, d_model], [b, num_heads, targ_seq_len, inp_seq_len]\n",
    "        attn2 = self.dropout2(attn2)\n",
    "        out2 = self.layernorm2(out1 + attn2)  # 残差&层归一化 [b, targ_seq_len, d_model]\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # =>[b, targ_seq_len, d_model]\n",
    "        ffn_output = self.dropout3(ffn_output)\n",
    "        out3 = self.layernorm3(out2 + ffn_output)  # 残差&层归一化 =>[b, targ_seq_len, d_model]\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "        # [b, targ_seq_len, d_model], [b, num_heads, targ_seq_len, targ_seq_len], [b, num_heads, targ_seq_len, inp_seq_len]\n",
    "\n",
    "\n",
    "# sample_decoder_layer = DecoderLayer(512, 8, 2048) #\n",
    "# y = torch.rand(64, 40, 512) # [b, seq_len, d_model]\n",
    "# sample_decoder_layer_output,_,_ = sample_decoder_layer(y, sample_encoder_layer_output, None, None)\n",
    "# print(sample_decoder_layer_output.shape) # [64, 40, 512]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 9.encoder\n",
    "编码器 包括：\n",
    "- 输入嵌入（Input Embedding）\n",
    "- 位置编码（Positional Encoding）\n",
    "- N 个编码器层（encoder layers）\n",
    "\n",
    "输入经过嵌入（embedding）后，该嵌入与位置编码相加。该加法结果的输出是编码器层的输入。编码器的输出是解码器的输入\n",
    "\n",
    "注意：缩放 embedding\n",
    "\n",
    "原始论文的3.4节Embeddings and Softmax最后一句有提到： In the embedding layers, we multiply those weights by $\\sqrt{d_{model}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers,  # N个encoder layer\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dff,  # 点式前馈网络内层fn的维度\n",
    "                 input_vocab_size,  # 输入词表大小（源语言（法语））\n",
    "                 maximun_position_encoding,\n",
    "                 rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=input_vocab_size, embedding_dim=d_model)\n",
    "        self.pos_encoding = positional_encoding(maximun_position_encoding,\n",
    "                                                d_model)  # =>[1, max_pos_encoding, d_model=512]\n",
    "\n",
    "        # self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate).cuda() for _ in range(num_layers)] # 不行\n",
    "        self.enc_layers = torch.nn.ModuleList([EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)])\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(rate)\n",
    "\n",
    "    # x [b, inp_seq_len]\n",
    "    # mask [b, 1, 1, inp_sel_len]\n",
    "    def forward(self, x, mask):\n",
    "        inp_seq_len = x.shape[-1]\n",
    "\n",
    "        # adding embedding and position encoding\n",
    "        x = self.embedding(x)  # [b, inp_seq_len]=>[b, inp_seq_len, d_model]\n",
    "        # 缩放 embedding 原始论文的3.4节有提到： In the embedding layers, we multiply those weights by \\sqrt{d_model}.\n",
    "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
    "        pos_encoding = self.pos_encoding[:, :inp_seq_len, :]\n",
    "        pos_encoding = pos_encoding.cuda()  # ###############\n",
    "        x += pos_encoding  # [b, inp_seq_len, d_model]\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, mask)  # [b, inp_seq_len, d_model]=>[b, inp_seq_len, d_model]\n",
    "        return x  # [b, inp_seq_len, d_model]\n",
    "\n",
    "\n",
    "# sample_encoder = Encoder(num_layers=2,\n",
    "#                          d_model=512,\n",
    "#                          num_heads=8,\n",
    "#                          dff=2048,\n",
    "#                          input_vocab_size=8500,\n",
    "#                          maximun_position_encoding=10000)\n",
    "# sample_encoder = sample_encoder.to(device)\n",
    "\n",
    "# x = torch.tensor(np.random.randint(low=0, high=200, size=(64, 42))) # [b, seq_len]\n",
    "# # print(x.shape)\n",
    "# sample_encoder_output = sample_encoder(x.cuda(), None)\n",
    "# print(sample_encoder_output.shape) # [b, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mn_Encoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers,  # N个encoder layer\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dff,  # 点式前馈网络内层fn的维度\n",
    "                 input_vocab_size,  # 输入词表大小（源语言（法语））\n",
    "                 maximun_position_encoding,\n",
    "                 seq_len,\n",
    "                 rate=0.1):\n",
    "        super(mn_Encoder, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=input_vocab_size, embedding_dim=d_model)\n",
    "        self.pos_encoding = positional_encoding(maximun_position_encoding,\n",
    "                                                d_model)  # =>[1, max_pos_encoding, d_model=512]\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate).cuda() for _ in range(num_layers)] # 不行\n",
    "        self.enc_layers = torch.nn.ModuleList([mn_EncoderLayer(d_model, num_heads, dff, seq_len, rate) for _ in range(num_layers)])\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(rate)\n",
    "\n",
    "    # x [b, inp_seq_len]\n",
    "    # mask [b, 1, 1, inp_sel_len]\n",
    "    def forward(self, x, mask):\n",
    "        inp_seq_len = x.shape[-1]\n",
    "\n",
    "        # adding embedding and position encoding\n",
    "        x = self.embedding(x)  # [b, inp_seq_len]=>[b, inp_seq_len, d_model]\n",
    "        # 缩放 embedding 原始论文的3.4节有提到： In the embedding layers, we multiply those weights by \\sqrt{d_model}.\n",
    "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
    "        pos_encoding = self.pos_encoding[:, :inp_seq_len, :]\n",
    "        pos_encoding = pos_encoding.cuda()  # ###############\n",
    "        x += pos_encoding  # [b, inp_seq_len, d_model]\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, mask)  # [b, inp_seq_len, d_model]=>[b, inp_seq_len, d_model]\n",
    "        return x  # [b, inp_seq_len, d_model]\n",
    "\n",
    "\n",
    "# sample_encoder = mn_Encoder(num_layers=2,\n",
    "#                          d_model=512,\n",
    "#                          num_heads=8,\n",
    "#                          dff=2048,\n",
    "#                          input_vocab_size=8500,\n",
    "#                          maximun_position_encoding=10000,\n",
    "#                          seq_len = MAX_LENGTH)\n",
    "# sample_encoder = sample_encoder.to(device)\n",
    "\n",
    "# temp_inp = torch.tensor(np.random.randint(low=0, high=200, size=(64, MAX_LENGTH))) # [b, inp_seq_len]\n",
    "# print(x.shape)\n",
    "# sample_encoder_output = sample_encoder(temp_inp.cuda(), None)\n",
    "# print(sample_encoder_output.shape) # [b, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 10.decoder\n",
    "解码器包括：\n",
    "- 输出嵌入（Output Embedding）\n",
    "- 位置编码（Positional Encoding）\n",
    "\n",
    "N 个解码器层（decoder layers）\n",
    "\n",
    "目标（target）经过一个嵌入后，该嵌入和位置编码相加。该加法结果是解码器层的输入。解码器的输出是最后的线性层的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers,  # N个encoder layer\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dff,  # 点式前馈网络内层fn的维度\n",
    "                 target_vocab_size,  # target词表大小（目标语言（英语））\n",
    "                 maximun_position_encoding,\n",
    "                 rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=target_vocab_size, embedding_dim=d_model)\n",
    "        self.pos_encoding = positional_encoding(maximun_position_encoding,\n",
    "                                                d_model)  # =>[1, max_pos_encoding, d_model=512]\n",
    "\n",
    "        # self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate).cuda() for _ in range(num_layers)] # 不行\n",
    "        self.dec_layers = torch.nn.ModuleList([DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)])\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(rate)\n",
    "\n",
    "    # x [b, targ_seq_len]\n",
    "    # look_ahead_mask [b, 1, targ_seq_len, targ_seq_len] 这里传入的look_ahead_mask应该是已经结合了look_ahead_mask和padding mask的mask\n",
    "    # enc_output [b, inp_seq_len, d_model]\n",
    "    # padding_mask [b, 1, 1, inp_seq_len]\n",
    "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        targ_seq_len = x.shape[-1]\n",
    "\n",
    "        attention_weights = {}\n",
    "\n",
    "        # adding embedding and position encoding\n",
    "        x = self.embedding(x)  # [b, targ_seq_len]=>[b, targ_seq_len, d_model]\n",
    "        # 缩放 embedding 原始论文的3.4节有提到： In the embedding layers, we multiply those weights by \\sqrt{d_model}.\n",
    "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
    "        # x += self.pos_encoding[:, :targ_seq_len, :]  # [b, targ_seq_len, d_model]\n",
    "        pos_encoding = self.pos_encoding[:, :targ_seq_len, :]  # [b, targ_seq_len, d_model]\n",
    "        pos_encoding = pos_encoding.cuda() # ###############\n",
    "        x += pos_encoding  # [b, inp_seq_len, d_model]\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        #此处添加相对距离\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x, attn_block1, attn_block2 = self.dec_layers[i](x, enc_output, look_ahead_mask, padding_mask)\n",
    "            # => [b, targ_seq_len, d_model], [b, num_heads, targ_seq_len, targ_seq_len], [b, num_heads, targ_seq_len, inp_seq_len]\n",
    "\n",
    "            attention_weights[f'decoder_layer{i + 1}_block1'] = attn_block1\n",
    "            attention_weights[f'decoder_layer{i + 1}_block2'] = attn_block2\n",
    "\n",
    "        return x, attention_weights\n",
    "        # => [b, targ_seq_len, d_model],\n",
    "        # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "        #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "\n",
    "# sample_decoder = Decoder(num_layers=2,\n",
    "#                          d_model=512,\n",
    "#                          num_heads=8,\n",
    "#                          dff=2048,\n",
    "#                          target_vocab_size=8000,\n",
    "#                          maximun_position_encoding=5000)\n",
    "# sample_decoder = sample_decoder.to(device)\n",
    "\n",
    "# y = torch.tensor(np.random.randint(low=0, high=200, size=(64, 42))) # [b, seq_len]\n",
    "# # print(y.shape) # [64, 36]\n",
    "# output, attn = sample_decoder(y.cuda(),\n",
    "#                               enc_output=sample_encoder_output, # [64, 42, 512]\n",
    "#                               look_ahead_mask=None,\n",
    "#                               padding_mask=None)\n",
    "# print(output.shape) # [64, 36, 512]\n",
    "# print(attn['decoder_layer2_block1'].shape) # [64, 8, 36, 36]\n",
    "# print(attn['decoder_layer2_block2'].shape) # [64, 8, 36, 42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dependency_parsing_Decoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers,  # N个encoder layer\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dff,  # 点式前馈网络内层fn的维度\n",
    "                 target_vocab_size,  # target词表大小（目标语言（英语））\n",
    "                 maximun_position_encoding,\n",
    "                 rate=0.1):\n",
    "        super(dependency_parsing_Decoder, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=target_vocab_size, embedding_dim=d_model)\n",
    "        self.pos_encoding = positional_encoding(maximun_position_encoding,\n",
    "                                                d_model)  # =>[1, max_pos_encoding, d_model=512]\n",
    "        \n",
    "        self.dependency_parsing_Attention = torch.nn.ModuleList([dependency_parsing_Attention(d_model)])\n",
    "        # self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate).cuda() for _ in range(num_layers)] # 不行\n",
    "        self.dec_layers = torch.nn.ModuleList([DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)])\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(rate)\n",
    "\n",
    "    # x [b, targ_seq_len]\n",
    "    # look_ahead_mask [b, 1, targ_seq_len, targ_seq_len] 这里传入的look_ahead_mask应该是已经结合了look_ahead_mask和padding mask的mask\n",
    "    # enc_output [b, inp_seq_len, d_model]\n",
    "    # padding_mask [b, 1, 1, inp_seq_len]\n",
    "    def forward(self, x, dependency_parsing_matrix, enc_output, look_ahead_mask, padding_mask):\n",
    "#         print('dependency_parsing_Decoder_input:',x.shape)\n",
    "        targ_seq_len = x.shape[-1]\n",
    "\n",
    "        attention_weights = {}\n",
    "\n",
    "        # adding embedding and position encoding\n",
    "        x = self.embedding(x)  # [b, targ_seq_len]=>[b, targ_seq_len, d_model]\n",
    "        # 缩放 embedding 原始论文的3.4节有提到： In the embedding layers, we multiply those weights by \\sqrt{d_model}.\n",
    "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
    "        # x += self.pos_encoding[:, :targ_seq_len, :]  # [b, targ_seq_len, d_model]\n",
    "        pos_encoding = self.pos_encoding[:, :targ_seq_len, :]  # [b, targ_seq_len, d_model]\n",
    "        pos_encoding = pos_encoding.cuda() # ###############\n",
    "        x += pos_encoding  # [b, inp_seq_len, d_model]\n",
    "#         print('x += pos_encoding:',x.shape)\n",
    "        \n",
    "        #此处添加相对距离\n",
    "        x, _ = self.dependency_parsing_Attention[0](x, x, x, dependency_parsing_matrix, look_ahead_mask)\n",
    "        \n",
    "#         print('dependency_parsing_Attention:',x.shape)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x, attn_block1, attn_block2 = self.dec_layers[i](x, enc_output, look_ahead_mask, padding_mask)\n",
    "            # => [b, targ_seq_len, d_model], [b, num_heads, targ_seq_len, targ_seq_len], [b, num_heads, targ_seq_len, inp_seq_len]\n",
    "\n",
    "            attention_weights[f'decoder_layer{i + 1}_block1'] = attn_block1\n",
    "            attention_weights[f'decoder_layer{i + 1}_block2'] = attn_block2\n",
    "\n",
    "        return x, attention_weights\n",
    "        # => [b, targ_seq_len, d_model],\n",
    "        # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "        #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "\n",
    "# sample_decoder = dependency_parsing_Decoder(num_layers=2,\n",
    "#                          d_model=512,\n",
    "#                          num_heads=8,\n",
    "#                          dff=2048,\n",
    "#                          target_vocab_size=8000,\n",
    "#                          maximun_position_encoding=5000)\n",
    "# sample_decoder = sample_decoder.to(device)\n",
    "\n",
    "# y = torch.tensor(np.random.randint(low=0, high=200, size=(64, 42))) # [b, seq_len]\n",
    "# dependency_parsing_matrix = torch.tensor(np.random.randint(low=0, high=200, size=(64, 42, 42)))\n",
    "\n",
    "# output, attn = sample_decoder(y.cuda(),\n",
    "#                               dependency_parsing_matrix.cuda(),\n",
    "#                               enc_output=sample_encoder_output, # [64, 42, 512]\n",
    "#                               look_ahead_mask=None,\n",
    "#                               padding_mask=None)\n",
    "# print(output.shape) # [64, 36, 512]\n",
    "# print(attn['decoder_layer2_block1'].shape) # [64, 8, 36, 36]\n",
    "# print(attn['decoder_layer2_block2'].shape) # [64, 8, 36, 42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 11.搭建transformer\n",
    "transformer 含有3各部分\n",
    "- encoder\n",
    "- decoder\n",
    "- final linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# class Transformer(torch.nn.Module):\n",
    "#     def __init__(self,\n",
    "#                  num_layers,  # N个encoder layer\n",
    "#                  d_model,\n",
    "#                  num_heads,\n",
    "#                  dff,  # 点式前馈网络内层fn的维度\n",
    "#                  input_vocab_size,  # input此表大小（源语言（法语））\n",
    "#                  target_vocab_size,  # target词表大小（目标语言（英语））\n",
    "#                  pe_input,  # input max_pos_encoding\n",
    "#                  pe_target,  # input max_pos_encoding\n",
    "#                  rate=0.1):\n",
    "#         super(Transformer, self).__init__()\n",
    "\n",
    "#         self.encoder = Encoder(num_layers,\n",
    "#                                d_model,\n",
    "#                                num_heads,\n",
    "#                                dff,\n",
    "#                                input_vocab_size,\n",
    "#                                pe_input,\n",
    "#                                rate)\n",
    "#         self.decoder = Decoder(num_layers,\n",
    "#                                d_model,\n",
    "#                                num_heads,\n",
    "#                                dff,\n",
    "#                                target_vocab_size,\n",
    "#                                pe_target,\n",
    "#                                rate)\n",
    "#         self.final_layer = torch.nn.Linear(d_model, target_vocab_size)\n",
    "\n",
    "#     # inp [b, inp_seq_len]\n",
    "#     # targ [b, targ_seq_len]\n",
    "#     # enc_padding_mask [b, 1, 1, inp_seq_len]\n",
    "#     # look_ahead_mask [b, 1, targ_seq_len, targ_seq_len]\n",
    "#     # dec_padding_mask [b, 1, 1, inp_seq_len] # 注意这里的维度是inp_seq_len\n",
    "#     def forward(self, inp, targ, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "#         enc_output = self.encoder(inp, enc_padding_mask)  # =>[b, inp_seq_len, d_model]\n",
    "\n",
    "#         dec_output, attention_weights = self.decoder(targ, enc_output, look_ahead_mask, dec_padding_mask)\n",
    "#         # => [b, targ_seq_len, d_model],\n",
    "#         # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "#         #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "#         final_output = self.final_layer(dec_output)  # =>[b, targ_seq_len, target_vocab_size]\n",
    "\n",
    "#         return final_output, attention_weights\n",
    "#         # [b, targ_seq_len, target_vocab_size]\n",
    "#         # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "#         #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "\n",
    "\n",
    "# sample_transformer = Transformer(num_layers=2,\n",
    "#                                  d_model=512,\n",
    "#                                  num_heads=8,\n",
    "#                                  dff=2048,\n",
    "#                                  input_vocab_size=8500,\n",
    "#                                  target_vocab_size=8000,\n",
    "#                                  pe_input=10000,\n",
    "#                                  pe_target=6000)\n",
    "# sample_transformer = sample_transformer.to(device)\n",
    "\n",
    "# temp_inp = torch.tensor(np.random.randint(low=0, high=200, size=(64, 36))) # [b, inp_seq_len]\n",
    "# temp_targ = torch.tensor(np.random.randint(low=0, high=200, size=(64, 42))) # [b, targ_seq_len]\n",
    "\n",
    "# fn_out, attn = sample_transformer(temp_inp.cuda(), temp_targ.cuda(), None, None, None)\n",
    "# print(fn_out.shape) # [64, 36, 8000]\n",
    "# print(attn['decoder_layer2_block1'].shape) # [64, 8, 36, 36]\n",
    "# print(attn['decoder_layer2_block2'].shape) # [64, 8, 36, 42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mn_Transformer(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers,  # N个encoder layer\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dff,  # 点式前馈网络内层fn的维度\n",
    "                 input_vocab_size,  # input此表大小（源语言（法语））\n",
    "                 target_vocab_size,  # target词表大小（目标语言（英语））\n",
    "                 pe_input,  # input max_pos_encoding\n",
    "                 pe_target,  # input max_pos_encoding\n",
    "                 seq_len,\n",
    "                 rate=0.1):\n",
    "        super(mn_Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = mn_Encoder(num_layers,\n",
    "                               d_model,\n",
    "                               num_heads,\n",
    "                               dff,\n",
    "                               input_vocab_size,\n",
    "                               pe_input,\n",
    "                               seq_len,\n",
    "                               rate)\n",
    "        # 修改\n",
    "        self.decoder = dependency_parsing_Decoder(num_layers,\n",
    "                               d_model,\n",
    "                               num_heads,\n",
    "                               dff,\n",
    "                               target_vocab_size,\n",
    "                               pe_target,\n",
    "                               rate)\n",
    "        self.final_layer = torch.nn.Linear(d_model, target_vocab_size)\n",
    "\n",
    "    # inp [b, inp_seq_len]\n",
    "    # targ [b, targ_seq_len]\n",
    "    # enc_padding_mask [b, 1, 1, inp_seq_len]\n",
    "    # look_ahead_mask [b, 1, targ_seq_len, targ_seq_len]\n",
    "    # dec_padding_mask [b, 1, 1, inp_seq_len] # 注意这里的维度是inp_seq_len\n",
    "    def forward(self, inp, targ, dependency_parsing_matrix, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        enc_output = self.encoder(inp, enc_padding_mask)  # =>[b, inp_seq_len, d_model]\n",
    "        dec_output, attention_weights = self.decoder(targ, dependency_parsing_matrix, enc_output, look_ahead_mask, dec_padding_mask)\n",
    "        # => [b, targ_seq_len, d_model],\n",
    "        # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "        #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "        final_output = self.final_layer(dec_output)  # =>[b, targ_seq_len, target_vocab_size]\n",
    "\n",
    "        return final_output, attention_weights\n",
    "        # [b, targ_seq_len, target_vocab_size]\n",
    "        # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "        #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_transformer = mn_Transformer(num_layers=2,\n",
    "#                                  d_model=128,\n",
    "#                                  num_heads=8,\n",
    "#                                  dff=2048,\n",
    "#                                  input_vocab_size=8500,\n",
    "#                                  target_vocab_size=8000,\n",
    "#                                  pe_input=10000,\n",
    "#                                  pe_target=10000,\n",
    "#                                  seq_len=MAX_LENGTH+2)\n",
    "# sample_transformer = sample_transformer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_inp = torch.tensor(np.random.randint(low=0, high=200, size=(64, MAX_LENGTH+2))) # [b, inp_seq_len]\n",
    "# temp_targ = torch.tensor(np.random.randint(low=0, high=200, size=(64, MAX_LENGTH+2))) # [b, targ_seq_len]\n",
    "# dependency_parsing_matrix = torch.tensor(np.random.randint(low=0, high=200, size=(64, MAX_LENGTH+2, MAX_LENGTH+2)))\n",
    "\n",
    "# fn_out, attn = sample_transformer(temp_inp.cuda(), temp_targ.cuda(),dependency_parsing_matrix.cuda(), None, None, None)\n",
    "\n",
    "\n",
    "# # for step, (inp, targ) in enumerate(train_dataloader, start=1):\n",
    "# #     print(inp,inp.shape,'\\n',targ,targ.shape)\n",
    "# #     fn_out, attn = sample_transformer(inp.cuda(), targ.cuda(), None, None, None)\n",
    "# #     break\n",
    "\n",
    "\n",
    "\n",
    "# print(fn_out.shape) # [64, 36, 8000]\n",
    "# print(attn['decoder_layer2_block1'].shape) # [64, 8, 36, 36]\n",
    "# print(attn['decoder_layer2_block2'].shape) # [64, 8, 36, 42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 12.设置超参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_layers： 5\n",
      "d_model： 256\n",
      "dff： 2048\n",
      "num_heads： 8\n",
      "dropout_rate： 0.2\n"
     ]
    }
   ],
   "source": [
    "#Transformer 的基础模型使用的数值为：num_layers=6，d_model = 512，dff = 2048\n",
    "#为了让本示例小且相对较快，已经减小了num_layers、 d_model 和 dff 的值。\n",
    "num_layers = 5\n",
    "d_model = 256\n",
    "dff = 2048\n",
    "num_heads = 8\n",
    "\n",
    "print('num_layers：', num_layers)\n",
    "print('d_model：', d_model)\n",
    "print('dff：', dff)\n",
    "print('num_heads：', num_heads)\n",
    "\n",
    "dropout_rate = 0.2\n",
    "print('dropout_rate：', dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 13.优化器\n",
    "根据论文中的公式，将 Adam 优化器与自定义的学习速率调度程序（scheduler）配合使用。\n",
    "\n",
    "$$lrate = d_{model}^{-0.5} * min(\\text{step_num}^{-0.5},\\; \\text{step_num}*\\text{warmup_steps}^{-1.5})$$\n",
    "\n",
    "关于pytorch optimizer，参考：\n",
    "\n",
    "https://www.cnblogs.com/wanghui-garcia/p/10895397.html\n",
    "\n",
    "https://www.jianshu.com/p/5d85a59f1bac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, d_model, warm_steps=4):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warm_steps\n",
    "\n",
    "        super(CustomSchedule, self).__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"\n",
    "        # rsqrt 函数用于计算 x 元素的平方根的倒数.  即= 1 / sqrt{x}\n",
    "        arg1 = torch.rsqrt(torch.tensor(self._step_count, dtype=torch.float32))\n",
    "        arg2 = torch.tensor(self._step_count * (self.warmup_steps ** -1.5), dtype=torch.float32)\n",
    "        dynamic_lr = torch.rsqrt(self.d_model) * torch.minimum(arg1, arg2)\n",
    "        \"\"\"\n",
    "        # print('*'*27, self._step_count)\n",
    "        arg1 = self._step_count ** (-0.5)\n",
    "        arg2 = self._step_count * (self.warmup_steps ** -1.5)\n",
    "        dynamic_lr = (self.d_model ** (-0.5)) * min(arg1, arg2)\n",
    "        # print('dynamic_lr:', dynamic_lr)\n",
    "        return [dynamic_lr for group in self.optimizer.param_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # 测试\n",
    "# model = sample_transformer\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "# learning_rate = CustomSchedule(optimizer, d_model, warm_steps=4000)\n",
    "\n",
    "# lr_list = []\n",
    "# for i in range(1, 20000):\n",
    "#     learning_rate.step()\n",
    "#     lr_list.append(learning_rate.get_lr()[0])\n",
    "# plt.figure()\n",
    "# plt.plot(np.arange(1, 20000), lr_list)\n",
    "# plt.legend(['warmup=4000 steps'])\n",
    "# plt.ylabel(\"Learning Rate\")\n",
    "# plt.xlabel(\"Train Step\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # 其他的学习率调度器测试，例如pytorch自带的StepLR\n",
    "# _model = sample_transformer\n",
    "# _optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# _learning_rate = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# lr_list = []\n",
    "# for i in range(1, 50):\n",
    "#     _learning_rate.step()\n",
    "#     lr_list.append(_learning_rate.get_lr()[0])\n",
    "# plt.figure()\n",
    "# plt.plot(np.arange(1, 50), lr_list)\n",
    "# plt.legend(['StepLR:gamma=0.5'])\n",
    "# plt.ylabel(\"Learning Rate\")\n",
    "# plt.xlabel(\"Train Step\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 14.损失和评价准则\n",
    "\n",
    "计算loss时要把mask=1的位置的去除掉\n",
    "\n",
    "### 【大坑！】\n",
    "【注意】，当输入是多维时交叉熵的参数维度，跟tf2不一样，tf2中pred是【b,seq_len,vocab_size】\n",
    "pytorch中pred应该调整为【b,vocab_size,seq_len】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 'none'表示直接返回b个样本的loss，默认求平均\n",
    "loss_object = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "# 【注意】，当输入是多维时交叉熵的参数维度，跟tf2不一样，tf2中pred是【b,seq_len,vocab_size】\n",
    "# pytorch中pred应该调整为【b,vocab_size,seq_len】\n",
    "\"\"\"\n",
    "- Input: :math:`(N, C)` where `C = number of classes`, or\n",
    "          :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1`\n",
    "          in the case of `K`-dimensional loss.\n",
    "\n",
    "- Target: :math:`(N)` where each value is :math:`0 \\leq \\text{targets}[i] \\leq C-1`, or\n",
    "          :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \\geq 1` in the case of\n",
    "          K-dimensional loss.\n",
    "\"\"\"\n",
    "\n",
    "# real [b, targ_seq_len]\n",
    "# pred [b, targ_seq_len, target_vocab_size]\n",
    "def mask_loss_func(real, pred):\n",
    "    # print(real.shape, pred.shape)\n",
    "    # _loss = loss_object(pred, real) # [b, targ_seq_len]\n",
    "    _loss = loss_object(pred.transpose(-1, -2), real)  # [b, targ_seq_len]\n",
    "\n",
    "    # logical_not  取非\n",
    "    # mask 每个元素为bool值，如果real中有pad，则mask相应位置就为False\n",
    "    # mask = torch.logical_not(real.eq(0)).type(_loss.dtype) # [b, targ_seq_len] pad=0的情况\n",
    "    mask = torch.logical_not(real.eq(pad)).type(_loss.dtype)  # [b, targ_seq_len] pad!=0的情况\n",
    "\n",
    "    # 对应位置相乘，token上的损失被保留了下来，pad的loss被置为0或False 去掉，不计算在内\n",
    "    _loss *= mask\n",
    "\n",
    "    return _loss.sum() / mask.sum().item()\n",
    "\n",
    "# 另一种实现方式\n",
    "def mask_loss_func2(real, pred):\n",
    "    # _loss = loss_object(pred, real) # [b, targ_seq_len]\n",
    "    _loss = loss_object(pred.transpose(-1, -2), real)  # [b, targ_seq_len]\n",
    "    # mask = torch.logical_not(real.eq(0)) # [b, targ_seq_len] bool值\n",
    "    mask = torch.logical_not(real.eq(pad)) # [b, targ_seq_len] bool值\n",
    "    _loss = _loss.masked_select(mask) # mask必须是BoolTensor或ByteTensor类型\n",
    "    return _loss.mean()\n",
    "\n",
    "\n",
    "# y_pred = torch.randn(3,3) # [3,3]\n",
    "# y_true = torch.tensor([1,2,0]) # [3]\n",
    "# # print(y_true.shape, y_pred.shape)\n",
    "# print(loss_object(y_pred, y_true))\n",
    "# print('计算loss时把mask的也算进去了,损失会偏大？：', loss_object(y_pred, y_true).mean())\n",
    "# print('计算loss时去除mask:', mask_loss_func(y_true, y_pred))\n",
    "# print('计算loss时去除mask:', mask_loss_func2(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "同样计算metric（如acc）时要把mask=1的位置的去除掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# real [b, targ_seq_len]\n",
    "# pred [b, targ_seq_len, target_vocab_size]\n",
    "def mask_accuracy_func(real, pred):\n",
    "    _pred = pred.argmax(dim=-1)  # [b, targ_seq_len, target_vocab_size]=>[b, targ_seq_len]\n",
    "    corrects = _pred.eq(real)  # [b, targ_seq_len] bool值\n",
    "\n",
    "    # logical_not  取非\n",
    "    # mask 每个元素为bool值，如果real中有pad，则mask相应位置就为False\n",
    "    # mask = torch.logical_not(real.eq(0)) # [b, targ_seq_len] bool值 pad=0的情况\n",
    "    mask = torch.logical_not(real.eq(pad))  # [b, targ_seq_len] bool值 pad!=0的情况\n",
    "\n",
    "    # 对应位置相乘，token上的值被保留了下来，pad上的值被置为0或False 去掉，不计算在内\n",
    "    corrects *= mask\n",
    "\n",
    "    return corrects.sum().float() / mask.sum().item()\n",
    "\n",
    "# 另一种实现方式\n",
    "def mask_accuracy_func2(real, pred):\n",
    "    _pred = pred.argmax(dim=-1) # [b, targ_seq_len, target_vocab_size]=>[b, targ_seq_len]\n",
    "    corrects = _pred.eq(real).type(torch.float32) # [b, targ_seq_len]\n",
    "    # mask = torch.logical_not(real.eq(0)) # [b, targ_seq_len] bool值\n",
    "    mask = torch.logical_not(real.eq(pad)) # [b, targ_seq_len] bool值\n",
    "    corrects = corrects.masked_select(mask) # [真正有token的个数] 平摊开成1维的\n",
    "\n",
    "    return corrects.mean()\n",
    "\n",
    "def mask_accuracy_func3(real, pred):\n",
    "    _pred = pred.argmax(dim=-1) # [b, targ_seq_len, target_vocab_size]=>[b, targ_seq_len]\n",
    "    corrects = _pred.eq(real) # [b, targ_seq_len] bool值\n",
    "    # mask = torch.logical_not(real.eq(0)) # [b, targ_seq_len] bool值\n",
    "    mask = torch.logical_not(real.eq(pad)) # [b, targ_seq_len] bool值\n",
    "    corrects = torch.logical_and(corrects, mask)\n",
    "    # print(corrects.dtype) # bool\n",
    "    # print(corrects.sum().dtype) #int64\n",
    "    return corrects.sum().float()/mask.sum().item()\n",
    "\n",
    "# y_pred = torch.randn(3,3) # [3,3]\n",
    "# y_true = torch.tensor([0,2,1]) # [3] 最后一个1表示pad噢~\n",
    "# print(y_true)\n",
    "# print(y_pred)\n",
    "# print('计算acc时去除mask:', mask_accuracy_func(y_true, y_pred))\n",
    "# print('计算acc时去除mask:', mask_accuracy_func2(y_true, y_pred))\n",
    "# print('计算acc时去除mask:', mask_accuracy_func3(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 15.生成mask\n",
    "\n",
    "$$Attention(Q,K,V)=softmax_{(k)}(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "注意：实现时对mask的处理\n",
    "\n",
    "mask=1的位置是pad或者future token，乘以-1e9（-1*10^9）成为负无穷，经过softmax后会趋于0\n",
    "\n",
    "【注意】：在计算decoder的第2个attention block时（encoder-decoder attention），\n",
    "从scaled dot-product的公式可以看出，此时Q是decoder的第一个attention block的输出，\n",
    "而K，V都来自encoder的输出。QK^T后得到[...,seq_len_q, seq_len_k]，而softmax是在seq_len_k维\n",
    "上进行的，softmax后seq_len_k维上pad的位置被置于0，乘以V（seq_len_k=seq_len_v=encoder output的seq_len）\n",
    "后那些pad的位置还是只会是0。所以仅从公式就可以看出decoder的第2个attention block的padding mask是基于\n",
    "encoder output 的seq_len即整个模型的inp_seq_len来设置的，而不是targ_seq_len。\n",
    "\n",
    "从注意力角度来看，可以理解成：当 Q 接收到decoder的第一个注意力块的输出，并且 K 接收到encoder的输出时，注意力权重表示根据encoder的输出赋予decoder输入的重要性。\n",
    "换一种说法，decoder通过查看encoder输出和对其自身输出的自注意`力，预测下一个词。\n",
    "\n",
    "另外 padding mask 的维度是[b,1,1,seq_len] 一般只关注最后一个seq_len维度上pad(无论是在\n",
    "self-attention 或者encoder-decoder attention上都比较容易理解)\n",
    "\n",
    "而 look-ahead mask 是decoder self-attention时用于mask future token的，因为是“自”注意力，\n",
    "所以维度是[...,seq_len, seq_len]，需要自己跟自己运算，所以在QK（此时Q=K=V）矩阵乘的结果的最后2个维度上都需要mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# inp [b, inp_seq_len] 序列已经加入pad填充\n",
    "# targ [b, targ_seq_len] 序列已经加入pad填充\n",
    "def create_mask(inp, targ):\n",
    "    # encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(inp)  # =>[b,1,1,inp_seq_len] mask=1的位置为pad\n",
    "\n",
    "    # decoder's first attention block(self-attention)\n",
    "    # 使用的padding create_mask & look-ahead create_mask\n",
    "    look_ahead_mask = create_look_ahead_mask(targ.shape[-1])  # =>[targ_seq_len,targ_seq_len] ##################\n",
    "    dec_targ_padding_mask = create_padding_mask(targ)  # =>[b,1,1,targ_seq_len]\n",
    "    combined_mask = torch.max(look_ahead_mask, dec_targ_padding_mask)  # 结合了2种mask =>[b,1,targ_seq_len,targ_seq_len]\n",
    "\n",
    "    # decoder's second attention block(encoder-decoder attention) 使用的padding create_mask\n",
    "    # 【注意】：这里的mask是用于遮挡encoder output的填充pad，而encoder的输出与其输入shape都是[b,inp_seq_len,d_model]\n",
    "    # 所以这里mask的长度是inp_seq_len而不是targ_mask_len\n",
    "    dec_padding_mask = create_padding_mask(inp)  # =>[b,1,1,inp_seq_len] mask=1的位置为pad\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
    "    # [b,1,1,inp_seq_len], [b,1,targ_seq_len,targ_seq_len], [b,1,1,inp_seq_len]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 16.训练和保存\n",
    "\n",
    "### 配置检查点 save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 47,206,837 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "transformer = mn_Transformer(num_layers,\n",
    "                          d_model,\n",
    "                          num_heads,\n",
    "                          dff,\n",
    "                          input_vocab_size,\n",
    "                          target_vocab_size,\n",
    "                          pe_input=input_vocab_size,\n",
    "                          pe_target=target_vocab_size,\n",
    "                          seq_len=MAX_LENGTH+2,\n",
    "                          rate=dropout_rate)\n",
    "\n",
    "# print(transformer) # 打印模型基本信息\n",
    "\n",
    "transformer = transformer.to(device)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {count_parameters(transformer):,} trainable parameters')\n",
    "\n",
    "\n",
    "if ngpu > 1: # 并行化\n",
    "    transformer = torch.nn.DataParallel(transformer,  device_ids=list(range(ngpu))) # 设置并行执行  device_ids=[0,1]\n",
    "\n",
    "# optimizer = torch.optim.Adam(transformer.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "# lr_scheduler = CustomSchedule(optimizer, d_model, warm_steps=4000)\n",
    "\n",
    "optimizer = torch.optim.AdamW(transformer.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.02, amsgrad=True)\n",
    "\n",
    "lr_scheduler = CustomSchedule(optimizer, d_model, warm_steps)\n",
    "lr_scheduler2 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max',factor=0.5,verbose=True,min_lr= 0,patience=1)\n",
    "#optimizer = adamod.AdaMod(transformer.parameters(), lr=1e-3, beta3=0.999)\n",
    "# print('optimizer：', optimizer)\n",
    "\n",
    "#lr_scheduler2 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max',factor=0.5,verbose=True,min_lr= 0,patience=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 关于position encoding的一点自己的理解：\n",
    "\"\"\"\n",
    "为什么这里传入的pe_input是input_vocab_size，也就是说position encoding传入的pos参数是词表大小\n",
    "而我一开始理解的max_seq_len的大小，所以才在encoder中获取位置编码是self.pos_encoding[:, :seq_len, :]\n",
    "但知道看到这里传入的是词表大小我才发现我理解错了\n",
    "\n",
    "其实position encoding 就跟embedding 一样，就是【vocab_size x d_model】的\n",
    "但此时另一个问题又出现了，那为什么在获取position encoding时 使用的是pos_encoding[:, :seq_len, :]而不是\n",
    "像embedding一样 去look-up 查询每个token的位置表示呢？而是直接取得前seq_len个表示作为位置表示？\n",
    "\n",
    "这就体现了这种位置编码的巧妙之处，他不需要训练，就能独特的表示每一个单独的token，而每个token之间又存在关系\n",
    "所以这并不需要位置编码和token是一一对应（固定死的），只需要位置编码能传达这2点信息（即token的独特性和相对依赖性）就够了\n",
    "即使是同一个token 在一个句子的不同的位置时 他的位置编码当然也是不一样的了 如果像embedding那样去查 那一个句子的\n",
    "不同位置的相同token  其表示就会是一样的  就体现不出位置关系\n",
    "\n",
    "这样一想的话，似乎把pe_input设置成max_seq_len也是可以的。但是这里实现position encoding方式（sin  cos）\n",
    "可以在测试阶段接受长度超过训练集实例的情况！所以干脆把pe_input设置成词表大小？\n",
    "\"\"\"\n",
    "\n",
    "# inp [b,inp_seq_len]\n",
    "# targ [b,targ_seq_len]\n",
    "\"\"\"\n",
    "拆分targ, 例如：sentence = \"SOS A lion in the jungle is sleeping EOS\"\n",
    "tar_inp = \"<start>> A lion in the jungle is sleeping\"\n",
    "tar_real = \"A lion in the jungle is sleeping <end>\"\n",
    "\"\"\"\n",
    "def train_step(model, inp, targ, dependency_matrix):\n",
    "    # 目标（target）被分成了 tar_inp 和 tar_real\n",
    "    # tar_inp 作为输入传递到解码器。\n",
    "    # tar_real 是位移了 1 的同一个输入：在 tar_inp 中的每个位置，tar_real 包含了应该被预测到的下一个标记（token）。\n",
    "    targ_inp = targ[:, :-1] # targ_inp的尺寸是[batch,MAX_LENGTH+1],因为舍弃了<end> https://zhuanlan.zhihu.com/p/166608727\n",
    "#     print('targ_inp:',targ_inp.shape)\n",
    "    targ_real = targ[:, 1:]\n",
    "    \n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_mask(inp, targ_inp)\n",
    "#     print('enc_padding_mask',enc_padding_mask.shape)\n",
    "#     print('combined_mask',combined_mask.shape)\n",
    "#     print('dec_padding_mask',dec_padding_mask.shape)\n",
    "    \n",
    "    inp = inp.to(device)\n",
    "    targ_inp = targ_inp.to(device)\n",
    "    targ_real = targ_real.to(device)\n",
    "    dependency_matrix = dependency_matrix.to(device)\n",
    "#     print('inp.shape',inp.shape)\n",
    "#     print('targ_inp.shape',targ_inp.shape)\n",
    "#     print('dependency_matrix.shape',dependency_matrix.shape)\n",
    "    \n",
    "    enc_padding_mask = enc_padding_mask.to(device)\n",
    "    combined_mask = combined_mask.to(device)\n",
    "    dec_padding_mask = dec_padding_mask.to(device)\n",
    "    \n",
    "    # print('device:', inp.device, targ_inp)\n",
    "\n",
    "    model.train()  # 设置train mode\n",
    "\n",
    "    optimizer.zero_grad()  # 梯度清零\n",
    "\n",
    "    # forward\n",
    "    prediction, _ = model(inp, targ_inp, dependency_matrix, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "    # [b, targ_seq_len, target_vocab_size]\n",
    "    # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "    #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "\n",
    "    loss = mask_loss_func(targ_real, prediction)\n",
    "    metric = mask_accuracy_func(targ_real, prediction)\n",
    "\n",
    "    # backward\n",
    "    loss.backward()  # 反向传播计算梯度\n",
    "    optimizer.step()  # 更新参数\n",
    "\n",
    "    return loss.item(), metric.item()\n",
    "\n",
    "\n",
    "# # 检查train_step()的效果\n",
    "# batch_src, batch_targ = next(iter(train_dataloader)) # [64,10], [64,10]\n",
    "# print(train_step(transformer, batch_src, batch_targ))\n",
    "# \"\"\"\n",
    "# x += pos_encoding  # [b, inp_seq_len, d_model]\n",
    "# RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0!\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def validate_step(model, inp, targ, dependency_matrix):\n",
    "    targ_inp = targ[:, :-1]\n",
    "    targ_real = targ[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_mask(inp, targ_inp)\n",
    "\n",
    "    inp = inp.to(device)\n",
    "    targ_inp = targ_inp.to(device)\n",
    "    targ_real = targ_real.to(device)\n",
    "    dependency_matrix = dependency_matrix.to(device)\n",
    "    \n",
    "#     print('inp.shape:',inp.shape)\n",
    "#     print('targ_inp.shape:',targ_inp.shape)\n",
    "#     print('dependency_matrix.shape:',dependency_matrix.shape)\n",
    "    \n",
    "    enc_padding_mask = enc_padding_mask.to(device)\n",
    "    combined_mask = combined_mask.to(device)\n",
    "    dec_padding_mask = dec_padding_mask.to(device)\n",
    "\n",
    "    model.eval()  # 设置eval mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # forward\n",
    "        prediction, _ = model(inp, targ_inp, dependency_matrix, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "        # [b, targ_seq_len, target_vocab_size]\n",
    "        # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "        #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "\n",
    "        val_loss = mask_loss_func(targ_real, prediction)\n",
    "        val_metric = mask_accuracy_func(targ_real, prediction)\n",
    "\n",
    "    return val_loss.item(), val_metric.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print_trainstep_every = 500  # 每50个step做一次打印\n",
    "\n",
    "metric_name = 'acc'\n",
    "# df_history = pd.DataFrame(columns=['epoch', 'loss', metric_name]) # 记录训练历史信息\n",
    "df_history = pd.DataFrame(columns=['epoch', 'loss', metric_name, 'val_loss', 'val_' + metric_name])\n",
    "\n",
    "\n",
    "# 打印时间\n",
    "def printbar():\n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m_%d %H:%M:%S')\n",
    "    print('\\n' + \"====\"*8 + '%s'%nowtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, epochs, train_dataloader, val_dataloader, dep_train, dep_val, print_every):\n",
    "    starttime = time.time()\n",
    "    print('*' * 10, 'start training...')\n",
    "    printbar()\n",
    "\n",
    "    best_acc = 0.\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        loss_sum = 0.\n",
    "        metric_sum = 0.\n",
    "        \n",
    "        # dependency_matrix\n",
    "        for step, (inp, targ) in enumerate(train_dataloader, start=1):\n",
    "            # inp [64, 10] , targ [64, 10] \n",
    "#             print('inp.shape:',inp.shape)\n",
    "#             print('targ.shape:',targ.shape)\n",
    "            \n",
    "            # 提取依存句法矩阵\n",
    "            st = (step-1) * inp.shape[0]\n",
    "            end = st + inp.shape[0]\n",
    "            dependency_matrix = dep_train[st:end,:,:]\n",
    "#             print('dependency_matrix.shape:',dependency_matrix.shape)\n",
    "            \n",
    "            loss, metric = train_step(model, inp, targ, dependency_matrix)\n",
    "\n",
    "            loss_sum += loss\n",
    "            metric_sum += metric\n",
    "\n",
    "            # 打印batch级别日志\n",
    "            if step % print_every == 0:\n",
    "                print('*' * 5, f'[step = {step}] loss: {loss_sum / step:.5f}, {metric_name}: {metric_sum / step:.5f}')\n",
    "\n",
    "            lr_scheduler.step()  # 更新学习率\n",
    "\n",
    "        # 一个epoch的train结束，做一次验证\n",
    "        # test(model, train_dataloader)\n",
    "#         print('val')\n",
    "        val_loss_sum = 0.\n",
    "        val_metric_sum = 0.\n",
    "        for val_step, (inp, targ) in enumerate(val_dataloader, start=1):\n",
    "            # inp [64, 10] , targ [64, 10]\n",
    "            \n",
    "            # 提取依存句法矩阵\n",
    "            st = (val_step-1) * inp.shape[0]\n",
    "            end = st + inp.shape[0]\n",
    "            dependency_matrix = dep_val[st:end,:,:]\n",
    "            \n",
    "            loss, metric = validate_step(model, inp, targ, dependency_matrix)\n",
    "\n",
    "            val_loss_sum += loss\n",
    "            val_metric_sum += metric\n",
    "\n",
    "        # 记录和收集1个epoch的训练（和验证）信息\n",
    "        # record = (epoch, loss_sum/step, metric_sum/step)\n",
    "        record = (epoch, loss_sum/step, metric_sum/step, val_loss_sum/val_step, val_metric_sum/val_step)\n",
    "        df_history.loc[epoch - 1] = record\n",
    "\n",
    "        # 打印epoch级别的日志\n",
    "        # print('*'*8, 'EPOCH = {} loss: {:.3f}, {}: {:.3f}'.format(\n",
    "        #        record[0], record[1], metric_name, record[2]))\n",
    "        print('EPOCH = {} loss: {:.5f}, {}: {:.5f}, val_loss: {:.5f}, val_{}: {:.5f}'.format(\n",
    "            record[0], record[1], metric_name, record[2], record[3], metric_name, record[4]))\n",
    "        printbar()\n",
    "        \n",
    "        # 监视loss的同时监视val_acc，防止loss下降而val_loss不下降\n",
    "        lr_scheduler2.step(record[4])\n",
    "        \n",
    "        # 保存模型\n",
    "        # current_acc_avg = metric_sum / step\n",
    "        current_acc_avg = val_metric_sum / val_step # 看验证集指标\n",
    "        \n",
    "        # 每隔几个epoch保存更好的模型，最后的几个epoch只要优于best_acc都保存\n",
    "        if (current_acc_avg > best_acc and epoch%6==0) or (current_acc_avg > best_acc and (epochs + 1 - epoch < 2)):\n",
    "            best_acc = current_acc_avg\n",
    "            checkpoint = train_model_save + '{:03d}_ckpt.tar'.format(epoch)\n",
    "            if device.type == 'cuda' and ngpu > 1:\n",
    "                # model_sd = model.module.state_dict()  ##################\n",
    "                model_sd = copy.deepcopy(model.module.state_dict())\n",
    "            else:\n",
    "                # model_sd = model.state_dict(),  ##################\n",
    "                model_sd = copy.deepcopy(model.state_dict())  ##################\n",
    "            torch.save({\n",
    "                'loss': loss_sum / step,\n",
    "                'epoch': epoch,\n",
    "                'net': model_sd,\n",
    "                'opt': optimizer.state_dict(),\n",
    "                'lr_scheduler': lr_scheduler.state_dict()\n",
    "            }, checkpoint)\n",
    "\n",
    "\n",
    "    print('finishing training...')\n",
    "    endtime = time.time()\n",
    "    time_elapsed = endtime - starttime\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    return df_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dep_train = torch.zeros(len(train_pairs), MAX_LENGTH+1, MAX_LENGTH+1)\n",
    "dep_val = torch.zeros(len(val_pairs), MAX_LENGTH+1, MAX_LENGTH+1)\n",
    "dep_test = torch.zeros(len(test_pairs), MAX_LENGTH+1, MAX_LENGTH+1)\n",
    "\n",
    "# for step, (inp, targ) in enumerate(test_dataloader, start=1):\n",
    "    \n",
    "#     st = (step-1) * inp.shape[0]\n",
    "#     end = st + inp.shape[0]\n",
    "#     dependency_matrix = dep_test[st:end,:,:]\n",
    "    \n",
    "#     if step%1 == 0:\n",
    "#         print(step)\n",
    "#         print(inp.shape)\n",
    "#         print(targ.shape)\n",
    "#         print(dependency_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import pickle\n",
    "# # import torch\n",
    "# with open(r'/home/chengkun/jupyter_projects/Magic-NLPer-main/data/data_sets/dep_train_pairs','rb') as f:\n",
    "#     dep_train = pickle.load(f)\n",
    "#     f.close()\n",
    "\n",
    "# with open(r'/home/chengkun/jupyter_projects/Magic-NLPer-main/data/data_sets/dep_val_pairs','rb') as f:\n",
    "#     dep_val = pickle.load(f)\n",
    "#     f.close()\n",
    "    \n",
    "# # dep_test = torch.zeros(len(test_pairs), MAX_LENGTH+1, MAX_LENGTH+1)\n",
    "\n",
    "# print(dep_train.shape)\n",
    "\n",
    "\n",
    "\n",
    "# with open(r'/home/chengkun/jupyter_projects/Magic-NLPer-main/data/data_sets/dep_val_pairs','rb') as f:\n",
    "#     count  = 0\n",
    "#     while True:\n",
    "#         try:\n",
    "#             aa=pickle.load(f)\n",
    "#             count = count + 1\n",
    "#             if count%240 == 0:\n",
    "#                 print(count)\n",
    "#         except EOFError:\n",
    "#             break\n",
    "# print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 开始训练\n",
    "df_history = train_model(transformer, EPOCHS, train_dataloader, val_dataloader, \n",
    "                         dep_train, dep_val, print_trainstep_every)\n",
    "\n",
    "# 保存df_history变量\n",
    "save_file = open(result_save + \"transformer_df_history.bin\", \"wb\")\n",
    "pickle.dump(df_history, save_file)\n",
    "save_file.close()\n",
    "print(df_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 绘制训练曲线\n",
    "def plot_metric(df_history, metric):\n",
    "    plt.figure()\n",
    "\n",
    "    train_metrics = df_history[metric]\n",
    "    val_metrics = df_history['val_' + metric]  #\n",
    "\n",
    "    epochs = range(1, len(train_metrics) + 1)\n",
    "\n",
    "    plt.plot(epochs, train_metrics, 'bo--')\n",
    "    plt.plot(epochs, val_metrics, 'ro-')  #\n",
    "\n",
    "    plt.title('Training and validation ' + metric)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([\"train_\" + metric, 'val_' + metric])\n",
    "    plt.savefig(result_save + metric + '.png')  # 保存图片\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "plot_metric(df_history, 'loss')\n",
    "plot_metric(df_history, metric_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 17.评估\n",
    "这里的评估 没有额外的使用测试集测评，还是拿验证集测试的。另外可以对一个法语句子进行翻译，看看翻译的结果如何\n",
    "\n",
    "以下步骤用于评估：\n",
    "- 用法语分词器（tokenizer_pt）编码输入语句。此外，添加<start>和<end>标记，这样输入就与模型训练的内容相同。这是编码器输入。\n",
    "- 解码器输入为 <start> token id\n",
    "- 计算padding mask 和 look ahead mask\n",
    "- 解码器通过查看编码器输出和它自身的输出（自注意力）给出预测。\n",
    "- 选择最后一个词并计算它的 argmax。\n",
    "- 将预测的词concat到解码器输入，然后传递给解码器。\n",
    "- 在这种方法中，解码器根据它之前预测的words预测下一个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint: /home/chengkun/jupyter_projects/Magic-NLPer-main/best_save/2021-12-15_17:27:45/035_ckpt.tar\n",
      "Loading model ...\n",
      "Model loaded ...\n"
     ]
    }
   ],
   "source": [
    "# 加载model\n",
    "a=os.listdir(train_model_save)\n",
    "a.sort()\n",
    "print(a[-1])\n",
    "checkpoint = train_model_save + a[-1]\n",
    "# checkpoint = \"/home/chengkun/jupyter_projects/Magic-NLPer-main/best_save/2021-12-15_17:27:45/035_ckpt.tar\"\n",
    "print('checkpoint:', checkpoint)\n",
    "\n",
    "# ckpt = torch.load(checkpoint, map_location=device)  # dict  save 在 CPU 加载到GPU\n",
    "ckpt = torch.load(checkpoint)  # dict  save 在 GPU 加载到 GPU\n",
    "# print('ckpt', ckpt)\n",
    "transformer_sd = ckpt['net']\n",
    "# optimizer_sd = ckpt['opt'] # 不重新训练的话不需要\n",
    "# lr_scheduler_sd = ckpt['lr_scheduler']\n",
    "\n",
    "reload_model = mn_Transformer(num_layers,\n",
    "                           d_model,\n",
    "                           num_heads,\n",
    "                           dff,\n",
    "                           input_vocab_size,\n",
    "                           target_vocab_size,\n",
    "                           pe_input=input_vocab_size,\n",
    "                           pe_target=target_vocab_size,\n",
    "                           seq_len=MAX_LENGTH+2,\n",
    "                           rate=dropout_rate)\n",
    "\n",
    "reload_model = reload_model.to(device)\n",
    "# reload_model.load_state_dict(transformer_sd)\n",
    "\n",
    "if ngpu > 1:\n",
    "    reload_model = torch.nn.DataParallel(reload_model,  device_ids=list(range(ngpu))) # 设置并行执行  device_ids=[0,1]\n",
    "\n",
    "\n",
    "print('Loading model ...')\n",
    "if device.type == 'cuda' and ngpu > 1:\n",
    "   reload_model.module.load_state_dict(transformer_sd)\n",
    "else:\n",
    "   reload_model.load_state_dict(transformer_sd)\n",
    "print('Model loaded ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** final test...\n",
      "******** Test: loss: 1.44124, test_acc: 0.70940\n"
     ]
    }
   ],
   "source": [
    "def test(model, dataloader, dep):\n",
    "    # model.eval() # 设置为eval mode\n",
    "\n",
    "    test_loss_sum = 0.\n",
    "    test_metric_sum = 0.\n",
    "    for test_step, (inp, targ) in enumerate(dataloader, start=1):\n",
    "        # inp [64, 10] , targ [64, 10]\n",
    "        \n",
    "        st = (test_step-1) * inp.shape[0]\n",
    "        end = st + inp.shape[0]\n",
    "        dependency_matrix = dep[st:end,:,:]\n",
    "        \n",
    "#         print('inp.shape',inp.shape)\n",
    "#         print('targ.shape',targ.shape)\n",
    "#         print('dependency_matrix.shape',dependency_matrix.shape)\n",
    "        \n",
    "        loss, metric = validate_step(model, inp, targ, dependency_matrix)\n",
    "        # print('*'*8, loss, metric)\n",
    "\n",
    "        test_loss_sum += loss\n",
    "        test_metric_sum += metric\n",
    "    # 打印\n",
    "    print('*' * 8,\n",
    "          'Test: loss: {:.5f}, {}: {:.5f}'.format(test_loss_sum / test_step, 'test_acc', test_metric_sum / test_step))\n",
    "\n",
    "\n",
    "# 在测试集上测试指标，这里使用val_dataloader模拟测试集\n",
    "print('*' * 8, 'final test...')\n",
    "test(reload_model, test_dataloader, dep_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 916, 32, 5, 838, 262, 170, 242, 2675, 39, 30, 32, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "<end> ᠶ᠋ᠢᠨ ᠦ᠋ᠳ ᠤᠯᠤᠰ ᠶᠡᠬᠡ ᠪᠠᠨ ᠦ᠌ ᠬᠠᠤᠯᠢ <start> .\n",
      "<end> 的 局 业 限 司 是 上 <start> .\n"
     ]
    }
   ],
   "source": [
    "def tokenizer_encode(tokenize, sentence, vocab):\n",
    "    # print(type(vocab)) # torchtext.vocab.Vocab\n",
    "    # print(len(vocab))\n",
    "    sentence = normalizeString(sentence)\n",
    "    # print(type(sentence)) # str\n",
    "    sentence = tokenize(sentence)  # list\n",
    "#     print(sentence)\n",
    "    sentence = ['<start>'] + sentence + ['<end>']\n",
    "    if len(sentence) < MAX_LENGTH + 2:\n",
    "        sentence = sentence + (MAX_LENGTH + 2 - len(sentence)) * ['<pad>']\n",
    "#     print(sentence)\n",
    "    sentence_ids = [vocab.stoi[token] for token in sentence]\n",
    "    # print(sentence_ids, type(sentence_ids[0])) # int\n",
    "    return sentence_ids\n",
    "\n",
    "\n",
    "def tokenzier_decode(sentence_ids, vocab):\n",
    "    sentence = [vocab.itos[id] for id in sentence_ids if id<len(vocab)]\n",
    "    # print(sentence)\n",
    "    return \" \".join(sentence)\n",
    "\n",
    "# 只有一个句子，不需要加pad\n",
    "s = 'ᠲᠦᠩᠯᠢᠶᠣᠣ ᠬᠣᠲᠠ ᠶ᠋ᠢᠨ ᡁᠢ ᠶᠤᠸᠠᠨ ᠠᠱᠢᠨ ᠲᠡᠷᠭᠡ ᠲᠦᠷᠢᠶᠡᠰᠦᠯᠡᠭᠦᠯᠬᠦ ᠬᠢᠵᠠᠭᠠᠷᠲᠤ ᠺᠣᠮᠫᠠᠨᠢ ᠬᠣᠲᠠ'\n",
    "# print(len(s.split()))\n",
    "print(tokenizer_encode(tokenizer, s, SRC_TEXT.vocab))\n",
    "\n",
    "\n",
    "s_ids = [3, 5, 251, 17, 46, 35, 12, 36, 4, 2]\n",
    "print(tokenzier_decode(s_ids, SRC_TEXT.vocab))\n",
    "print(tokenzier_decode(s_ids, TARG_TEXT.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real target: 内 蒙 古 盛 和 汽 车 服 务 有 限 公 司\n",
      "pred_sentence: <start> 内 蒙 古 盛 和 汽 车 服 务 有 限 公 司\n"
     ]
    }
   ],
   "source": [
    "# inp_sentence 一个法语句子，例如\"je pars en vacances pour quelques jours .\"\n",
    "def evaluate(model, inp_sentence, dependency_matrix):\n",
    "    model.eval()  # 设置eval mode\n",
    "\n",
    "    inp_sentence_ids = tokenizer_encode(tokenizer, inp_sentence, SRC_TEXT.vocab)  # 转化为索引\n",
    "#     print(tokenzier_decode(inp_sentence_ids, SRC_TEXT.vocab))\n",
    "    encoder_input = torch.tensor(inp_sentence_ids).unsqueeze(dim=0)  # =>[b=1, inp_seq_len=10]\n",
    "#     print('encoder_input.shape：', encoder_input.shape)\n",
    "\n",
    "    decoder_input = [TARG_TEXT.vocab.stoi['<start>']]\n",
    "#     print(decoder_input)\n",
    "    decoder_input = torch.tensor(decoder_input).unsqueeze(0)  # =>[b=1,seq_len=1]\n",
    "#     print('decoder_input.shape：', decoder_input.shape)\n",
    "#     print(MAX_LENGTH)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(MAX_LENGTH + 2):\n",
    "            enc_padding_mask, combined_mask, dec_padding_mask = create_mask(encoder_input.cpu(), decoder_input.cpu())\n",
    "            # [b,1,1,inp_seq_len], [b,1,targ_seq_len,inp_seq_len], [b,1,1,inp_seq_len]\n",
    "\n",
    "            encoder_input = encoder_input.to(device)\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            enc_padding_mask = enc_padding_mask.to(device)\n",
    "            combined_mask = combined_mask.to(device)\n",
    "            dec_padding_mask = dec_padding_mask.to(device)\n",
    "            dependency_matrix = dependency_matrix.to(device)\n",
    "\n",
    "            # forward\n",
    "            \n",
    "#             print('encoder_input.shape',encoder_input.shape)\n",
    "#             print('decoder_input.shape',decoder_input.shape)\n",
    "#             print('dependency_matrix.shape',dependency_matrix.shape)\n",
    "            \n",
    "            predictions, attention_weights = model(encoder_input,\n",
    "                                                   decoder_input,\n",
    "                                                   dependency_matrix,\n",
    "                                                   enc_padding_mask,\n",
    "                                                   combined_mask,\n",
    "                                                   dec_padding_mask)\n",
    "            # [b=1, targ_seq_len, target_vocab_size]\n",
    "            # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "            #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "\n",
    "            # 看最后一个词并计算它的 argmax\n",
    "            prediction = predictions[:, -1:, :]  # =>[b=1, 1, target_vocab_size]\n",
    "            prediction_id = torch.argmax(prediction, dim=-1)  # => [b=1, 1]\n",
    "            # print('prediction_id:', prediction_id, prediction_id.dtype) # torch.int64\n",
    "            if prediction_id.squeeze().item() == TARG_TEXT.vocab.stoi['<end>']:\n",
    "                return decoder_input.squeeze(dim=0), attention_weights\n",
    "\n",
    "            decoder_input = torch.cat([decoder_input, prediction_id],\n",
    "                                      dim=-1)  # [b=1,targ_seq_len=1]=>[b=1,targ_seq_len=2]\n",
    "            # decoder_input在逐渐变长\n",
    "\n",
    "    return decoder_input.squeeze(dim=0), attention_weights\n",
    "    # [targ_seq_len],\n",
    "    # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "    #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "\n",
    "\n",
    "\n",
    "# s = 'je pars en vacances pour quelques jours .'\n",
    "# evaluate(s)\n",
    "\n",
    "\n",
    "# s = 'I want to know now .'\n",
    "# s_targ = 'Je veux le savoir maintenant.'\n",
    "# 翻译时不提供外部知识，因为目标句子不参与翻译，因此dependency_matrix设置为0\n",
    "s = 'ᠥᠪᠥᠷ ᠣᠩᠭᠣᠯ ᠤ᠋ᠨ ᠱᠸᠩ ᠾᠧ ᠠᠱᠢᠨ ᠲᠡᠷᠭᠡᠨ ᠦ᠌ ᠦᠢᠯᠡᠴᠢᠯᠡᠭᠡ ᠶ᠋ᠢᠨ ᠬᠢᠵᠠᠭᠠᠷᠲᠤ ᠺᠣᠮᠫᠠᠨᠢ'\n",
    "s_targ = '内 蒙 古 盛 和 汽 车 服 务 有 限 公 司'\n",
    "dep = torch.zeros(1)\n",
    "\n",
    "pred_result, attention_weights = evaluate(reload_model, s, dep)\n",
    "pred_sentence = tokenzier_decode(pred_result, TARG_TEXT.vocab)\n",
    "print('real target:', s_targ)\n",
    "print('pred_sentence:', pred_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-17 11:14:35\n",
      "测试集句子数目： 9779\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "5000.0\n",
      "6000.0\n",
      "7000.0\n",
      "8000.0\n",
      "9000.0\n",
      "2021-12-17 15:14:06\n"
     ]
    }
   ],
   "source": [
    "# 批量翻译\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "dep = torch.zeros(1)\n",
    "sentence_pairs = test_pairs\n",
    "print('测试集句子数目：', len(sentence_pairs))\n",
    "\n",
    "# total_score = 0\n",
    "number = 0.0\n",
    "f1 = open(result_save + 'target1.txt','w', encoding='utf-8')\n",
    "f2 = open(result_save + 'pred1.txt','w', encoding='utf-8')\n",
    "for pair in sentence_pairs:\n",
    "    pred_result, _ = evaluate(reload_model, pair[0],dep)\n",
    "    pred_sentence = tokenzier_decode(pred_result, TARG_TEXT.vocab).replace('<start>','').replace('<end>','').replace('\\n','')\n",
    "\n",
    "    f1.write(pair[1] + '\\n')\n",
    "    f2.write(pred_sentence + '\\n')\n",
    "\n",
    "    number = number + 1\n",
    "    if number%1000 ==0:\n",
    "        print(number)\n",
    "\n",
    "f1.close()\n",
    "f2.close()\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ᠱᠢᠯᠢ ᠶᠢᠨ ᠭᠣᠣᠯ ᠠᠢᠮᠠᠭ ᠤ᠋ᠨ ᠷᠦᠩ ᠳᠡ ᠹᠠᠩ ᠹᠠ ᠬᠢᠵᠠᠭᠠᠷᠲᠤ ᠬᠠᠷᠢᠭᠤᠴᠠᠯᠭᠠᠲᠤ ᠺᠣᠮᠫᠠᠨᠢ\n",
      "target: 锡 林 郭 勒 盟 荣 达 房 发 有 限 责 任 公 司\n",
      "pred: <start> 锡 林 郭 勒 盟 荣 达 房 发 有 限 责 任 公 司\n",
      "\n",
      "input: ᠡᠦᠲᠦ ᠦᠶ᠎ᠡ ᠶ᠋ᠢᠨ ᠡᠨᠧᠷᠭᠢ ᠶ᠋ᠢᠨ ᠠᠵᠤ ᠠᠬᠤᠢ ᠬᠡᠮᠡᠬᠦ ᠡᠨᠡᠬᠦ ᠬᠠᠭᠤᠳᠠᠰᠤ ᠵᠣᠬᠢᠶᠠᠯ ᠢ᠋ ᠰᠠᠢᠲᠤᠷ ᠵᠣᠬᠢᠶᠠᠵᠤ ᠣᠯᠠᠨ ᠡᠱᠢᠲᠦ ᠬᠥᠭᠵᠢᠯᠲᠡ ᠣᠯᠠᠨ ᠲᠤᠢᠯᠱᠢᠷᠠᠯ ᠤ᠋ᠨ ᠲᠤᠯᠭᠠᠭᠤᠷᠢᠯᠠᠯ ᠲᠠᠢ ᠣᠳᠣ ᠦᠶ᠎ᠡ ᠶ᠋ᠢᠨ ᠠᠵᠤ ᠦᠢᠯᠡᠰ ᠤ᠋ᠨ ᠱᠢᠨ᠎ᠡ ᠲᠣᠭᠲᠠᠯᠴᠠᠭ᠎ᠠ ᠶ᠋ᠢ ᠴᠢᠷᠮᠠᠢᠨ ᠴᠣᠭᠴᠠᠯᠠᠶ᠎ᠠ\n",
      "target: 做 好 现 代 能 源 经 济 这 篇 文 章 . 努 力 构 建 多 元 发 展 . 多 极 支 撑 的 现 代 产 业 新 体 系\n",
      "pred: <start> 创 新 现 代 能 源 经 济 这 篇 文 章 . 努 力 构 建 多 元 发 展 的 多 极 支 撑 现 代 产 业 新 体 系\n",
      "\n",
      "input: ᠲᠡᠷᠡ ᠤᠳᠠᠲᠠᠯ᠎ᠠ ᠬᠠᠷᠠᠭᠳᠠᠭᠰᠠᠨ ᠦᠭᠡᠢ ᠪᠣᠯᠬᠣᠷ ᠪᠢ ᠶᠡᠬᠡᠳᠡ ᠰᠠᠨᠠᠭ᠎ᠠ ᠵᠣᠪᠠᠵᠤ ᠪᠠᠢᠨ᠎ᠠ .\n",
      "target: 她 好 久 不 露 面 叫 我 担 心 死 了 .\n",
      "pred: <start> 他 很 久 没 看 . 我 很 担 心 .\n",
      "\n",
      "input: ᠪᠠᠲᠤᠯ ᠴᠢᠩᠰᠠᠩ . ᠲᠣᠭᠣᠭᠠᠨ ᠲᠠᠢᠱᠢ . ᠡᠰᠡᠨ ᠲᠠᠢᠱᠢ ᠶ᠋ᠢᠨ ᠦᠶ᠎ᠡ ᠳ᠋ᠦ᠍\n",
      "target: 巴 图 拉 丞 相 . 托 欢 太 师 . 也 先 太 师 时 期 .\n",
      "pred: <start> 巴 图 拉 丞 相 . 托 欢 太 师 . 也 先 太 师 时 期 .\n",
      "\n",
      "input: ᠹᠢᠯᠢᠮ ᠢ᠋ᠶ᠋ᠡᠨ ᠣᠷᠢᠶᠠᠵᠤ ᠠᠪᠳᠠᠭ ᠨᠢ ᠡᠢᠮᠦ ᠵᠢᠷᠤᠭ ᠤ᠋ᠨ ᠠᠱᠢᠨ ᠶ᠋ᠢᠨ ᠪᠠᠰᠠ ᠨᠢᠭᠡ ᠣᠨᠴᠠᠯᠢᠭ .\n",
      "target: 这 种 相 机 的 另 一 特 点 是 自 动 卷 片 .\n",
      "pred: <start> 接 着 卷 片 是 这 种 相 机 的 另 一 种 特 征 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 随机选个句子翻译\n",
    "dep = torch.zeros(1)\n",
    "def evaluateRandomly(n=5):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('input:', pair[0])\n",
    "        print('target:', pair[1])\n",
    "        pred_result, attentions = evaluate(reload_model, pair[0],dep)\n",
    "        pred_sentence = tokenzier_decode(pred_result, TARG_TEXT.vocab)\n",
    "        print('pred:', pred_sentence)\n",
    "        print('')\n",
    "\n",
    "\n",
    "evaluateRandomly(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 18.attention 的可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 可视化attenton 这里我们只展示...block2的attention，即[b, num_heads, targ_seq_len, inp_seq_len]\n",
    "# attention: {'decoder_layer{i + 1}_block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "#             'decoder_layer{i + 1}_block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "# sentence: [seq_len]，例如：'je recherche un assistant .'\n",
    "# pred_result: [seq_len]，例如：'<start> i m looking for an assistant .'\n",
    "# layer: 字符串类型，表示模型decoder的N层decoder-layer的第几层的attention，形如'decoder_layer{i}_block1'或'decoder_layer{i}_block2'\n",
    "def plot_attention_weights(attention, sentence, pred_sentence, layer):\n",
    "\n",
    "    # block2 attention[layer] => [b=1, num_heads, targ_seq_len, inp_seq_len]\n",
    "    attention = torch.squeeze(attention[layer], dim=0) # => [num_heads, targ_seq_len, inp_seq_len]\n",
    "    print('attention.shape',attention.shape)\n",
    "\n",
    "    # print(matplotlib.matplotlib_fname())\n",
    "    plt.rcParams['font.sans-serif'] = ['Mongolian Baiti']\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    \n",
    "    sentence = sentence.split()\n",
    "    \n",
    "#     print('sentence.shape',np.array(sentence).shape[0])\n",
    "    pred_sentence = pred_sentence.split()\n",
    "#     print('pred_sentence.shape',np.array(pred_sentence).shape[0])\n",
    "    \n",
    "    \n",
    "#     attention = attention[:,:len(pred_sentence),:len(sentence)+2]\n",
    "    \n",
    "    fig = plt.figure(figsize=(len(pred_sentence), len(sentence)+2)) #figsize=(attention.shape[1], attention.shape[2])\n",
    "    \n",
    "    for head in range(attention.shape[0]):\n",
    "        ax = fig.add_subplot(2, 4, head + 1)  # 111是单个整数编码的子绘图网格参数。例如，“111”表示“1×1网格，第一子图”，“234”表示“2×3网格，第四子图”\n",
    "\n",
    "        cax = ax.matshow(attention[head].cpu(), cmap='viridis')  # 绘制网格热图？注意力权重\n",
    "        # fig.colorbar(cax)#给子图添加colorbar（颜色条或渐变色条）\n",
    "\n",
    "        fontdict = {'fontsize': 10}\n",
    "\n",
    "        # 设置轴刻度线\n",
    "        ax.set_xticks(range(len(sentence)+2))  # 算上start和end\n",
    "        ax.set_yticks(range(len(pred_sentence)))\n",
    "        \n",
    "        ax.set_xlim(-0.5,len(sentence) + 0.5)  # 设定x座标轴的范围\n",
    "        ax.set_ylim(len(pred_sentence) - 1.5, -0.5)  # 设定y座标轴的范围\n",
    "\n",
    "        # 设置轴\n",
    "        ax.set_xticklabels(['<start>']+sentence+['<end>'], fontdict=fontdict, rotation=90)  # 顺时间旋转90度\n",
    "        ax.set_yticklabels(pred_sentence, fontdict=fontdict, family = 'SimHei')\n",
    "\n",
    "        ax.set_xlabel('Head {}'.format(head + 1))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(result_save+'attention_{}.pdf'.format(layer),format='pdf',bbox_inches='tight')\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "def translate(sentence_pair, dep, plot=None):\n",
    "    print('input:', sentence_pair[0])\n",
    "    print('target:', sentence_pair[1])\n",
    "    pred_result, attention_weights = evaluate(reload_model, sentence_pair[0],dep)\n",
    "    print('attention_weights:', attention_weights.keys())\n",
    "    pred_sentence = tokenzier_decode(pred_result, TARG_TEXT.vocab)\n",
    "    print('pred:', pred_sentence)\n",
    "    print('')\n",
    "\n",
    "    if plot:\n",
    "        plot_attention_weights(attention_weights, sentence_pair[0], pred_sentence, plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# dep = torch.zeros(1)\n",
    "# translate(test_pairs[481], dep, 'decoder_layer5_block2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 45.97, 72.1/52.5/41.2/33.9 (BP=0.959, ratio=0.959, hyp_len=224671, ref_len=234185)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = os.popen('/home/chengkun/moses/mosesdecoder/scripts/generic/multi-bleu.perl -lc /home/chengkun/jupyter_projects/Magic-NLPer-main/best_save/{}/target1.txt < /home/chengkun/jupyter_projects/Magic-NLPer-main/best_save/{}/pred1.txt'.format(begin_time,begin_time))\n",
    "print(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.966\n"
     ]
    }
   ],
   "source": [
    "target = []\n",
    "with open('/home/chengkun/jupyter_projects/Magic-NLPer-main/best_save/{}/target1.txt'.format(begin_time),'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        target.append(line.strip('\\n'))#去掉列表中每一个元素的换行符\n",
    "target = [target]\n",
    "f.close()\n",
    "\n",
    "pred = []\n",
    "with open('/home/chengkun/jupyter_projects/Magic-NLPer-main/best_save/{}/pred1.txt'.format(begin_time),'r', encoding='utf-8') as f1:\n",
    "    for line in f1.readlines():\n",
    "        pred.append(line.strip('\\n'))#去掉列表中每一个元素的换行符\n",
    "\n",
    "f1.close()\n",
    "\n",
    "\n",
    "bleu = sacrebleu.corpus_bleu(pred, target,smooth_method='none')\n",
    "bleu_score = format(bleu.score,'.3f')\n",
    "print(bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os.system('cp  {} {}'.format(checkpoint,result_save))\n",
    "os.system('cp  {} {}'.format('/home/chengkun/jupyter_projects/Magic-NLPer-main/re/primer-ez/pytask-primer.log',result_save))\n",
    "os.system('cp  {} {}'.format('/home/chengkun/jupyter_projects/Magic-NLPer-main/re/primer-ez/transformer_improved_encoder_decoder_zero1.log',result_save))\n",
    "os.system('cp  {} {}'.format('/home/chengkun/jupyter_projects/Magic-NLPer-main/train_save/transformer_improved_encoder_decoder_zero/*',result_save))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ᠪᠠᠶᠠᠨᠨᠠᠭᠤᠷ ᠬᠣᠲᠠ ᠶ᠋ᠢᠨ ᠲᠠᠷᠢᠶᠠᠯᠠᠩ ᠠᠯ ᠠᠵᠤ ᠠᠬᠤᠢ ᠶ᠋ᠢᠨ ᠡᠳᠡᠭᠡ ᠵᠠᠩᠭᠢ ᠶᠢᠨ ᠲᠥᠪ\n",
      "target: 巴 彦 淖 尔 市 农 牧 业 信 息 中 心\n",
      "attention_weights: dict_keys(['decoder_layer1_block1', 'decoder_layer1_block2', 'decoder_layer2_block1', 'decoder_layer2_block2', 'decoder_layer3_block1', 'decoder_layer3_block2', 'decoder_layer4_block1', 'decoder_layer4_block2', 'decoder_layer5_block1', 'decoder_layer5_block2'])\n",
      "pred: <start> 巴 彦 淖 尔 市 农 牧 业 信 息 中 心\n",
      "\n",
      "attention.shape torch.Size([8, 13, 102])\n",
      "input: ᠪᠠᠶᠠᠨᠨᠠᠭᠤᠷ ᠬᠣᠲᠠ ᠶ᠋ᠢᠨ ᠲᠠᠷᠢᠶᠠᠯᠠᠩ ᠠᠯ ᠠᠵᠤ ᠠᠬᠤᠢ ᠶ᠋ᠢᠨ ᠡᠳᠡᠭᠡ ᠵᠠᠩᠭᠢ ᠶᠢᠨ ᠲᠥᠪ\n",
      "target: 巴 彦 淖 尔 市 农 牧 业 信 息 中 心\n",
      "attention_weights: dict_keys(['decoder_layer1_block1', 'decoder_layer1_block2', 'decoder_layer2_block1', 'decoder_layer2_block2', 'decoder_layer3_block1', 'decoder_layer3_block2', 'decoder_layer4_block1', 'decoder_layer4_block2', 'decoder_layer5_block1', 'decoder_layer5_block2'])\n",
      "pred: <start> 巴 彦 淖 尔 市 农 牧 业 信 息 中 心\n",
      "\n",
      "attention.shape torch.Size([8, 13, 13])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x1008 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x1008 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dep = torch.zeros(1)\n",
    "translate(test_pairs[481], dep, 'decoder_layer5_block2')\n",
    "translate(test_pairs[481], dep, 'decoder_layer5_block1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.rename(result_save[:-1], result_save[:-1]+'_'+str(bleu_score)+'_'+str(seed)+'_'+file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 19.总结\n",
    "\n",
    "代码还有以下待完善的地方：\n",
    "- 没有实现标签平滑\n",
    "- 在训练过程中使用了teacher-forcing，即总是会将target传递到下一时间步长。更好的做法是设置一个teacher_forcing_ration\n",
    "- 在evaluate阶段的解码使用的是greedy search decode，即对于每一步，我们只需从具有最高 softmax 值的 decoder_output 中选择单词。可以尝试使用更好的beam search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
