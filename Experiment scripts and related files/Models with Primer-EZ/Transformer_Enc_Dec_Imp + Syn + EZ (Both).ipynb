{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "transformer的核心：self-attentions\n",
    "\n",
    "transformer **优点**：\n",
    "- 无需对跨数据的时间/空间域的关系作出假设\n",
    "- 并行计算\n",
    "- distant item 互相影响彼此的输出\n",
    "- 可以学习到长距离依赖关系\n",
    "\n",
    "transformer **缺点**：\n",
    "- 对于每一个step的xt的输出，是由整个历史信息计算得出，而不再是当前输入和hidden，这可能效率较低\n",
    "- 如果输入具有时间/空间域的关系，则需要加入位置编码，否则整个model也只能看作是一个词袋模型\n",
    "\n",
    "目录\n",
    "* [1.加载数据 建立input pipeline](#)\n",
    "* [2.位置编码 positional encoding](#)\n",
    "* [3.掩码 masking](#3)\n",
    "* [4.scaled dot product attention](#)\n",
    "* [5.multi-head attention](#)\n",
    "* [6.point wise feed forward network](#)\n",
    "* [7.encoder layer](#)\n",
    "* [8.decoder layer](#)\n",
    "* [9.encoder](#)\n",
    "* [10.decoder](#)\n",
    "* [11.搭建transformer](#)\n",
    "* [12.设置超参](#)\n",
    "* [13.优化器](#)\n",
    "* [14.损失和评价准则](#)\n",
    "* [15.生成mask](#)\n",
    "* [16.训练和保存](#)\n",
    "* [17.评估](#)\n",
    "* [18.attention的可视化](#)\n",
    "* [19.总结](#)\n",
    "\n",
    "\n",
    "## 1.加载数据 建立input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer_improved_encoder_decoder_dep1\n",
      "2021-12-21_13:38:34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import random\n",
    "import re\n",
    "# from tqdm import tqdm  # 进度条\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import unicodedata\n",
    "import datetime\n",
    "import time\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import sacrebleu\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import pickle\n",
    "import torch_optimizer as optim\n",
    "# import adamod\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "from labml_helpers.module import Module\n",
    "from labml_nn.transformers import MultiHeadAttention\n",
    "\n",
    "# print(os.getcwd()) # /home/xijian\n",
    "file_name = 'transformer_improved_encoder_decoder_dep1'\n",
    "print(file_name)\n",
    "\n",
    "train_model_save = '/home/chengkun/jupyter_projects/Magic-NLPer-main/train_save/transformer_improved_encoder_decoder_dep/'\n",
    "shutil.rmtree(train_model_save)\n",
    "os.mkdir(train_model_save)\n",
    "\n",
    "#保存必要结果\n",
    "begin_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()).replace(' ','_')\n",
    "print(begin_time)\n",
    "result_save = '/home/chengkun/jupyter_projects/Magic-NLPer-main/best_save/' + begin_time +'/'\n",
    "os.mkdir(result_save)\n",
    "\n",
    "os.system('cp  {} {} {}'.format('/home/chengkun/jupyter_projects/Magic-NLPer-main/re/primer-ez/transformer_improved_encoder_decoder_dep1.py','/home/chengkun/jupyter_projects/Magic-NLPer-main/re/primer-ez/transformer_improved_encoder_decoder_dep1.ipynb',result_save))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquaredReLU(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.relu = torch.nn.LeakyReLU()\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.relu(x)\n",
    "        return x * x\n",
    "\n",
    "class SpatialDepthWiseConvolution(Module):\n",
    "    def __init__(self, d_k: int, kernel_size: int = 3):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = torch.nn.Conv1d(in_channels=d_k, out_channels=d_k,\n",
    "                              kernel_size=(kernel_size,), padding=(kernel_size - 1,), groups=d_k)\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x.unsqueeze(-1).permute(1, 0, 3, 2)\n",
    "        seq_len, batch_size, heads, d_k= x.shape\n",
    "        x = x.permute(1, 2, 3, 0)\n",
    "        x = x.view(batch_size * heads, d_k, seq_len)\n",
    "        x = self.conv(x)\n",
    "        x = x[:, :, :-(self.kernel_size - 1)]\n",
    "        x = x.view(batch_size, heads, d_k, seq_len)\n",
    "        x = x.permute(0, 3, 2, 1) # [batch_size, seq_len, heads, d_k]\n",
    "        x = x.view(batch_size, seq_len, heads, d_k)\n",
    "        x = torch.squeeze(x,2)\n",
    "        return x\n",
    "\n",
    "# class MultiDConvHeadAttention(MultiHeadAttention):\n",
    "#     def __init__(self, heads: int, d_model: int, dropout_prob: float = 0.1):\n",
    "#         super().__init__(heads, d_model, dropout_prob)\n",
    "        \n",
    "#         self.query = torch.nn.Sequential(self.query, SpatialDepthWiseConvolution(self.d_k))\n",
    "#         self.key = torch.nn.Sequential(self.key, SpatialDepthWiseConvolution(self.d_k))\n",
    "#         self.value = torch.nn.Sequential(self.value, SpatialDepthWiseConvolution(self.d_k))\n",
    "        \n",
    "# m = SquaredReLU()\n",
    "# n = torch.nn.LeakyReLU()\n",
    "# input = torch.randn(2)\n",
    "# print(m(input))\n",
    "# print(n(input)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu0: 0 gpu1: 3\n"
     ]
    }
   ],
   "source": [
    "gpu0 = int(sys.argv[1])\n",
    "gpu1 = int(sys.argv[2])\n",
    "seed = int(sys.argv[3])\n",
    "\n",
    "# gpu0 = 0\n",
    "# gpu1 = 3\n",
    "# seed = 6421\n",
    "print('gpu0:',gpu0,'gpu1:',gpu1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngpu： 2\n",
      "batch： 120\n",
      "MAX_LENGTH： 100\n",
      "EPOCHS： 35\n",
      "warm_steps： 3000\n",
      "seed： 6421\n"
     ]
    }
   ],
   "source": [
    "# 设置超参数\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"{},{}\".format(gpu0, gpu1)\n",
    "ngpu = 2\n",
    "print('ngpu：', ngpu)\n",
    "\n",
    "batch = 120\n",
    "print('batch：', batch)\n",
    "\n",
    "# MAX_LENGTH = d_model//num_heads\n",
    "MAX_LENGTH = 100\n",
    "print('MAX_LENGTH：', MAX_LENGTH)\n",
    "EPOCHS = 35\n",
    "print('EPOCHS：', EPOCHS)\n",
    "warm_steps=3000\n",
    "print('warm_steps：', warm_steps)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()  #检测是否有可用的gpu\n",
    "device = torch.device(\"cuda:0\" if (use_cuda and ngpu > 0) else \"cpu\")\n",
    "\n",
    "# seed = 256\n",
    "print('seed：', seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "# torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 数据读取\n",
    "# 当你用read_csv读文件的时候，如果文本里包含英文双引号，直接读取会导致行数变少或是直接如下报错停止\n",
    "# 此时应该对read_csv设置参数控制csv中的引号常量，设定quoting=3或是quoting=csv.QUOTE_NONE”（注：用第二种要先导csv库）然后问题就解决了。\n",
    "\n",
    "data_dir = '/home/chengkun/jupyter_projects/Magic-NLPer-main/data/'\n",
    "\n",
    "data_df = pd.read_csv(data_dir + 'ch_mn_50_nodict.txt',  # 数据格式：英语\\t法语，注意我们的任务源语言是法语，目标语言是英语\n",
    "                      encoding='UTF-8', sep='\\t', header=None,quoting=3,\n",
    "                      names=['mn', 'ch'], index_col=False)\n",
    "\n",
    "# print(data_df.shape)\n",
    "# print(data_df.values.shape)\n",
    "# print(data_df.values[0])\n",
    "# print(data_df.values[0].shape)\n",
    "# data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "\n",
    "# 规范化字符串\n",
    "def normalizeString(s):\n",
    "    # print(s) # list  ['Go.']\n",
    "    # s = s[0]\n",
    "    s = s.lower().strip()\n",
    "    #s = unicodeToAscii(s)\n",
    "    #s = re.sub(r\"([.!?])\", r\" \\1\", s)  # \\1表示group(1)即第一个匹配到的 即匹配到'.'或者'!'或者'?'后，一律替换成'空格.'或者'空格!'或者'空格？'\n",
    "    #s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)  # 非字母以及非.!?的其他任何字符 一律被替换成空格\n",
    "    s = re.sub(r'[\\s]+', \" \", s)  # 将出现的多个空格，都使用一个空格代替。例如：w='abc  1   23  1' 处理后：w='abc 1 23 1'\n",
    "    return s\n",
    "\n",
    "\n",
    "# print(normalizeString('Va !'))\n",
    "# print(normalizeString('Go.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairs num= 500000\n"
     ]
    }
   ],
   "source": [
    "pairs = [[normalizeString(s) for s in line] for line in data_df.values]\n",
    "\n",
    "print('pairs num=', len(pairs))\n",
    "# print(pairs[0])\n",
    "# print(pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "经过过滤后平行语料数目为： 498921\n"
     ]
    }
   ],
   "source": [
    "# 文件是英译法，我们实现的是法译英，所以进行了reverse，所以pair[1]是英语\n",
    "# 为了快速训练，仅保留“我是”“你是”“他是”等简单句子，并且删除原始文本长度大于10个标记的样本\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH \n",
    "\n",
    "def filterPairs(pairs):\n",
    "    # 过滤，并交换句子顺序，得到法英句子对（之前是英法句子对）\n",
    "    return [[pair[1], pair[0]] for pair in pairs if filterPair(pair)]\n",
    "\n",
    "\n",
    "pairs = filterPairs(pairs)\n",
    "\n",
    "print('经过过滤后平行语料数目为：', len(pairs))\n",
    "# print(pairs[0])\n",
    "# print(random.choice(pairs))\n",
    "# print(np.array(pairs).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集句子数目： 479163\n",
      "验证集句子数目： 9979\n",
      "测试集句子数目： 9779\n"
     ]
    }
   ],
   "source": [
    "# 划分数据集：训练集和验证集\n",
    "##0.0338 0.03485\n",
    "##50 0.020 0.020\n",
    "train_test, val_pairs = train_test_split(pairs, test_size=0.020, random_state=1234)\n",
    "train_pairs, test_pairs = train_test_split(train_test, test_size=0.020, random_state=1234)\n",
    "\n",
    "print('训练集句子数目：', len(train_pairs))\n",
    "print('验证集句子数目：', len(val_pairs))\n",
    "print('测试集句子数目：', len(test_pairs))\n",
    "# print(test_pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(r'/home/chengkun/jupyter_projects/Magic-NLPer-main/data/data_sets/train_pairs','wb') as f:\n",
    "#     pickle.dump(train_pairs, f)\n",
    "#     f.close()\n",
    "\n",
    "# with open(r'/home/chengkun/jupyter_projects/Magic-NLPer-main/data/data_sets/val_pairs','wb') as f:\n",
    "#     pickle.dump(val_pairs, f)\n",
    "#     f.close()\n",
    "\n",
    "# with open(r'/home/chengkun/jupyter_projects/Magic-NLPer-main/data/data_sets/test_pairs','wb') as f:\n",
    "#     pickle.dump(test_pairs, f)\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集句子数目： 479163\n",
      "验证集句子数目： 9979\n",
      "测试集句子数目： 9779\n"
     ]
    }
   ],
   "source": [
    "# with open(r'/home/chengkun/jupyter_projects/Magic-NLPer-main/data/data_sets/train_pairs','rb') as f:\n",
    "#     train_pairs = pickle.load(f)\n",
    "#     f.close()\n",
    "\n",
    "# with open(r'/home/chengkun/jupyter_projects/Magic-NLPer-main/data/data_sets/val_pairs','rb') as f:\n",
    "#     val_pairs = pickle.load(f)\n",
    "#     f.close()\n",
    "\n",
    "# with open(r'/home/chengkun/jupyter_projects/Magic-NLPer-main/data/data_sets/test_pairs','rb') as f:\n",
    "#     test_pairs = pickle.load(f)\n",
    "#     f.close()\n",
    "\n",
    "# print('训练集句子数目：', len(train_pairs))\n",
    "# print('验证集句子数目：', len(val_pairs))\n",
    "# print('测试集句子数目：', len(test_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = lambda x: x.split() # 分词器\n",
    "\n",
    "SRC_TEXT = torchtext.data.Field(sequential=True,\n",
    "                                tokenize=tokenizer,\n",
    "                                # lower=True,\n",
    "                                fix_length=MAX_LENGTH + 2,\n",
    "                                preprocessing=lambda x: ['<start>'] + x + ['<end>'],\n",
    "                                # after tokenizing but before numericalizing\n",
    "                                # postprocessing # after numericalizing but before the numbers are turned into a Tensor\n",
    "                                )\n",
    "TARG_TEXT = torchtext.data.Field(sequential=True,\n",
    "                                 tokenize=tokenizer,\n",
    "                                 # lower=True,\n",
    "                                 fix_length=MAX_LENGTH + 2,\n",
    "                                 preprocessing=lambda x: ['<start>'] + x + ['<end>'],\n",
    "                                 )\n",
    "\n",
    "\n",
    "def get_dataset(pairs, src, targ):\n",
    "    fields = [('src', src), ('targ', targ)]  # filed信息 fields dict[str, Field])\n",
    "    examples = []  # list(Example)\n",
    "    for mn, ch in pairs: # 进度条\n",
    "        # 创建Example时会调用field.preprocess方法\n",
    "        examples.append(torchtext.data.Example.fromlist([mn, ch], fields))\n",
    "    return examples, fields\n",
    "\n",
    "\n",
    "# examples, fields = get_dataset(pairs, SRC_TEXT, TARG_TEXT)\n",
    "\n",
    "ds_train = torchtext.data.Dataset(*get_dataset(train_pairs, SRC_TEXT, TARG_TEXT))\n",
    "ds_val = torchtext.data.Dataset(*get_dataset(val_pairs, SRC_TEXT, TARG_TEXT))\n",
    "ds_test = torchtext.data.Dataset(*get_dataset(test_pairs, SRC_TEXT, TARG_TEXT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds_train\n",
      "20 ['<start>', 'ᠴᠢᠯᠠᠭᠤᠨ', 'ᠲᠣᠰᠣ', 'ᠭᠠᠳᠠᠭᠱᠢᠯᠠᠭᠤᠯᠬᠤ', 'ᠤᠯᠤᠰ', 'ᠤ᠋ᠨ', 'ᠵᠣᠬᠢᠶᠠᠨ', 'ᠪᠠᠢᠭᠤᠯᠤᠯᠲᠠ', 'ᠴᠢᠯᠠᠭᠤᠨ', 'ᠲᠣᠰᠣ', 'ᠭᠠᠳᠠᠭᠱᠢᠯᠠᠭᠤᠯᠬᠤ', 'ᠪᠠᠨ', 'ᠵᠣᠭᠰᠣᠭᠠᠨ', '.', 'ᠦᠨ\\u180eᠡ', 'ᠥᠰᠬᠦ', 'ᠶ᠋ᠢ', 'ᠬᠦᠯᠢᠶᠡᠨ\\u180eᠡ', '.', '<end>']\n",
      "22 ['<start>', '石', '油', '输', '出', '国', '家', '组', '织', '冻', '结', '石', '油', '输', '出', '.', '价', '格', '看', '涨', '.', '<end>']\n",
      "ds_val\n",
      "25 ['<start>', 'ᠴᠠᠭᠠᠨ', 'ᠰᠠᠷ\\u180eᠠ', 'ᠶ᠋ᠢᠨ', 'ᠦᠶ\\u180eᠡ', 'ᠪᠡᠷ', 'ᠬᠥᠮᠦᠰ', 'ᠬᠠᠭᠤᠴᠢᠨ', 'ᠶᠣᠰᠣ', 'ᠵᠠᠩᠱᠢᠯ', 'ᠢ᠋ᠶ᠋ᠠᠨ', 'ᠪᠠᠷᠢᠮᠲᠠᠯᠠᠵᠤ', 'ᠪᠠᠢᠭ\\u180eᠠ', 'ᠪᠥᠭᠡᠳ', 'ᠤᠯᠠᠮ', 'ᠢ᠋ᠶ᠋ᠠᠷ', 'ᠱᠢᠨ\\u180eᠡ', 'ᠵᠦᠢᠯ', 'ᠢ᠋', 'ᠡᠷᠢᠨ', 'ᠬᠠᠢᠬᠤ', 'ᠪᠣᠯᠵᠤ', 'ᠪᠠᠢᠨ\\u180eᠠ', '.', '<end>']\n",
      "35 ['<start>', '人', '们', '在', '固', '守', '原', '有', '过', '年', '消', '费', '方', '式', '的', '同', '时', '.', '也', '在', '欣', '赏', '.', '渴', '望', '着', '浪', '漫', '的', '时', '尚', '情', '趣', '.', '<end>']\n",
      "ds_test\n",
      "13 ['<start>', 'ᠵᠠᠷᠯᠠᠨ', 'ᠨᠡᠢᠲᠡᠯᠡᠬᠦ', 'ᠬᠤᠭᠤᠴᠠᠭ\\u180eᠠ', 'ᠨᠢ', 'ᠬᠣᠷᠢᠨ', 'ᠡᠳᠦᠷ', 'ᠡᠴᠡ', 'ᠳᠤᠲᠠᠭᠤ', 'ᠪᠠᠢᠵᠤ', 'ᠪᠣᠯᠬᠤ', 'ᠦᠭᠡᠢ', '<end>']\n",
      "13 ['<start>', '公', '示', '时', '间', '不', '得', '少', '于', '二', '十', '日', '<end>']\n"
     ]
    }
   ],
   "source": [
    "# # 查看1个样本的信息\n",
    "print('ds_train')\n",
    "print(len(ds_train[0].src), ds_train[0].src)\n",
    "print(len(ds_train[0].targ), ds_train[0].targ)\n",
    "print('ds_val')\n",
    "print(len(ds_val[0].src), ds_val[0].src)\n",
    "print(len(ds_val[0].targ), ds_val[0].targ)\n",
    "print('ds_test')\n",
    "print(len(ds_test[0].src), ds_test[0].src)\n",
    "print(len(ds_test[0].targ), ds_test[0].targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型大小与词表大小正相关，控制词表大小\n",
      "0-20：<unk> <pad> . <end> <start> ᠶ᠋ᠢᠨ ᠤ᠋ᠨ ᠤ᠋ ᠦ᠋ᠨ ᠶ᠋ᠢ ᠢ᠋ ᠳ᠋ᠤ᠌ ᠦ᠌ ᠨᠢ ᠦᠭᠡᠢ ᠳ᠋ᠦ᠍ ᠠ᠋ᠴᠠ ᠤᠯᠤᠰ ᠬᠡᠷᠡᠭᠲᠡᠢ ᠭᠠᠵᠠᠷ\n",
      "\n",
      "input_vocab_size： 110560\n",
      "target_vocab_size： 6085\n"
     ]
    }
   ],
   "source": [
    "# 构建词典\n",
    "print('模型大小与词表大小正相关，控制词表大小')\n",
    "SRC_TEXT.build_vocab(ds_train,min_freq=1)  # 建立词表 并建立token和ID的映射关系\n",
    "# print(len(SRC_TEXT.vocab))\n",
    "# print(SRC_TEXT.vocab.itos[0])\n",
    "# print(SRC_TEXT.vocab.itos[1])\n",
    "# print(SRC_TEXT.vocab.itos[2])\n",
    "# print(SRC_TEXT.vocab.itos[3])\n",
    "# print(SRC_TEXT.vocab.stoi['<start>'])\n",
    "# print(SRC_TEXT.vocab.stoi['<end>'])\n",
    "\n",
    "# 模拟decode\n",
    "res = []\n",
    "for id in range(20):\n",
    "    res.append(SRC_TEXT.vocab.itos[id])\n",
    "print('0-20：'+' '.join(res)+'\\n')\n",
    "\n",
    "TARG_TEXT.build_vocab(ds_train,min_freq=1)\n",
    "\n",
    "# print(len(TARG_TEXT.vocab))\n",
    "# print(TARG_TEXT.vocab.itos[0])\n",
    "# print(TARG_TEXT.vocab.itos[1])\n",
    "# print(TARG_TEXT.vocab.itos[2])\n",
    "# print(TARG_TEXT.vocab.itos[3])\n",
    "# print(TARG_TEXT.vocab.stoi['<start>'])\n",
    "# print(TARG_TEXT.vocab.stoi['<end>'])\n",
    "\n",
    "input_vocab_size = len(SRC_TEXT.vocab)\n",
    "target_vocab_size = len(TARG_TEXT.vocab)\n",
    "\n",
    "print('input_vocab_size：', input_vocab_size)\n",
    "print('target_vocab_size：', target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = batch * ngpu\n",
    "\n",
    "# 构建数据管道迭代器\n",
    "train_iter, val_iter, test_iter= torchtext.data.Iterator.splits(\n",
    "    (ds_train, ds_val, ds_test),\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x.src),\n",
    "    batch_sizes=(BATCH_SIZE, BATCH_SIZE, BATCH_SIZE)\n",
    ")\n",
    "\n",
    "\n",
    "# # 查看数据管道信息，此时会触发postprocessing，如果有的话\n",
    "# for BATCH in train_iter:\n",
    "#     # 注意，这里text第0维不是batch，而是seq_len\n",
    "#     print(BATCH.src.shape, BATCH.targ.shape)  # [12,64], [12,64]\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 将数据管道组织成与torch.utils.data.DataLoader相似的inputs, targets的输出形式\n",
    "class DataLoader:\n",
    "    def __init__(self, data_iter):\n",
    "        self.data_iter = data_iter\n",
    "        self.length = len(data_iter)  # 一共有多少个batch？\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __iter__(self):\n",
    "        # 注意，在此处调整text的shape为batch first\n",
    "        for batch in self.data_iter:\n",
    "            yield (torch.transpose(batch.src, 0, 1), torch.transpose(batch.targ, 0, 1))\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_iter)\n",
    "val_dataloader = DataLoader(val_iter)\n",
    "test_dataloader = DataLoader(test_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_dataloader): 1997\n",
      "len(val_dataloader): 42\n",
      "len(test_dataloader): 41\n"
     ]
    }
   ],
   "source": [
    "# 查看数据管道\n",
    "print('len(train_dataloader):', len(train_dataloader))  # 句子总数/batch数\n",
    "print('len(val_dataloader):', len(val_dataloader))  # 句子总数/batch数\n",
    "print('len(test_dataloader):', len(test_dataloader))  # 句子总数/batch数\n",
    "\n",
    "# for batch_src, batch_targ in train_dataloader:\n",
    "#     print('batch_src.shape:',batch_src.shape,'\\n','batch_targ.shape:',batch_targ.shape)  # [256,12], [256,12]\n",
    "#     print(batch_src, batch_src.dtype)\n",
    "#     print(batch_targ, batch_targ.dtype)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dep_train = torch.randn(len(train_pairs),MAX_LENGTH+2,MAX_LENGTH+2)\n",
    "# dep_val = torch.randn(len(val_pairs),MAX_LENGTH+2,MAX_LENGTH+2)\n",
    "# dep_test = torch.randn(len(test_pairs),MAX_LENGTH+2,MAX_LENGTH+2)\n",
    "\n",
    "# for step, (inp, targ) in enumerate(train_dataloader, start=1):\n",
    "#     print(step)\n",
    "#     st = (step-1) * inp.shape[0]\n",
    "#     end = st + inp.shape[0]\n",
    "#     dependency_matrix = dep_train[st:end,:,:]\n",
    "#     print(inp.shape)\n",
    "#     print(targ.shape)\n",
    "#     print(dependency_matrix.shape)\n",
    "    \n",
    "# for step, (inp, targ) in enumerate(test_dataloader, start=1):\n",
    "#     print(step)\n",
    "#     st = (step-1) * inp.shape[0]\n",
    "#     end = st + inp.shape[0]\n",
    "#     dependency_matrix = dep_test[st:end,:,:]\n",
    "#     print(inp.shape)\n",
    "#     print(targ.shape)\n",
    "#     print(dependency_matrix.shape)\n",
    "\n",
    "# for step, (inp, targ) in enumerate(val_dataloader, start=1):\n",
    "#     print(step)\n",
    "#     st = (step-1) * inp.shape[0]\n",
    "#     end = st + inp.shape[0]\n",
    "#     dependency_matrix = dep_val[st:end,:,:]\n",
    "#     print(inp.shape)\n",
    "#     print(targ.shape)\n",
    "#     print(dependency_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.位置编码 positional encoding\n",
    "绝对位置编码\n",
    "\n",
    "由于model中不含有任何recurrence or convolution，所以句子中token的相对位置关系无法体现，\n",
    "所以就需要在embedding vector中加入position encoding vector（维度相同）。这样每个词的词向量\n",
    "在 $d_{model}$ 维的空间中，就可以基于meaning和position来计算相似度或相关性\n",
    "\n",
    "$$\\begin{array}{ll} & PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}}) \\\\ & PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})\\end{array}$$\n",
    "\n",
    "**特点：**\n",
    "- （1）后面位置是前面位置的线性组合，保证了即使位置不是相邻的，也可能有关系（[参考这里](#https://blog.csdn.net/zhulinniao/article/details/104462228/)）\n",
    "- （2）每个位置的编码又是独特的\n",
    "- （3）每两个位置的encoding互相做点积，位置越远，点积的值越小，自己和自己点积，值最大\n",
    "\n",
    "![jupyter-img1](./imgs/im1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 计算角度：pos * 1/(10000^(2i/d))\n",
    "def get_angles(pos, i, d_model):\n",
    "    # 2*(i//2)保证了2i，这部分计算的是1/10000^(2i/d)\n",
    "    angle_rates = 1 / np.power(10000, 2 * (i // 2) / np.float32(d_model))  # => [1, 512]\n",
    "    return pos * angle_rates  # [50,1]*[1,512]=>[50, 512]\n",
    "\n",
    "\n",
    "# np.arange()函数返回一个有终点和起点的固定步长的排列，如[1,2,3,4,5]，起点是1，终点是5，步长为1\n",
    "# 注意：起点终点是左开右闭区间，即start=1,end=6，才会产生[1,2,3,4,5]\n",
    "# 只有一个参数时，参数值为终点，起点取默认值0，步长取默认值1。\n",
    "def positional_encoding(position, d_model):  # d_model是位置编码的长度，相当于position encoding的embedding_dim？\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],  # [50, 1]\n",
    "                            np.arange(d_model)[np.newaxis, :],  # [1, d_model=512]\n",
    "                            d_model)\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])  # 2i\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])  # 2i+2\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]  # [50,512]=>[1,50,512]\n",
    "    return torch.tensor(pos_encoding, dtype=torch.float32)\n",
    "\n",
    "pos_encoding = positional_encoding(50, 512)\n",
    "# print(pos_encoding.shape) # [1,50,512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def draw_pos_encoding(pos_encoding):\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(pos_encoding[0], cmap='RdBu') # 绘制分类图\n",
    "    plt.xlabel('Depth')\n",
    "    plt.xlim((0, 512))\n",
    "    plt.ylabel('Position')\n",
    "    plt.colorbar() # 条形bar颜色图例\n",
    "    plt.savefig(result_save+'pos_encoding.png')\n",
    "    #plt.show()\n",
    "\n",
    "draw_pos_encoding(pos_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.掩码 masking\n",
    "这里用到的mask有2种：\n",
    "- padding mask：mask pad，即句子中为pad(value=0)的位置处其mask值为1\n",
    "- look-ahead mask：mask future token，将当前token后面的词mask掉，只让看到前面的词，即future token位置的mask值为1\n",
    "\n",
    "**【注意】：** 因为我这里使用的是torchtext里的tokenizer,从前面可以看出它的词表里pad的index=1，而不是常规的0。\n",
    "这里要特别注意，不然容易出错！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "mask 表示掩码，它对某些值进行掩盖，使其在参数更新时不产生效果。\n",
    "Transformer 模型里面涉及两种 mask，分别是 padding mask 和 sequence mask。\n",
    "其中，padding mask 在所有的 scaled dot-product attention 里面都需要用到，\n",
    "而 sequence mask 只有在 decoder 的 self-attention 里面用到。\n",
    "'''\n",
    "# 需要mask的位置，替换为1，seq：sentence_len x d_model \n",
    "#返回值为：sentence_len x 1 x 1 x d_model 为什么要扩展维度\n",
    "\n",
    "pad = 1 # 重要！\n",
    "def create_padding_mask(seq):  # seq [b, seq_len]\n",
    "    # seq = torch.eq(seq, torch.tensor(0)).float() # pad=0的情况\n",
    "    seq = torch.eq(seq, torch.tensor(pad)).float()  # pad!=0\n",
    "    return seq[:, np.newaxis, np.newaxis, :]  # =>[b, 1, 1, seq_len]\n",
    "\n",
    "# x = torch.tensor([[7, 6, 0, 0, 1],\n",
    "#                   [1, 2, 3, 0, 0],\n",
    "#                   [0, 0, 0, 4, 5]])\n",
    "# print(x.shape) # [3,5]\n",
    "# print(x)\n",
    "# mask = create_padding_mask(x)\n",
    "# print(mask.shape, mask.dtype) # [3,1,1,5]\n",
    "# print(mask)\n",
    "\n",
    "# 用train_dataloader的第一个BATCH_SIZE中的input来测试\n",
    "# for step, (inp, targ) in enumerate(train_dataloader, start=1):\n",
    "#     print(inp.shape,'\\t',targ.shape)\n",
    "#     print(inp,'\\n',targ)\n",
    "#     break\n",
    "\n",
    "# input_mask = create_padding_mask(inp)\n",
    "# targ_mask = create_padding_mask(targ)\n",
    "# print(input_mask,input_mask.shape)\n",
    "# print(targ_mask,targ_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = []\n",
    "# b = []\n",
    "# for step, (inp, targ) in enumerate(test_dataloader):\n",
    "#     a.append(inp.shape[1])\n",
    "#     b.append(targ.shape[1])\n",
    "# #     print(inp,'\\n',inp.shape,targ.shape)\n",
    "# #     break\n",
    "# print(a)\n",
    "# print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pad = 1 # 重要！\n",
    "# def mn_create_padding_mask(seq):  # seq [b, seq_len]\n",
    "#     if seq.shape[1] < 10:\n",
    "#         d = torch.ones([seq.shape[0],10-seq.shape[1]])\n",
    "#         seq = torch.cat((seq, d), dim=1)\n",
    "#     print(seq)\n",
    "#     # seq = torch.eq(seq, torch.tensor(0)).float() # pad=0的情况\n",
    "#     seq = torch.eq(seq, torch.tensor(pad)).float()  # pad!=0\n",
    "#     return seq[:, np.newaxis, np.newaxis, :]  # =>[b, 1, 1, seq_len]\n",
    "\n",
    "# x = torch.tensor([[7, 6, 0, 0, 1],\n",
    "#                   [1, 2, 3, 0, 0],\n",
    "#                   [0, 0, 0, 4, 5]])\n",
    "# print(x.shape) # [3,5]\n",
    "# print(x)\n",
    "# mask = mn_create_padding_mask(x)\n",
    "# print(mask.shape, mask.dtype) # [3,1,1,5]\n",
    "# print(mask)\n",
    "\n",
    "# # create two sample vectors\n",
    "# # inps = torch.randn([64, 161])\n",
    "# d = torch.ones([3,4])\n",
    "\n",
    "# # bring d into the same format, and then concatenate tensors\n",
    "# # new_inps = torch.cat((x, d.unsqueeze(2)), dim=-1)\n",
    "# new_inps = torch.cat((x,d),dim=1)\n",
    "# print(new_inps.shape)  # [64, 161, 2]\n",
    "# print(new_inps)\n",
    "# print(x.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# torch.triu(tensor, diagonal=0) 求上三角矩阵，diagonal默认为0表示主对角线的上三角矩阵\n",
    "# diagonal>0，则主对角上面的第|diagonal|条次对角线的上三角矩阵\n",
    "# diagonal<0，则主对角下面的第|diagonal|条次对角线的上三角矩阵\n",
    "#look-ahead_mask 用于对未预测的token进行掩码，这意味着要预测第三个单词，只会使用第一个和第二个单词。 \n",
    "\n",
    "def create_look_ahead_mask(size):  # seq_len\n",
    "    mask = torch.triu(torch.ones((size, size)), diagonal=1)\n",
    "    # mask = mask.device() #\n",
    "    return mask  # [seq_len, seq_len]\n",
    "\n",
    "# x = torch.rand(1,3)\n",
    "# print(x.shape)\n",
    "# print(x)\n",
    "# mask = create_look_ahead_mask(x.shape[1])\n",
    "# print(mask.shape, mask.dtype)\n",
    "# print(mask)\n",
    "\n",
    "# 用train_dataloader的第一个BATCH_SIZE中的target来测试\n",
    "# for step, (inp, targ) in enumerate(train_dataloader, start=1):\n",
    "#     print(targ,'\\n',inp.shape,targ.shape)\n",
    "#     break\n",
    "\n",
    "# look_ahead_mask = create_look_ahead_mask(targ.shape[-1])\n",
    "# print(look_ahead_mask,look_ahead_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4.scaled dot product attention\n",
    "![jupyter-img2](./imgs/im2.jpg)\n",
    "\n",
    "$$Attention(Q,K,V)=softmax_{(k)}(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "注意：实现时对mask的处理\n",
    "\n",
    "mask=1的位置是pad或者future token，乘以-1e9（-1*10^9）成为负无穷，经过softmax后会趋于0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    \"\"\"\n",
    "    #计算注意力权重。\n",
    "    q, k, v 必须具有匹配的前置维度。 且dq=dk\n",
    "    k, v 必须有匹配的倒数第二个维度，例如：seq_len_k = seq_len_v。\n",
    "    #虽然 mask 根据其类型（填充或前瞻）有不同的形状，\n",
    "    #但是 mask 必须能进行广播转换以便求和。\n",
    "\n",
    "    #参数:\n",
    "        q: 请求的形状 == (..., seq_len_q, depth)\n",
    "        k: 主键的形状 == (..., seq_len_k, depth)\n",
    "        v: 数值的形状 == (..., seq_len_v, depth_v)  seq_len_k = seq_len_v\n",
    "        mask: Float 张量，其形状能转换成\n",
    "              (..., seq_len_q, seq_len_k)。默认为None。\n",
    "    \n",
    "    # self-attention中q=k=v这点和attention不同，需要先明确\n",
    "    #q和k相似度计算是为了获取到最合适的值，也就是值的给与注意力的值\n",
    "    #softmax是为了获取这一系列相似度值的占比（这也就是所谓的权重值）\n",
    "    #加权是和v也就是本身进行加权，求和是为了获取粒度单词和完整句子之间的关系值计算\n",
    "    #那么自然而然的，所谓的qkv这一系列操作的目的就是\n",
    "    #先通过对本身的各个向量值进行相似度计算，然后通过softmax获取本身向量的权重值，在和本身进行加权计算，\n",
    "    #最后在求和，这样子就获取了一个词和本身所有的词的权重值，然后将所有词的权重值作为输入向量，这也就是所谓的自注意力机制。\n",
    "\n",
    "    #返回值:\n",
    "        #输出，注意力权重\n",
    "    \"\"\"\n",
    "    # matmul(a,b)矩阵乘:a b的最后2个维度要能做乘法，即a的最后一个维度值==b的倒数第2个纬度值，\n",
    "    # 除此之外，其他维度值必须相等或为1（为1时会广播）\n",
    "    matmul_qk = torch.matmul(q, k.transpose(-1, -2))  # 矩阵乘 =>[..., seq_len_q, seq_len_k]\n",
    "\n",
    "    # 缩放matmul_qk\n",
    "    dk = torch.tensor(k.shape[-1], dtype=torch.float32)  # k的深度dk，或叫做depth_k\n",
    "    scaled_attention_logits = matmul_qk / torch.sqrt(dk)  # [..., seq_len_q, seq_len_k]\n",
    "\n",
    "    # 将 mask 加入到缩放的张量上(重要！)\n",
    "    if mask is not None:  # mask: [b, 1, 1, seq_len]\n",
    "        # mask=1的位置是pad，乘以-1e9（-1*10^9）成为负无穷，经过softmax后会趋于0\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax 在最后一个轴（seq_len_k）上归一化\n",
    "    attention_weights = torch.nn.functional.softmax(scaled_attention_logits, dim=-1)  # [..., seq_len_q, seq_len_k]\n",
    "\n",
    "    output = torch.matmul(attention_weights, v)  # =>[..., seq_len_q, depth_v]\n",
    "    return output, attention_weights  # [..., seq_len_q, depth_v], [..., seq_len_q, seq_len_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mn_scaled_dot_product_attention(q, k, v, m, mask=None):\n",
    "    \"\"\"\n",
    "    #计算注意力权重。\n",
    "    q, k, v 必须具有匹配的前置维度。 且dq=dk\n",
    "    k, v 必须有匹配的倒数第二个维度，例如：seq_len_k = seq_len_v。\n",
    "    #虽然 mask 根据其类型（填充或前瞻）有不同的形状，\n",
    "    #但是 mask 必须能进行广播转换以便求和。\n",
    "\n",
    "    #参数:\n",
    "        q: 请求的形状 == (..., seq_len_q, depth)\n",
    "        k: 主键的形状 == (..., seq_len_k, depth)\n",
    "        v: 数值的形状 == (..., seq_len_v, depth_v)  seq_len_k = seq_len_v\n",
    "        m: 数值的形状 == (..., seq_len_k, seq_len_v) m必须是个方阵\n",
    "        mask: Float 张量，其形状能转换成\n",
    "              (..., seq_len_q, seq_len_k)。默认为None。\n",
    "\n",
    "    #返回值:\n",
    "        #输出，注意力权重\n",
    "    \"\"\"\n",
    "    # matmul(a,b)矩阵乘:a b的最后2个维度要能做乘法，即a的最后一个维度值==b的倒数第2个纬度值，\n",
    "    # 除此之外，其他维度值必须相等或为1（为1时会广播）\n",
    "    matmul_qk = torch.matmul(q, k.transpose(-1, -2))  # 矩阵乘 =>[..., seq_len_q, seq_len_k]\n",
    "\n",
    "    # 缩放matmul_qk\n",
    "    dk = torch.tensor(k.shape[-1], dtype=torch.float32)  # k的深度dk，或叫做depth_k\n",
    "    scaled_attention_logits = matmul_qk / torch.sqrt(dk) # [..., seq_len_q, seq_len_k]\n",
    "#     print('scaled_attention_logits.shape',scaled_attention_logits.shape)\n",
    "#     print('m.shape',m.shape)\n",
    "#     print('q*k^t：', scaled_attention_logits.shape)\n",
    "    \n",
    "#     scaled_attention_logits = torch.matmul(scaled_attention_logits,m)\n",
    "    scaled_attention_logits = scaled_attention_logits * m\n",
    "#     print('q*k^t*m：', scaled_attention_logits.shape)\n",
    "    \n",
    "    # 将 mask 加入到缩放的张量上(重要！)\n",
    "    if mask is not None:  # mask: [b, 1, 1, seq_len]\n",
    "        # mask=1的位置是pad，乘以-1e9（-1*10^9）成为负无穷，经过softmax后会趋于0\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax 在最后一个轴（seq_len_k）上归一化\n",
    "    attention_weights = torch.nn.functional.softmax(scaled_attention_logits, dim=-1)  # [..., seq_len_q, seq_len_k]\n",
    "    \n",
    "    output = torch.matmul(attention_weights, v)  # =>[..., seq_len_q, depth_v]\n",
    "#     print('output：', output.shape)\n",
    "    return output, attention_weights  # [..., seq_len_q, depth_v], [..., seq_len_q, seq_len_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dependency_parsing_scaled_dot_product_attention(q, k, v, dependency_parsing_matrix, mask=None):\n",
    "    \"\"\"\n",
    "    #计算注意力权重。\n",
    "    q, k, v 必须具有匹配的前置维度。 且dq=dk\n",
    "    k, v 必须有匹配的倒数第二个维度，例如：seq_len_k = seq_len_v。\n",
    "    #虽然 mask 根据其类型（填充或前瞻）有不同的形状，\n",
    "    #但是 mask 必须能进行广播转换以便求和。\n",
    "\n",
    "    #参数:\n",
    "        q: 请求的形状 == (..., seq_len_q, depth)\n",
    "        k: 主键的形状 == (..., seq_len_k, depth)\n",
    "        v: 数值的形状 == (..., seq_len_v, depth_v)  seq_len_k = seq_len_v\n",
    "        mask: Float 张量，其形状能转换成\n",
    "              (..., seq_len_q, seq_len_k)。默认为None。\n",
    "    \n",
    "    # self-attention中q=k=v这点和attention不同，需要先明确\n",
    "    #q和k相似度计算是为了获取到最合适的值，也就是值的给与注意力的值\n",
    "    #softmax是为了获取这一系列相似度值的占比（这也就是所谓的权重值）\n",
    "    #加权是和v也就是本身进行加权，求和是为了获取粒度单词和完整句子之间的关系值计算\n",
    "    #那么自然而然的，所谓的qkv这一系列操作的目的就是\n",
    "    #先通过对本身的各个向量值进行相似度计算，然后通过softmax获取本身向量的权重值，在和本身进行加权计算，\n",
    "    #最后在求和，这样子就获取了一个词和本身所有的词的权重值，然后将所有词的权重值作为输入向量，这也就是所谓的自注意力机制。\n",
    "\n",
    "    #返回值:\n",
    "        #输出，注意力权重\n",
    "    \"\"\"\n",
    "    # matmul(a,b)矩阵乘:a b的最后2个维度要能做乘法，即a的最后一个维度值==b的倒数第2个纬度值，\n",
    "    # 除此之外，其他维度值必须相等或为1（为1时会广播）\n",
    "    matmul_qk = torch.matmul(q, k.transpose(-1, -2))  # 矩阵乘 =>[..., seq_len_q, seq_len_k]\n",
    "#     print('matmul_qk:', matmul_qk.size())\n",
    "#     print('dependency_parsing_matrix:', dependency_parsing_matrix.size())\n",
    "    matmul_qk = matmul_qk + dependency_parsing_matrix\n",
    "\n",
    "    # 缩放matmul_qk\n",
    "    dk = torch.tensor(k.shape[-1], dtype=torch.float32)  # k的深度dk，或叫做depth_k\n",
    "    scaled_attention_logits = matmul_qk / torch.sqrt(dk)  # [..., seq_len_q, seq_len_k]\n",
    "#     print('scaled_attention_logits.shape:', scaled_attention_logits.shape)\n",
    "    # 将 mask 加入到缩放的张量上(重要！)\n",
    "    if mask is not None:  # mask: [b, 1, 1, seq_len]\n",
    "#         print('mask.shape', mask.shape)\n",
    "        mask = mask.reshape(-1,mask.shape[-1],mask.shape[-1])\n",
    "        # mask=1的位置是pad，乘以-1e9（-1*10^9）成为负无穷，经过softmax后会趋于0\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "    \n",
    "    # softmax 在最后一个轴（seq_len_k）上归一化\n",
    "    attention_weights = torch.nn.functional.softmax(scaled_attention_logits, dim=-1)  # [..., seq_len_q, seq_len_k]\n",
    "\n",
    "    output = torch.matmul(attention_weights, v)  # =>[..., seq_len_q, depth_v]\n",
    "#     print('output.shape',output.shape)\n",
    "    return output, attention_weights  # [..., seq_len_q, depth_v], [..., seq_len_q, seq_len_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原来的\n",
    "# class dependency_parsing_Attention(torch.nn.Module):\n",
    "#     def __init__(self, d_model):\n",
    "#         super(dependency_parsing_Attention, self).__init__()\n",
    "\n",
    "#         self.d_model = d_model\n",
    "\n",
    "#         self.wq = torch.nn.Linear(d_model, d_model)\n",
    "#         self.wk = torch.nn.Linear(d_model, d_model)\n",
    "#         self.wv = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "#         self.final_linear = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "#     def forward(self, q, k, v,dependency_parsing_matrix,mask):  # q=k=v=x [b, seq_len, embedding_dim] embedding_dim其实也=d_model\n",
    "#         batch_size = q.shape[0]\n",
    "\n",
    "#         q = self.wq(q)  # =>[b, seq_len, d_model]\n",
    "#         k = self.wk(k)  # =>[b, seq_len, d_model]\n",
    "#         v = self.wv(v)  # =>[b, seq_len, d_model]\n",
    "# #         print('q:',q.shape)\n",
    "# #         print('k:',q.shape)\n",
    "# #         print('v:',q.shape)\n",
    "\n",
    "#         scaled_attention, attention_weights = dependency_parsing_scaled_dot_product_attention(q, k, v, dependency_parsing_matrix, mask)\n",
    "#         # => [b, seq_len_q, d_model=512], [b, seq_len_q, seq_len_k]\n",
    "\n",
    "#         output = self.final_linear(scaled_attention)  # =>[b, seq_len_q, d_model=512]\n",
    "#         return output, attention_weights  # [b, seq_len_q, d_model=512], [b, seq_len_q, seq_len_k]\n",
    "\n",
    "\n",
    "# # x = torch.rand(2, 5, 6) # [b,seq_len,d_model,embedding_dim]\n",
    "# # dependency_parsing_matrix = torch.rand(2,5,5)\n",
    "\n",
    "# # temp_mha = dependency_parsing_Attention(d_model=6)\n",
    "\n",
    "# # out, attn_weights = temp_mha(x, x, x, dependency_parsing_matrix, mask=None)\n",
    "# # print(out.shape, attn_weights.shape) # [1, 60, 512], [1, 8, 60, 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加入premer-ez的\n",
    "class dependency_parsing_Attention(torch.nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(dependency_parsing_Attention, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.wq = torch.nn.Linear(d_model, d_model)\n",
    "        self.wk = torch.nn.Linear(d_model, d_model)\n",
    "        self.wv = torch.nn.Linear(d_model, d_model)\n",
    "        self.SDWC = SpatialDepthWiseConvolution(d_k=d_model,kernel_size=3)\n",
    "        #primer_ez\n",
    "        self.final_linear = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v,dependency_parsing_matrix,mask):  # q=k=v=x [b, seq_len, embedding_dim] embedding_dim其实也=d_model\n",
    "        batch_size = q.shape[0]\n",
    "        \n",
    "        q = self.wq(q)  # =>[b, seq_len, d_model]\n",
    "#         print('q',q.shape)\n",
    "        q = self.SDWC(q)\n",
    "        k = self.wk(k)  # =>[b, seq_len, d_model]\n",
    "        k = self.SDWC(k)\n",
    "        v = self.wv(v)  # =>[b, seq_len, d_model]\n",
    "        v = self.SDWC(v)\n",
    "#         print('q:',q.shape)\n",
    "#         print('k:',q.shape)\n",
    "#         print('v:',q.shape)\n",
    "\n",
    "\n",
    "        scaled_attention, attention_weights = dependency_parsing_scaled_dot_product_attention(q, k, v, dependency_parsing_matrix, mask)\n",
    "        # => [b, seq_len_q, d_model=512], [b, seq_len_q, seq_len_k]\n",
    "\n",
    "        output = self.final_linear(scaled_attention)  # =>[b, seq_len_q, d_model=512]\n",
    "        return output, attention_weights  # [b, seq_len_q, d_model=512], [b, seq_len_q, seq_len_k]\n",
    "\n",
    "# b = 120\n",
    "# s = 100\n",
    "# d = 256\n",
    "# x = torch.rand(b, s, d) # [b,seq_len,d_model]\n",
    "# dependency_parsing_matrix = torch.rand(b,s,s)\n",
    "# temp_mha = dependency_parsing_Attention(d_model=d)\n",
    "# out, attn_weights = temp_mha(x, x, x, dependency_parsing_matrix, mask=None)\n",
    "# print(out.shape, attn_weights.shape) # [1, 60, 512], [1, 8, 60, 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def print_out(q, k, v, m):\n",
    "#     temp_out, temp_attn = mn_scaled_dot_product_attention(q, k, v, m, None)\n",
    "#     print('Attention weights are:')\n",
    "#     print(temp_attn)\n",
    "#     print('Output is:')\n",
    "#     print(temp_out)\n",
    "\n",
    "# np.set_printoptions(suppress=True) # 设置不以科学计数法的形式显示数据\n",
    "\n",
    "# temp_k = torch.tensor([[10,0,0],\n",
    "#                        [0,10,0],\n",
    "#                        [0,0,10],\n",
    "#                        [0,0,10]], dtype=torch.float32) # [4,3]\n",
    "# temp_v = torch.tensor([[1,0],\n",
    "#                        [10,0],\n",
    "#                        [100,5],\n",
    "#                        [1000,6]], dtype=torch.float32) #[4,2]\n",
    "\n",
    "# temp_m = torch.tensor([[10,0,0,0],\n",
    "#                        [0,10,0,0],\n",
    "#                        [0,0,10,0],\n",
    "#                        [0,0,10,0]], dtype=torch.float32) # [4,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # query aligns with第2个key (key的第2列)，得到attention weights：[0. 1. 0. 0.]\n",
    "# # 所以第2行 的value值被返回\n",
    "# temp_q = torch.tensor([[0,10,0]], dtype=torch.float32) # [1,3]\n",
    "# print_out(temp_q, temp_k, temp_v,temp_m)\n",
    "# # Attention weights：[0. 1. 0. 0.]， Output：[[10.  0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # query aligns with 重复的key (key的第3列和第4列)，得到attention weights：[0. 0. 0.5 0.5]\n",
    "# # 所以第3行的value值与第4行的value值平均化后，被返回 【(100+1000)/2=550, (5+6)/2=5.5】\n",
    "# temp_q = torch.tensor([[0,0,10]], dtype=torch.float32) # [1,3]\n",
    "# print_out(temp_q, temp_k, temp_v,temp_m)\n",
    "# # Attention weights：[0. 0. 0.5 0.5]， Output：[[550.  5.5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # query aligns with 第1个key (key的第1列)和第2个key (key的第2列)，得到attention weights：[0.5 0.5 0. 0.]\n",
    "# # 所以第1行的value值与第2行的value值平均化后，被返回 【(1+10)/2=5.5, (0.+0.)/2=0.】\n",
    "# temp_q = torch.tensor([[10,10,0]], dtype=torch.float32) # [1,3]\n",
    "# print_out(temp_q, temp_k, temp_v,temp_m)\n",
    "# # Attention weights：[0.5 0.5 0. 0.]， Output：[[5.5.  0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # 传入所有的query\n",
    "# temp_q = torch.tensor([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=torch.float32)  # (3, 3)\n",
    "# print_out(temp_q, temp_k, temp_v,temp_m)\n",
    "# \"\"\"\n",
    "# # Attention weights：tensor(\n",
    "# [[0.  0.  0.5 0.5]\n",
    "#  [0.  1.  0.  0. ]\n",
    "#  [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)，\n",
    "# # Output：tensor(\n",
    "# [[550.    5.5]\n",
    "#  [ 10.    0. ]\n",
    "#  [  5.5   0. ]], shape=(3, 2), dtype=float32)\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # relative_positions\n",
    "# def relative_positions(seq_len):\n",
    "#     result = []\n",
    "#     for i in range(seq_len):\n",
    "#         front = list(range(-i, 0))\n",
    "#         end = list(range(seq_len - i))\n",
    "#         result.append(front + end)\n",
    "#     return result\n",
    "# relative_positions(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import math\n",
    "# # import torch\n",
    "# # from torch import nn\n",
    "\n",
    "# class RelativeGlobalAttention(torch.nn.Module):\n",
    "#     def __init__(self, d_model, num_heads, max_len=100, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.d_model = d_model\n",
    "#         self.num_heads = num_heads\n",
    "        \n",
    "#         assert d_model % self.num_heads == 0\n",
    "#         self.d_head = d_model // self.num_heads\n",
    "# #         d_head, remainder = divmod(d_model, num_heads)\n",
    "# #         if remainder:\n",
    "# #             raise ValueError(\n",
    "# #                 \"incompatible `d_model` and `num_heads`\"\n",
    "# #             )\n",
    "\n",
    "#         self.max_len = max_len\n",
    "\n",
    "#         self.key = torch.nn.Linear(d_model, d_model)\n",
    "#         self.value = torch.nn.Linear(d_model, d_model)\n",
    "#         self.query = torch.nn.Linear(d_model, d_model)\n",
    "#         self.dropout = torch.nn.Dropout(dropout)\n",
    "#         self.Er = torch.nn.Parameter(torch.randn(max_len, self.d_head))\n",
    "        \n",
    "#         self.register_buffer(\n",
    "#             \"mask\", \n",
    "#             torch.tril(torch.randn(max_len, max_len))\n",
    "#             .unsqueeze(0).unsqueeze(0)\n",
    "#         )\n",
    "# #         print('self.mask.shape：', self.mask.shape)\n",
    "#         # self.mask.shape = (1, 1, max_len, max_len)\n",
    "\n",
    "#     def skew(self, QEr):\n",
    "#         # QEr.shape = (batch_size, num_heads, seq_len, seq_len)\n",
    "# #         print('QEr.shape:',QEr.shape)\n",
    "#         padded = torch.nn.functional.pad(QEr, (1, 0), 'constant', 1)\n",
    "# #         print('padded.shape:',padded.shape)\n",
    "# #         print(padded)\n",
    "#         # padded.shape = (batch_size, num_heads, seq_len, 1 + seq_len)\n",
    "#         batch_size, num_heads, num_rows, num_cols = padded.shape\n",
    "#         reshaped = padded.reshape(batch_size, num_heads, num_cols, num_rows)\n",
    "#         # reshaped.size = (batch_size, num_heads, 1 + seq_len, seq_len)\n",
    "#         Srel = reshaped[:, :, 1:, :]\n",
    "#         # Srel.shape = (batch_size, num_heads, seq_len, seq_len)\n",
    "#         return Srel\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         # x.shape == (batch_size, seq_len, d_model)\n",
    "#         batch_size, seq_len, _ = x.shape\n",
    "#         if seq_len > self.max_len:\n",
    "#             raise ValueError(\n",
    "#                 \"sequence length exceeds model capacity\"\n",
    "#             )\n",
    "        \n",
    "#         k_t = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
    "#         # k_t.shape = (batch_size, num_heads, d_head, seq_len)\n",
    "#         v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
    "#         q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
    "#         # shape = (batch_size, num_heads, seq_len, d_head)\n",
    "        \n",
    "#         start = self.max_len - seq_len\n",
    "#         Er_t = self.Er[start:, :].transpose(0, 1)\n",
    "#         print(Er_t)\n",
    "#         # Er_t.shape = (d_head, seq_len)\n",
    "#         QEr = torch.matmul(q, Er_t)\n",
    "#         # QEr.shape = (batch_size, num_heads, seq_len, seq_len)\n",
    "#         Srel = self.skew(QEr)\n",
    "#         # Srel.shape = (batch_size, num_heads, seq_len, seq_len)\n",
    "        \n",
    "#         QK_t = torch.matmul(q, k_t)\n",
    "#         # QK_t.shape = (batch_size, num_heads, seq_len, seq_len)\n",
    "#         # （Q*K^T+Srel）\n",
    "#         print('QK_t.shape：',QK_t.shape)\n",
    "#         print('Srel.shape：',Srel.shape)\n",
    "#         attn = (QK_t + Srel) / math.sqrt(q.size(-1))\n",
    "#         mask = self.mask[:, :, :seq_len, :seq_len]\n",
    "#         # mask.shape = (1, 1, seq_len, seq_len)\n",
    "#         attn = attn.masked_fill(mask == 1, float(\"-inf\"))\n",
    "#         # attn.shape = (batch_size, num_heads, seq_len, seq_len)\n",
    "#         attn = torch.nn.functional.softmax(attn, dim=-1)\n",
    "#         out = torch.matmul(attn, v)\n",
    "#         # out.shape = (batch_size, num_heads, seq_len, d_head)\n",
    "#         out = out.transpose(1, 2)\n",
    "#         # out.shape == (batch_size, seq_len, num_heads, d_head)\n",
    "#         out = out.reshape(batch_size, seq_len, -1)\n",
    "#         # out.shape == (batch_size, seq_len, d_model)\n",
    "#         return self.dropout(out)\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# test_in = torch.zeros(2, 5, 4) # batch_size, seq_len, d_model\n",
    "# l = RelativeGlobalAttention(d_model=4, num_heads=2,max_len=5)\n",
    "\n",
    "# l(test_in).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = 1\n",
    "# channal = 2\n",
    "# width = 5\n",
    "# height = 5\n",
    "# data = torch.randn(batch, channal, width, height) #　batch, channal, width, height\n",
    "# # 当pad只有两个参数时，如pad=(1,2)代表对最后一个维度改变——左边扩充一列，右边扩充2列\n",
    "# # 当pad有四个参数，代表对最后两个维度扩充，pad = (左边填充数， 右边填充数， 上边填充数， 下边填充数)\n",
    "# # 当pad有六个参数时，代表队最后三个维度扩充，\n",
    "# # pad = (左边填充数， 右边填充数， 上边填充数， 下边填充数， 前边填充数，后边填充数)\n",
    "# pad = (1,0)\n",
    "# data = torch.nn.functional.pad(data, pad, 'constant', 0)\n",
    "# print(data)\n",
    "# print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import torch\n",
    "# # import torch.nn as nn\n",
    "# # import math\n",
    "\n",
    "# # def d(tensor=None):\n",
    "# #     if tensor is None:\n",
    "# #         return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# #     return 'cuda' if tensor.is_cuda else 'cpu'\n",
    "\n",
    "# # class RelativeAttentionError(Exception):\n",
    "# #     pass\n",
    "\n",
    "# class RelativeMultiheadedAttention(torch.nn.Module):\n",
    "#     \"\"\"\n",
    "#     Narrow multiheaded attention. Each attention head inspects a \n",
    "#     fraction of the embedding space and expresses attention vectors for each sequence position as a weighted average of all (earlier) positions.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, d_model, heads=8, dropout=0.1, max_length=102, relative_pos=False):\n",
    "\n",
    "#         super().__init__()\n",
    "        \n",
    "# #         if d_model % heads != 0:\n",
    "# #             raise RelativeAttentionError(\"Number of heads does not divide model dimension\")\n",
    "            \n",
    "#         assert d_model % heads == 0\n",
    "#         self.d_model = d_model\n",
    "#         self.heads = heads\n",
    "#         s = d_model // heads\n",
    "        \n",
    "#         self.linears = torch.nn.ModuleList([torch.nn.Linear(s, s, bias=False) for i in range(3)])\n",
    "#         self.recombine_heads = torch.nn.Linear(heads * s, d_model)\n",
    "#         self.dropout = torch.nn.Dropout(p=dropout)\n",
    "#         self.max_length = max_length\n",
    "#         #relative positional embeddings\n",
    "#         self.relative_pos = relative_pos\n",
    "#         if relative_pos:\n",
    "#             self.Er = torch.randn([heads, self.max_length, s])\n",
    "#         else:\n",
    "#             self.Er = None\n",
    "\n",
    "#     def forward(self, x, mask):\n",
    "#         #batch size, sequence length, embedding dimension\n",
    "#         b, t, e = x.size()\n",
    "#         h = self.heads\n",
    "#         #each head inspects a fraction of the embedded space\n",
    "#         #head dimension\n",
    "#         s = e // h\n",
    "#         #start index of position embedding\n",
    "#         embedding_start = self.max_length - t\n",
    "#         x = x.view(b,t,h,s)\n",
    "#         queries, keys, values = [w(x).transpose(1,2)\n",
    "#                 for w, x in zip(self.linears, (x,x,x))]\n",
    "#         if self.relative_pos:\n",
    "#             #apply same position embeddings across the batch\n",
    "#             #Is it possible to apply positional self-attention over\n",
    "#             #only half of all relative distances?\n",
    "#             Er  = self.Er[:, embedding_start:, :].unsqueeze(0)\n",
    "#             print('Er:',Er.shape)\n",
    "#             QEr = torch.matmul(queries, Er.transpose(-1,-2))\n",
    "#             QEr = self.mask_positions(QEr)\n",
    "#             #Get relative position attention scores\n",
    "#             #combine batch with head dimension\n",
    "#             SRel = self.skew(QEr).contiguous().view(b*h, t, t)\n",
    "#         else:\n",
    "#             SRel = torch.zeros([b*h, t, t])\n",
    "#         print('SRel:',SRel.shape)\n",
    "#         queries, keys, values = map(lambda x: x.contiguous()\\\n",
    "#                 .view(b*h, t, s), (queries, keys, values))\n",
    "#         print('queries:',queries.shape)\n",
    "#         print('keys:',keys.shape)\n",
    "#         print('values:',values.shape)\n",
    "#         #Compute scaled dot-product self-attention\n",
    "#         #scale pre-matrix multiplication   \n",
    "#         queries = queries / (e ** (1/4))\n",
    "#         keys    = keys / (e ** (1/4))\n",
    "#         print('queries:',queries.shape)\n",
    "#         print('keys:',keys.shape)\n",
    "#         print('keys.transpose(1, 2):',keys.transpose(1, 2).shape)\n",
    "\n",
    "#         scores = torch.bmm(queries, keys.transpose(1, 2))\n",
    "#         print('scores:',scores.shape)\n",
    "#         scores = scores + SRel\n",
    "#         #(b*h, t, t)\n",
    "\n",
    "#         subsequent_mask = torch.triu(torch.ones(1, t, t),\n",
    "#                 1)\n",
    "#         scores = scores.masked_fill(subsequent_mask == 1, -1e9)\n",
    "#         if mask is not None:\n",
    "#             mask = mask.repeat_interleave(h, 0)\n",
    "#             wtf = (mask == 0).nonzero().transpose(0,1)\n",
    "#             scores[wtf[0], wtf[1], :] = -1e9\n",
    "\n",
    "        \n",
    "#         #Convert scores to probabilities\n",
    "#         attn_probs = torch.nn.functional.softmax(scores, dim=2)\n",
    "#         attn_probs = self.dropout(attn_probs)\n",
    "#         #use attention to get a weighted average of values\n",
    "#         out = torch.bmm(attn_probs, values).view(b, h, t, s)\n",
    "#         #transpose and recombine attention heads\n",
    "#         out = out.transpose(1, 2).contiguous().view(b, t, s * h)\n",
    "#         #last linear layer of weights\n",
    "#         return self.recombine_heads(out)\n",
    "\n",
    "\n",
    "#     def mask_positions(self, qe):\n",
    "#         #QEr is a matrix of queries (absolute position) dot distance embeddings (relative pos).\n",
    "#         #Mask out invalid relative positions: e.g. if sequence length is L, the query at\n",
    "#         #L-1 can only attend to distance r = 0 (no looking backward).\n",
    "#         L = qe.shape[-1]\n",
    "#         mask = torch.triu(torch.ones(L, L), 1).flip(1)\n",
    "#         return qe.masked_fill((mask == 1), 0)\n",
    "\n",
    "#     def skew(self, qe):\n",
    "#         #pad a column of zeros on the left\n",
    "#         padded_qe = torch.nn.functional.pad(qe, [1,0])\n",
    "#         s = padded_qe.shape\n",
    "#         padded_qe = padded_qe.view(s[0], s[1], s[3], s[2])\n",
    "#         #take out first (padded) row\n",
    "#         return padded_qe[:,:,1:,:]\n",
    "\n",
    "# test_in = torch.zeros(2, 5, 4) # batch_size, seq_len, d_model\n",
    "# # print(test_in)\n",
    "# test = RelativeMultiheadedAttention(d_model=4, heads=2, dropout=0.1, max_length=102, relative_pos=True)\n",
    "# result = test(test_in, mask = None)\n",
    "# print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.multi-head attention\n",
    "![jupyter-img3](./imgs/im3.jpg)\n",
    "\n",
    "多头注意允许模型在不同的位置联合处理来自不同表示子空间的信息。\n",
    "（Multi-head attention allows the model to jointly attend to information\n",
    "from different representation subspaces at different positions.）\n",
    "\n",
    "$$\\begin{array}{ll}  & MultiHead(Q,K,V)=Concat(head_1,...,head_h)W^O \\\\ & where\\; head_i=Attention(QW_i^Q,KW_i^K,VW_i^V)\\end{array}$$\n",
    "\n",
    "其中投影维参数矩阵$W_i^Q\\in R^{d_{model}\\text{x}\\,d_k}$，$W_i^K\\in R^{d_{model}\\text{x}\\,d_k}$， $W_i^V\\in R^{d_{model}\\text{x}\\,d_v}$，$W^O\\in R^{hd_v\\text{x}\\,d_{model}}$。\n",
    "\n",
    "$h=8,\\;d_k = d_v = d_{model}/h = 64$\n",
    "\n",
    "multi-head attention有4部分组成：\n",
    "- linear layer 将 linear layer的结果 split到不同的head\n",
    "- scaled dot-product attention\n",
    "- 将所有的head 进行拼接 concat\n",
    "- final linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0  # 因为输入要被（平均？）split到不同的head\n",
    "\n",
    "        self.depth = d_model // self.num_heads  # 512/8=64，所以在scaled dot-product atten中dq=dk=64,dv也是64\n",
    "\n",
    "        self.wq = torch.nn.Linear(d_model, d_model)\n",
    "        self.wk = torch.nn.Linear(d_model, d_model)\n",
    "        self.wv = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.final_linear = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):  # x [b, seq_len, d_model]\n",
    "        x = x.view(batch_size, -1, self.num_heads,\n",
    "                   self.depth)  # [b, seq_len, d_model=512]=>[b, seq_len, num_head=8, depth=64]\n",
    "        return x.transpose(1, 2)  # [b, seq_len, num_head=8, depth=64]=>[b, num_head=8, seq_len, depth=64]\n",
    "\n",
    "    def forward(self, q, k, v, mask):  # q=k=v=x [b, seq_len, embedding_dim] embedding_dim其实也=d_model\n",
    "        batch_size = q.shape[0]\n",
    "\n",
    "        q = self.wq(q)  # =>[b, seq_len, d_model]\n",
    "        k = self.wk(k)  # =>[b, seq_len, d_model]\n",
    "        v = self.wv(v)  # =>[b, seq_len, d_model]\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # =>[b, num_head=8, seq_len, depth=64]\n",
    "        k = self.split_heads(k, batch_size)  # =>[b, num_head=8, seq_len, depth=64]\n",
    "        v = self.split_heads(v, batch_size)  # =>[b, num_head=8, seq_len, depth=64]\n",
    "\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        # => [b, num_head=8, seq_len_q, depth=64], [b, num_head=8, seq_len_q, seq_len_k]\n",
    "\n",
    "        scaled_attention = scaled_attention.transpose(1, 2)  # =>[b, seq_len_q, num_head=8, depth=64]\n",
    "        # 转置操作让张量存储结构扭曲，直接使用view方法会失败，可以使用reshape方法\n",
    "        concat_attention = scaled_attention.reshape(batch_size, -1, self.d_model)  # =>[b, seq_len_q, d_model=512]\n",
    "\n",
    "        output = self.final_linear(concat_attention)  # =>[b, seq_len_q, d_model=512]\n",
    "        return output, attention_weights  # [b, seq_len_q, d_model=512], [b, num_head=8, seq_len_q, seq_len_k]\n",
    "\n",
    "\n",
    "# temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "# x = torch.rand(1, 64, 512) # [b,seq_len,d_model,embedding_dim]\n",
    "# print(x.shape)\n",
    "# out, attn_weights = temp_mha(x, x, x, mask=None)\n",
    "# print(out.shape, attn_weights.shape) # [1, 60, 512], [1, 8, 60, 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class mn_MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads, seq_len):\n",
    "        super(mn_MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        assert d_model % self.num_heads == 0  # 因为输入要被（平均？）split到不同的head\n",
    "        \n",
    "        # 512/8=64，所以在mn_scaled dot-product atten中dq=dk=64,dv也是64\n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "\n",
    "        self.wq = torch.nn.Linear(d_model, d_model)\n",
    "        self.wk = torch.nn.Linear(d_model, d_model)\n",
    "        self.wv = torch.nn.Linear(d_model, d_model)\n",
    "        self.wm = torch.nn.Linear(d_model, seq_len*num_heads)\n",
    "\n",
    "        self.final_linear = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):  # x [b, seq_len, d_model]\n",
    "        x = x.view(batch_size, -1, self.num_heads,\n",
    "                   self.depth)  # [b, seq_len, d_model=512]=>[b, seq_len, num_head=8, depth=64]\n",
    "        return x.transpose(1, 2)  # [b, seq_len, num_head=8, depth=64]=>[b, num_head=8, seq_len, depth=64]\n",
    "    \n",
    "    def split_heads_m(self, x, batch_size):  # x [b, seq_len, d_model]\n",
    "        x = x.view(batch_size, -1, self.num_heads,\n",
    "                   self.seq_len)  # [b, seq_len, d_model=512]=>[b, seq_len, num_head=8, depth=64]\n",
    "        return x.transpose(1, 2)  # [b, seq_len, num_head=8, depth=64]=>[b, num_head=8, seq_len, depth=seq_len]\n",
    "    \n",
    "    def forward(self, q, k, v, m, mask):  # q=k=v=x [b, seq_len, embedding_dim] embedding_dim其实也=d_model\n",
    "        batch_size = q.shape[0]\n",
    "        \n",
    "        q = self.wq(q)  # =>[b, seq_len, d_model]\n",
    "#         print('q：', q.shape)\n",
    "        k = self.wk(k)  # =>[b, seq_len, d_model]\n",
    "#         print('k：', k.shape)\n",
    "        v = self.wv(v)  # =>[b, seq_len, d_model]\n",
    "#         print('v：', v.shape)\n",
    "        m = self.wm(m)  # =>[b, seq_len, seq_len]\n",
    "#         print('m：', m.shape)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # =>[b, num_head=8, seq_len, depth=64]\n",
    "#         print('q：', q.shape)\n",
    "        k = self.split_heads(k, batch_size)  # =>[b, num_head=8, seq_len, depth=64]\n",
    "#         print('k：', k.shape)\n",
    "        v = self.split_heads(v, batch_size)  # =>[b, num_head=8, seq_len, depth=64]\n",
    "#         print('v：', v.shape)\n",
    "        m = self.split_heads_m(m, batch_size)  # =>[b, num_head=8, seq_len, depth=64]\n",
    "#         print('m：', m.shape)\n",
    "        scaled_attention, attention_weights = mn_scaled_dot_product_attention(q, k, v, m, mask)\n",
    "        # => [b, num_head=8, seq_len_q, depth=64], [b, num_head=8, seq_len_q, seq_len_k]\n",
    "\n",
    "        scaled_attention = scaled_attention.transpose(1, 2)  # =>[b, seq_len_q, num_head=8, depth=64]\n",
    "        # 转置操作让张量存储结构扭曲，直接使用view方法会失败，可以使用reshape方法\n",
    "        concat_attention = scaled_attention.reshape(batch_size, -1, self.d_model)  # =>[b, seq_len_q, d_model=512]\n",
    "\n",
    "        output = self.final_linear(concat_attention)  # =>[b, seq_len_q, d_model=512]\n",
    "        return output, attention_weights  # [b, seq_len_q, d_model=512], [b, num_head=8, seq_len_q, seq_len_k]\n",
    "\n",
    "\n",
    "# temp_mha = mn_MultiHeadAttention(d_model=256, num_heads=4, seq_len=MAX_LENGTH)\n",
    "# x = torch.rand(128, MAX_LENGTH, 256) # [b,seq_len,d_model,embedding_dim]\n",
    "# print('输入：', x.shape)\n",
    "# out, attn_weights = temp_mha(x, x, x, x, mask=None)\n",
    "# print(out.shape, attn_weights.shape) # [1, 60, 512], [1, 8, 60, 60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 6.point wise feed forward network\n",
    "2层线性变换和一个ReLU激活：\n",
    "$$FFN(x) = max(0, xW1 + b1)W2 + b2$$\n",
    "\n",
    "这一层的input和output的维度都是$d_{model}$，而内层的维度的是$d_{ff}=2048$\n",
    "\n",
    "其实就是(nn.relu(x w1+b1))w2+b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 点式前馈网络\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    feed_forward_net = torch.nn.Sequential(\n",
    "        torch.nn.Linear(d_model, dff),  # [b, seq_len, d_model]=>[b, seq_len, dff=2048]\n",
    "        #torch.nn.ReLU(),\n",
    "        \n",
    "        # 更换激活函数为SquaredReLU()\n",
    "#         torch.nn.LeakyReLU(),\n",
    "        SquaredReLU(), #primer_ez\n",
    "        torch.nn.Linear(dff, d_model),  # [b, seq_len, dff=2048]=>[b, seq_len, d_model=512]\n",
    "    )\n",
    "    return feed_forward_net\n",
    "\n",
    "# sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
    "# input = torch.rand(64, 50, 512) # [b, seq_len, d_model]\n",
    "# print(sample_ffn(input).shape) # [b=64, seq_len=50, d_model=512]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "transformer model:\n",
    "\n",
    "(1) input sentence 传入N个encoder layer，句子的每个token都会有一个输出\n",
    "\n",
    "(2) decoder attends on encoder 的输出（encoder-decoder attention） 和 它自己的输入（self-attention），然后预测出下一个词\n",
    "\n",
    "## 7.encoder layer\n",
    "每个编码器层包括以下2个子层：\n",
    "- 多头注意力（有padding mask）\n",
    "- 点式前馈网络（Point wise feed forward networks）\n",
    "\n",
    "注意：每个子层还伴随着一个残差连接，然后进行“层归一化”（LayerNorm）。残差连接有助于避免深度网络中的梯度消失问题。\n",
    "\n",
    "每个子层的输出是 LayerNorm(x + Sublayer(x))。归一化是在 d_model（最后一个）维度完成的。\n",
    "\n",
    "注意：实现时在每个sub layer 之后加入了dropout层，再才进行add the sub layer input 和 normalized，即\n",
    "LayerNorm(x + Dropout(Sublayer(x)))\n",
    "\n",
    "Transformer 中有 N 个编码器层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)  # 多头注意力（padding mask）(self-attention)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "        self.layernorm2 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "\n",
    "        self.dropout1 = torch.nn.Dropout(rate)\n",
    "        self.dropout2 = torch.nn.Dropout(rate)\n",
    "\n",
    "    # x [b, inp_seq_len, embedding_dim] embedding_dim其实也=d_model\n",
    "    # mask [b,1,1,inp_seq_len]\n",
    "    def forward(self, x, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # =>[b, seq_len, d_model]\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)  # 残差&层归一化 =>[b, seq_len, d_model]\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # =>[b, seq_len, d_model]\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # 残差&层归一化 =>[b, seq_len, d_model]\n",
    "\n",
    "        return out2  # [b, seq_len, d_model]\n",
    "\n",
    "# layernorm = torch.nn.LayerNorm(normalized_shape=512, eps=1e-6)\n",
    "# x = torch.rand(4, 64, 512)\n",
    "# print(layernorm(x).shape)\n",
    "\n",
    "# sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "# x = torch.rand(64, 64, 512) # [b, seq_len, d_model]\n",
    "# sample_encoder_layer_output = sample_encoder_layer(x, None)\n",
    "# print(sample_encoder_layer_output.shape) # [64, 50, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mn_EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff, seq_len, rate=0.1):\n",
    "        super(mn_EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = mn_MultiHeadAttention(d_model, num_heads, seq_len)  # 多头注意力（padding mask）(self-attention)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "        self.layernorm2 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "\n",
    "        self.dropout1 = torch.nn.Dropout(rate)\n",
    "        self.dropout2 = torch.nn.Dropout(rate)\n",
    "\n",
    "    # x [b, inp_seq_len, embedding_dim] embedding_dim其实也=d_model\n",
    "    # mask [b,1,1,inp_seq_len]\n",
    "    def forward(self, x, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, x, mask)  # =>[b, seq_len, d_model]\n",
    "#         print(attn_output.shape)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)  # 残差&层归一化 =>[b, seq_len, d_model]\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # =>[b, seq_len, d_model]\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # 残差&层归一化 =>[b, seq_len, d_model]\n",
    "\n",
    "        return out2  # [b, seq_len, d_model]\n",
    "\n",
    "# layernorm = torch.nn.LayerNorm(normalized_shape=512, eps=1e-6)\n",
    "# x = torch.rand(4, MAX_LENGTH, 512)\n",
    "# print(layernorm(x).shape)\n",
    "\n",
    "# sample_encoder_layer = mn_EncoderLayer(512, 8, 2048,MAX_LENGTH)\n",
    "# x = torch.rand(64, MAX_LENGTH, 512) # [b, seq_len, d_model]\n",
    "# sample_encoder_layer_output = sample_encoder_layer(x, None)\n",
    "# print(sample_encoder_layer_output.shape) # [64, 50, 512]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 8.decoder layer\n",
    "\n",
    "每个解码器层包括以下3个子层：\n",
    "- masked的多头注意力（look ahead mask 和 padding mask）(self-attention)\n",
    "- 多头注意力（padding mask）(encoder-decoder attention)。\n",
    "    V和 K接收encoder的输出作为输入。Q接收masked的多头注意力子层的输出。\n",
    "- 点式前馈网络\n",
    "\n",
    "每个子层还伴随着一个残差连接，然后进行“层归一化”（LayerNorm）。\n",
    "\n",
    "每个子层的输出是 LayerNorm(x + Sublayer(x))。归一化是在 d_model（最后一个）维度完成的。\n",
    "\n",
    "Transformer 中共有 N 个解码器层。\n",
    "\n",
    "当 Q 接收到decoder的第一个注意力块的输出，并且 K 接收到encoder的输出时，注意力权重表示根据encoder的输出赋予decoder输入的重要性。\n",
    "换一种说法，decoder通过查看encoder输出和对其自身输出的自注意力，预测下一个词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model,\n",
    "                                       num_heads)  # masked的多头注意力（look ahead mask 和 padding mask）(self-attention)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)  # 多头注意力（padding mask）(encoder-decoder attention)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "        self.layernorm2 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "        self.layernorm3 = torch.nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "\n",
    "        self.dropout1 = torch.nn.Dropout(rate)\n",
    "        self.dropout2 = torch.nn.Dropout(rate)\n",
    "        self.dropout3 = torch.nn.Dropout(rate)\n",
    "\n",
    "    # x [b, targ_seq_len, embedding_dim] embedding_dim其实也=d_model=512\n",
    "    # look_ahead_mask [b, 1, targ_seq_len, targ_seq_len] 这里传入的look_ahead_mask应该是已经结合了look_ahead_mask和padding mask的mask\n",
    "    # enc_output [b, inp_seq_len, d_model]\n",
    "    # padding_mask [b, 1, 1, inp_seq_len]\n",
    "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x,\n",
    "                                               look_ahead_mask)  # =>[b, targ_seq_len, d_model], [b, num_heads, targ_seq_len, targ_seq_len]\n",
    "        attn1 = self.dropout1(attn1)\n",
    "        out1 = self.layernorm1(x + attn1)  # 残差&层归一化 [b, targ_seq_len, d_model]\n",
    "\n",
    "        # Q: receives the output from decoder's first attention block，即 masked multi-head attention sublayer\n",
    "        # K V: V (value) and K (key) receive the encoder output as inputs\n",
    "        attn2, attn_weights_block2 = self.mha2(out1, enc_output, enc_output,\n",
    "                                               padding_mask)  # =>[b, targ_seq_len, d_model], [b, num_heads, targ_seq_len, inp_seq_len]\n",
    "        attn2 = self.dropout2(attn2)\n",
    "        out2 = self.layernorm2(out1 + attn2)  # 残差&层归一化 [b, targ_seq_len, d_model]\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # =>[b, targ_seq_len, d_model]\n",
    "        ffn_output = self.dropout3(ffn_output)\n",
    "        out3 = self.layernorm3(out2 + ffn_output)  # 残差&层归一化 =>[b, targ_seq_len, d_model]\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "        # [b, targ_seq_len, d_model], [b, num_heads, targ_seq_len, targ_seq_len], [b, num_heads, targ_seq_len, inp_seq_len]\n",
    "\n",
    "\n",
    "# sample_decoder_layer = DecoderLayer(512, 8, 2048) #\n",
    "# y = torch.rand(64, 40, 512) # [b, seq_len, d_model]\n",
    "# sample_decoder_layer_output,_,_ = sample_decoder_layer(y, sample_encoder_layer_output, None, None)\n",
    "# print(sample_decoder_layer_output.shape) # [64, 40, 512]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 9.encoder\n",
    "编码器 包括：\n",
    "- 输入嵌入（Input Embedding）\n",
    "- 位置编码（Positional Encoding）\n",
    "- N 个编码器层（encoder layers）\n",
    "\n",
    "输入经过嵌入（embedding）后，该嵌入与位置编码相加。该加法结果的输出是编码器层的输入。编码器的输出是解码器的输入\n",
    "\n",
    "注意：缩放 embedding\n",
    "\n",
    "原始论文的3.4节Embeddings and Softmax最后一句有提到： In the embedding layers, we multiply those weights by $\\sqrt{d_{model}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers,  # N个encoder layer\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dff,  # 点式前馈网络内层fn的维度\n",
    "                 input_vocab_size,  # 输入词表大小（源语言（法语））\n",
    "                 maximun_position_encoding,\n",
    "                 rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=input_vocab_size, embedding_dim=d_model)\n",
    "        self.pos_encoding = positional_encoding(maximun_position_encoding,\n",
    "                                                d_model)  # =>[1, max_pos_encoding, d_model=512]\n",
    "\n",
    "        # self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate).cuda() for _ in range(num_layers)] # 不行\n",
    "        self.enc_layers = torch.nn.ModuleList([EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)])\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(rate)\n",
    "\n",
    "    # x [b, inp_seq_len]\n",
    "    # mask [b, 1, 1, inp_sel_len]\n",
    "    def forward(self, x, mask):\n",
    "        inp_seq_len = x.shape[-1]\n",
    "\n",
    "        # adding embedding and position encoding\n",
    "        x = self.embedding(x)  # [b, inp_seq_len]=>[b, inp_seq_len, d_model]\n",
    "        # 缩放 embedding 原始论文的3.4节有提到： In the embedding layers, we multiply those weights by \\sqrt{d_model}.\n",
    "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
    "        pos_encoding = self.pos_encoding[:, :inp_seq_len, :]\n",
    "        pos_encoding = pos_encoding.cuda()  # ###############\n",
    "        x += pos_encoding  # [b, inp_seq_len, d_model]\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, mask)  # [b, inp_seq_len, d_model]=>[b, inp_seq_len, d_model]\n",
    "        return x  # [b, inp_seq_len, d_model]\n",
    "\n",
    "\n",
    "# sample_encoder = Encoder(num_layers=2,\n",
    "#                          d_model=512,\n",
    "#                          num_heads=8,\n",
    "#                          dff=2048,\n",
    "#                          input_vocab_size=8500,\n",
    "#                          maximun_position_encoding=10000)\n",
    "# sample_encoder = sample_encoder.to(device)\n",
    "\n",
    "# x = torch.tensor(np.random.randint(low=0, high=200, size=(64, 42))) # [b, seq_len]\n",
    "# # print(x.shape)\n",
    "# sample_encoder_output = sample_encoder(x.cuda(), None)\n",
    "# print(sample_encoder_output.shape) # [b, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mn_Encoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers,  # N个encoder layer\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dff,  # 点式前馈网络内层fn的维度\n",
    "                 input_vocab_size,  # 输入词表大小（源语言（法语））\n",
    "                 maximun_position_encoding,\n",
    "                 seq_len,\n",
    "                 rate=0.1):\n",
    "        super(mn_Encoder, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=input_vocab_size, embedding_dim=d_model)\n",
    "        self.pos_encoding = positional_encoding(maximun_position_encoding,\n",
    "                                                d_model)  # =>[1, max_pos_encoding, d_model=512]\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate).cuda() for _ in range(num_layers)] # 不行\n",
    "        self.enc_layers = torch.nn.ModuleList([mn_EncoderLayer(d_model, num_heads, dff, seq_len, rate) for _ in range(num_layers)])\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(rate)\n",
    "\n",
    "    # x [b, inp_seq_len]\n",
    "    # mask [b, 1, 1, inp_sel_len]\n",
    "    def forward(self, x, mask):\n",
    "        inp_seq_len = x.shape[-1]\n",
    "\n",
    "        # adding embedding and position encoding\n",
    "        x = self.embedding(x)  # [b, inp_seq_len]=>[b, inp_seq_len, d_model]\n",
    "        # 缩放 embedding 原始论文的3.4节有提到： In the embedding layers, we multiply those weights by \\sqrt{d_model}.\n",
    "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
    "        pos_encoding = self.pos_encoding[:, :inp_seq_len, :]\n",
    "        pos_encoding = pos_encoding.cuda()  # ###############\n",
    "        x += pos_encoding  # [b, inp_seq_len, d_model]\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, mask)  # [b, inp_seq_len, d_model]=>[b, inp_seq_len, d_model]\n",
    "        return x  # [b, inp_seq_len, d_model]\n",
    "\n",
    "\n",
    "# sample_encoder = mn_Encoder(num_layers=2,\n",
    "#                          d_model=512,\n",
    "#                          num_heads=8,\n",
    "#                          dff=2048,\n",
    "#                          input_vocab_size=8500,\n",
    "#                          maximun_position_encoding=10000,\n",
    "#                          seq_len = MAX_LENGTH)\n",
    "# sample_encoder = sample_encoder.to(device)\n",
    "\n",
    "# temp_inp = torch.tensor(np.random.randint(low=0, high=200, size=(64, MAX_LENGTH))) # [b, inp_seq_len]\n",
    "# print(x.shape)\n",
    "# sample_encoder_output = sample_encoder(temp_inp.cuda(), None)\n",
    "# print(sample_encoder_output.shape) # [b, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 10.decoder\n",
    "解码器包括：\n",
    "- 输出嵌入（Output Embedding）\n",
    "- 位置编码（Positional Encoding）\n",
    "\n",
    "N 个解码器层（decoder layers）\n",
    "\n",
    "目标（target）经过一个嵌入后，该嵌入和位置编码相加。该加法结果是解码器层的输入。解码器的输出是最后的线性层的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers,  # N个encoder layer\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dff,  # 点式前馈网络内层fn的维度\n",
    "                 target_vocab_size,  # target词表大小（目标语言（英语））\n",
    "                 maximun_position_encoding,\n",
    "                 rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=target_vocab_size, embedding_dim=d_model)\n",
    "        self.pos_encoding = positional_encoding(maximun_position_encoding,\n",
    "                                                d_model)  # =>[1, max_pos_encoding, d_model=512]\n",
    "\n",
    "        # self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate).cuda() for _ in range(num_layers)] # 不行\n",
    "        self.dec_layers = torch.nn.ModuleList([DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)])\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(rate)\n",
    "\n",
    "    # x [b, targ_seq_len]\n",
    "    # look_ahead_mask [b, 1, targ_seq_len, targ_seq_len] 这里传入的look_ahead_mask应该是已经结合了look_ahead_mask和padding mask的mask\n",
    "    # enc_output [b, inp_seq_len, d_model]\n",
    "    # padding_mask [b, 1, 1, inp_seq_len]\n",
    "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        targ_seq_len = x.shape[-1]\n",
    "\n",
    "        attention_weights = {}\n",
    "\n",
    "        # adding embedding and position encoding\n",
    "        x = self.embedding(x)  # [b, targ_seq_len]=>[b, targ_seq_len, d_model]\n",
    "        # 缩放 embedding 原始论文的3.4节有提到： In the embedding layers, we multiply those weights by \\sqrt{d_model}.\n",
    "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
    "        # x += self.pos_encoding[:, :targ_seq_len, :]  # [b, targ_seq_len, d_model]\n",
    "        pos_encoding = self.pos_encoding[:, :targ_seq_len, :]  # [b, targ_seq_len, d_model]\n",
    "        pos_encoding = pos_encoding.cuda() # ###############\n",
    "        x += pos_encoding  # [b, inp_seq_len, d_model]\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        #此处添加相对距离\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x, attn_block1, attn_block2 = self.dec_layers[i](x, enc_output, look_ahead_mask, padding_mask)\n",
    "            # => [b, targ_seq_len, d_model], [b, num_heads, targ_seq_len, targ_seq_len], [b, num_heads, targ_seq_len, inp_seq_len]\n",
    "\n",
    "            attention_weights[f'decoder_layer{i + 1}_block1'] = attn_block1\n",
    "            attention_weights[f'decoder_layer{i + 1}_block2'] = attn_block2\n",
    "\n",
    "        return x, attention_weights\n",
    "        # => [b, targ_seq_len, d_model],\n",
    "        # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "        #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "\n",
    "# sample_decoder = Decoder(num_layers=2,\n",
    "#                          d_model=512,\n",
    "#                          num_heads=8,\n",
    "#                          dff=2048,\n",
    "#                          target_vocab_size=8000,\n",
    "#                          maximun_position_encoding=5000)\n",
    "# sample_decoder = sample_decoder.to(device)\n",
    "\n",
    "# y = torch.tensor(np.random.randint(low=0, high=200, size=(64, 42))) # [b, seq_len]\n",
    "# # print(y.shape) # [64, 36]\n",
    "# output, attn = sample_decoder(y.cuda(),\n",
    "#                               enc_output=sample_encoder_output, # [64, 42, 512]\n",
    "#                               look_ahead_mask=None,\n",
    "#                               padding_mask=None)\n",
    "# print(output.shape) # [64, 36, 512]\n",
    "# print(attn['decoder_layer2_block1'].shape) # [64, 8, 36, 36]\n",
    "# print(attn['decoder_layer2_block2'].shape) # [64, 8, 36, 42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dependency_parsing_Decoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers,  # N个encoder layer\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dff,  # 点式前馈网络内层fn的维度\n",
    "                 target_vocab_size,  # target词表大小（目标语言（英语））\n",
    "                 maximun_position_encoding,\n",
    "                 rate=0.1):\n",
    "        super(dependency_parsing_Decoder, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=target_vocab_size, embedding_dim=d_model)\n",
    "        self.pos_encoding = positional_encoding(maximun_position_encoding,\n",
    "                                                d_model)  # =>[1, max_pos_encoding, d_model=512]\n",
    "        \n",
    "        self.dependency_parsing_Attention = torch.nn.ModuleList([dependency_parsing_Attention(d_model)])\n",
    "        # self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate).cuda() for _ in range(num_layers)] # 不行\n",
    "        self.dec_layers = torch.nn.ModuleList([DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)])\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(rate)\n",
    "\n",
    "    # x [b, targ_seq_len]\n",
    "    # look_ahead_mask [b, 1, targ_seq_len, targ_seq_len] 这里传入的look_ahead_mask应该是已经结合了look_ahead_mask和padding mask的mask\n",
    "    # enc_output [b, inp_seq_len, d_model]\n",
    "    # padding_mask [b, 1, 1, inp_seq_len]\n",
    "    def forward(self, x, dependency_parsing_matrix, enc_output, look_ahead_mask, padding_mask):\n",
    "#         print('dependency_parsing_Decoder_input:',x.shape)\n",
    "        targ_seq_len = x.shape[-1]\n",
    "\n",
    "        attention_weights = {}\n",
    "\n",
    "        # adding embedding and position encoding\n",
    "        x = self.embedding(x)  # [b, targ_seq_len]=>[b, targ_seq_len, d_model]\n",
    "        # 缩放 embedding 原始论文的3.4节有提到： In the embedding layers, we multiply those weights by \\sqrt{d_model}.\n",
    "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
    "        # x += self.pos_encoding[:, :targ_seq_len, :]  # [b, targ_seq_len, d_model]\n",
    "        pos_encoding = self.pos_encoding[:, :targ_seq_len, :]  # [b, targ_seq_len, d_model]\n",
    "        pos_encoding = pos_encoding.cuda() # ###############\n",
    "        x += pos_encoding  # [b, inp_seq_len, d_model]\n",
    "#         print('x += pos_encoding:',x.shape)\n",
    "        \n",
    "        #此处添加相对距离\n",
    "        x, _ = self.dependency_parsing_Attention[0](x, x, x, dependency_parsing_matrix, look_ahead_mask)\n",
    "        \n",
    "#         print('dependency_parsing_Attention:',x.shape)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x, attn_block1, attn_block2 = self.dec_layers[i](x, enc_output, look_ahead_mask, padding_mask)\n",
    "            # => [b, targ_seq_len, d_model], [b, num_heads, targ_seq_len, targ_seq_len], [b, num_heads, targ_seq_len, inp_seq_len]\n",
    "\n",
    "            attention_weights[f'decoder_layer{i + 1}_block1'] = attn_block1\n",
    "            attention_weights[f'decoder_layer{i + 1}_block2'] = attn_block2\n",
    "\n",
    "        return x, attention_weights\n",
    "        # => [b, targ_seq_len, d_model],\n",
    "        # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "        #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "\n",
    "# sample_decoder = dependency_parsing_Decoder(num_layers=2,\n",
    "#                          d_model=512,\n",
    "#                          num_heads=8,\n",
    "#                          dff=2048,\n",
    "#                          target_vocab_size=8000,\n",
    "#                          maximun_position_encoding=5000)\n",
    "# sample_decoder = sample_decoder.to(device)\n",
    "\n",
    "# y = torch.tensor(np.random.randint(low=0, high=200, size=(64, 42))) # [b, seq_len]\n",
    "# dependency_parsing_matrix = torch.tensor(np.random.randint(low=0, high=200, size=(64, 42, 42)))\n",
    "\n",
    "# output, attn = sample_decoder(y.cuda(),\n",
    "#                               dependency_parsing_matrix.cuda(),\n",
    "#                               enc_output=sample_encoder_output, # [64, 42, 512]\n",
    "#                               look_ahead_mask=None,\n",
    "#                               padding_mask=None)\n",
    "# print(output.shape) # [64, 36, 512]\n",
    "# print(attn['decoder_layer2_block1'].shape) # [64, 8, 36, 36]\n",
    "# print(attn['decoder_layer2_block2'].shape) # [64, 8, 36, 42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 11.搭建transformer\n",
    "transformer 含有3各部分\n",
    "- encoder\n",
    "- decoder\n",
    "- final linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# class Transformer(torch.nn.Module):\n",
    "#     def __init__(self,\n",
    "#                  num_layers,  # N个encoder layer\n",
    "#                  d_model,\n",
    "#                  num_heads,\n",
    "#                  dff,  # 点式前馈网络内层fn的维度\n",
    "#                  input_vocab_size,  # input此表大小（源语言（法语））\n",
    "#                  target_vocab_size,  # target词表大小（目标语言（英语））\n",
    "#                  pe_input,  # input max_pos_encoding\n",
    "#                  pe_target,  # input max_pos_encoding\n",
    "#                  rate=0.1):\n",
    "#         super(Transformer, self).__init__()\n",
    "\n",
    "#         self.encoder = Encoder(num_layers,\n",
    "#                                d_model,\n",
    "#                                num_heads,\n",
    "#                                dff,\n",
    "#                                input_vocab_size,\n",
    "#                                pe_input,\n",
    "#                                rate)\n",
    "#         self.decoder = Decoder(num_layers,\n",
    "#                                d_model,\n",
    "#                                num_heads,\n",
    "#                                dff,\n",
    "#                                target_vocab_size,\n",
    "#                                pe_target,\n",
    "#                                rate)\n",
    "#         self.final_layer = torch.nn.Linear(d_model, target_vocab_size)\n",
    "\n",
    "#     # inp [b, inp_seq_len]\n",
    "#     # targ [b, targ_seq_len]\n",
    "#     # enc_padding_mask [b, 1, 1, inp_seq_len]\n",
    "#     # look_ahead_mask [b, 1, targ_seq_len, targ_seq_len]\n",
    "#     # dec_padding_mask [b, 1, 1, inp_seq_len] # 注意这里的维度是inp_seq_len\n",
    "#     def forward(self, inp, targ, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "#         enc_output = self.encoder(inp, enc_padding_mask)  # =>[b, inp_seq_len, d_model]\n",
    "\n",
    "#         dec_output, attention_weights = self.decoder(targ, enc_output, look_ahead_mask, dec_padding_mask)\n",
    "#         # => [b, targ_seq_len, d_model],\n",
    "#         # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "#         #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "#         final_output = self.final_layer(dec_output)  # =>[b, targ_seq_len, target_vocab_size]\n",
    "\n",
    "#         return final_output, attention_weights\n",
    "#         # [b, targ_seq_len, target_vocab_size]\n",
    "#         # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "#         #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "\n",
    "\n",
    "# sample_transformer = Transformer(num_layers=2,\n",
    "#                                  d_model=512,\n",
    "#                                  num_heads=8,\n",
    "#                                  dff=2048,\n",
    "#                                  input_vocab_size=8500,\n",
    "#                                  target_vocab_size=8000,\n",
    "#                                  pe_input=10000,\n",
    "#                                  pe_target=6000)\n",
    "# sample_transformer = sample_transformer.to(device)\n",
    "\n",
    "# temp_inp = torch.tensor(np.random.randint(low=0, high=200, size=(64, 36))) # [b, inp_seq_len]\n",
    "# temp_targ = torch.tensor(np.random.randint(low=0, high=200, size=(64, 42))) # [b, targ_seq_len]\n",
    "\n",
    "# fn_out, attn = sample_transformer(temp_inp.cuda(), temp_targ.cuda(), None, None, None)\n",
    "# print(fn_out.shape) # [64, 36, 8000]\n",
    "# print(attn['decoder_layer2_block1'].shape) # [64, 8, 36, 36]\n",
    "# print(attn['decoder_layer2_block2'].shape) # [64, 8, 36, 42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mn_Transformer(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers,  # N个encoder layer\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dff,  # 点式前馈网络内层fn的维度\n",
    "                 input_vocab_size,  # input此表大小（源语言（法语））\n",
    "                 target_vocab_size,  # target词表大小（目标语言（英语））\n",
    "                 pe_input,  # input max_pos_encoding\n",
    "                 pe_target,  # input max_pos_encoding\n",
    "                 seq_len,\n",
    "                 rate=0.1):\n",
    "        super(mn_Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = mn_Encoder(num_layers,\n",
    "                               d_model,\n",
    "                               num_heads,\n",
    "                               dff,\n",
    "                               input_vocab_size,\n",
    "                               pe_input,\n",
    "                               seq_len,\n",
    "                               rate)\n",
    "        # 修改\n",
    "        self.decoder = dependency_parsing_Decoder(num_layers,\n",
    "                               d_model,\n",
    "                               num_heads,\n",
    "                               dff,\n",
    "                               target_vocab_size,\n",
    "                               pe_target,\n",
    "                               rate)\n",
    "        self.final_layer = torch.nn.Linear(d_model, target_vocab_size)\n",
    "\n",
    "    # inp [b, inp_seq_len]\n",
    "    # targ [b, targ_seq_len]\n",
    "    # enc_padding_mask [b, 1, 1, inp_seq_len]\n",
    "    # look_ahead_mask [b, 1, targ_seq_len, targ_seq_len]\n",
    "    # dec_padding_mask [b, 1, 1, inp_seq_len] # 注意这里的维度是inp_seq_len\n",
    "    def forward(self, inp, targ, dependency_parsing_matrix, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        enc_output = self.encoder(inp, enc_padding_mask)  # =>[b, inp_seq_len, d_model]\n",
    "        dec_output, attention_weights = self.decoder(targ, dependency_parsing_matrix, enc_output, look_ahead_mask, dec_padding_mask)\n",
    "        # => [b, targ_seq_len, d_model],\n",
    "        # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "        #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "        final_output = self.final_layer(dec_output)  # =>[b, targ_seq_len, target_vocab_size]\n",
    "\n",
    "        return final_output, attention_weights\n",
    "        # [b, targ_seq_len, target_vocab_size]\n",
    "        # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "        #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_transformer = mn_Transformer(num_layers=2,\n",
    "#                                  d_model=128,\n",
    "#                                  num_heads=8,\n",
    "#                                  dff=2048,\n",
    "#                                  input_vocab_size=8500,\n",
    "#                                  target_vocab_size=8000,\n",
    "#                                  pe_input=10000,\n",
    "#                                  pe_target=10000,\n",
    "#                                  seq_len=MAX_LENGTH+2)\n",
    "# sample_transformer = sample_transformer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_inp = torch.tensor(np.random.randint(low=0, high=200, size=(64, MAX_LENGTH+2))) # [b, inp_seq_len]\n",
    "# temp_targ = torch.tensor(np.random.randint(low=0, high=200, size=(64, MAX_LENGTH+2))) # [b, targ_seq_len]\n",
    "# dependency_parsing_matrix = torch.tensor(np.random.randint(low=0, high=200, size=(64, MAX_LENGTH+2, MAX_LENGTH+2)))\n",
    "\n",
    "# fn_out, attn = sample_transformer(temp_inp.cuda(), temp_targ.cuda(),dependency_parsing_matrix.cuda(), None, None, None)\n",
    "\n",
    "\n",
    "# # for step, (inp, targ) in enumerate(train_dataloader, start=1):\n",
    "# #     print(inp,inp.shape,'\\n',targ,targ.shape)\n",
    "# #     fn_out, attn = sample_transformer(inp.cuda(), targ.cuda(), None, None, None)\n",
    "# #     break\n",
    "\n",
    "\n",
    "\n",
    "# print(fn_out.shape) # [64, 36, 8000]\n",
    "# print(attn['decoder_layer2_block1'].shape) # [64, 8, 36, 36]\n",
    "# print(attn['decoder_layer2_block2'].shape) # [64, 8, 36, 42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 12.设置超参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_layers： 6\n",
      "d_model： 256\n",
      "dff： 2048\n",
      "num_heads： 8\n",
      "dropout_rate： 0.2\n"
     ]
    }
   ],
   "source": [
    "#Transformer 的基础模型使用的数值为：num_layers=6，d_model = 512，dff = 2048\n",
    "#为了让本示例小且相对较快，已经减小了num_layers、 d_model 和 dff 的值。\n",
    "num_layers = 5\n",
    "d_model = 256\n",
    "dff = 2048\n",
    "num_heads = 8\n",
    "\n",
    "print('num_layers：', num_layers)\n",
    "print('d_model：', d_model)\n",
    "print('dff：', dff)\n",
    "print('num_heads：', num_heads)\n",
    "\n",
    "dropout_rate = 0.2\n",
    "print('dropout_rate：', dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 13.优化器\n",
    "根据论文中的公式，将 Adam 优化器与自定义的学习速率调度程序（scheduler）配合使用。\n",
    "\n",
    "$$lrate = d_{model}^{-0.5} * min(\\text{step_num}^{-0.5},\\; \\text{step_num}*\\text{warmup_steps}^{-1.5})$$\n",
    "\n",
    "关于pytorch optimizer，参考：\n",
    "\n",
    "https://www.cnblogs.com/wanghui-garcia/p/10895397.html\n",
    "\n",
    "https://www.jianshu.com/p/5d85a59f1bac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, d_model, warm_steps=4):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warm_steps\n",
    "\n",
    "        super(CustomSchedule, self).__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"\n",
    "        # rsqrt 函数用于计算 x 元素的平方根的倒数.  即= 1 / sqrt{x}\n",
    "        arg1 = torch.rsqrt(torch.tensor(self._step_count, dtype=torch.float32))\n",
    "        arg2 = torch.tensor(self._step_count * (self.warmup_steps ** -1.5), dtype=torch.float32)\n",
    "        dynamic_lr = torch.rsqrt(self.d_model) * torch.minimum(arg1, arg2)\n",
    "        \"\"\"\n",
    "        # print('*'*27, self._step_count)\n",
    "        arg1 = self._step_count ** (-0.5)\n",
    "        arg2 = self._step_count * (self.warmup_steps ** -1.5)\n",
    "        dynamic_lr = (self.d_model ** (-0.5)) * min(arg1, arg2)\n",
    "        # print('dynamic_lr:', dynamic_lr)\n",
    "        return [dynamic_lr for group in self.optimizer.param_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # 测试\n",
    "# model = sample_transformer\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "# learning_rate = CustomSchedule(optimizer, d_model, warm_steps=4000)\n",
    "\n",
    "# lr_list = []\n",
    "# for i in range(1, 20000):\n",
    "#     learning_rate.step()\n",
    "#     lr_list.append(learning_rate.get_lr()[0])\n",
    "# plt.figure()\n",
    "# plt.plot(np.arange(1, 20000), lr_list)\n",
    "# plt.legend(['warmup=4000 steps'])\n",
    "# plt.ylabel(\"Learning Rate\")\n",
    "# plt.xlabel(\"Train Step\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # 其他的学习率调度器测试，例如pytorch自带的StepLR\n",
    "# _model = sample_transformer\n",
    "# _optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# _learning_rate = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# lr_list = []\n",
    "# for i in range(1, 50):\n",
    "#     _learning_rate.step()\n",
    "#     lr_list.append(_learning_rate.get_lr()[0])\n",
    "# plt.figure()\n",
    "# plt.plot(np.arange(1, 50), lr_list)\n",
    "# plt.legend(['StepLR:gamma=0.5'])\n",
    "# plt.ylabel(\"Learning Rate\")\n",
    "# plt.xlabel(\"Train Step\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 14.损失和评价准则\n",
    "\n",
    "计算loss时要把mask=1的位置的去除掉\n",
    "\n",
    "### 【大坑！】\n",
    "【注意】，当输入是多维时交叉熵的参数维度，跟tf2不一样，tf2中pred是【b,seq_len,vocab_size】\n",
    "pytorch中pred应该调整为【b,vocab_size,seq_len】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 'none'表示直接返回b个样本的loss，默认求平均\n",
    "loss_object = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "# 【注意】，当输入是多维时交叉熵的参数维度，跟tf2不一样，tf2中pred是【b,seq_len,vocab_size】\n",
    "# pytorch中pred应该调整为【b,vocab_size,seq_len】\n",
    "\"\"\"\n",
    "- Input: :math:`(N, C)` where `C = number of classes`, or\n",
    "          :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1`\n",
    "          in the case of `K`-dimensional loss.\n",
    "\n",
    "- Target: :math:`(N)` where each value is :math:`0 \\leq \\text{targets}[i] \\leq C-1`, or\n",
    "          :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \\geq 1` in the case of\n",
    "          K-dimensional loss.\n",
    "\"\"\"\n",
    "\n",
    "# real [b, targ_seq_len]\n",
    "# pred [b, targ_seq_len, target_vocab_size]\n",
    "def mask_loss_func(real, pred):\n",
    "    # print(real.shape, pred.shape)\n",
    "    # _loss = loss_object(pred, real) # [b, targ_seq_len]\n",
    "    _loss = loss_object(pred.transpose(-1, -2), real)  # [b, targ_seq_len]\n",
    "\n",
    "    # logical_not  取非\n",
    "    # mask 每个元素为bool值，如果real中有pad，则mask相应位置就为False\n",
    "    # mask = torch.logical_not(real.eq(0)).type(_loss.dtype) # [b, targ_seq_len] pad=0的情况\n",
    "    mask = torch.logical_not(real.eq(pad)).type(_loss.dtype)  # [b, targ_seq_len] pad!=0的情况\n",
    "\n",
    "    # 对应位置相乘，token上的损失被保留了下来，pad的loss被置为0或False 去掉，不计算在内\n",
    "    _loss *= mask\n",
    "\n",
    "    return _loss.sum() / mask.sum().item()\n",
    "\n",
    "# 另一种实现方式\n",
    "def mask_loss_func2(real, pred):\n",
    "    # _loss = loss_object(pred, real) # [b, targ_seq_len]\n",
    "    _loss = loss_object(pred.transpose(-1, -2), real)  # [b, targ_seq_len]\n",
    "    # mask = torch.logical_not(real.eq(0)) # [b, targ_seq_len] bool值\n",
    "    mask = torch.logical_not(real.eq(pad)) # [b, targ_seq_len] bool值\n",
    "    _loss = _loss.masked_select(mask) # mask必须是BoolTensor或ByteTensor类型\n",
    "    return _loss.mean()\n",
    "\n",
    "\n",
    "# y_pred = torch.randn(3,3) # [3,3]\n",
    "# y_true = torch.tensor([1,2,0]) # [3]\n",
    "# # print(y_true.shape, y_pred.shape)\n",
    "# print(loss_object(y_pred, y_true))\n",
    "# print('计算loss时把mask的也算进去了,损失会偏大？：', loss_object(y_pred, y_true).mean())\n",
    "# print('计算loss时去除mask:', mask_loss_func(y_true, y_pred))\n",
    "# print('计算loss时去除mask:', mask_loss_func2(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "同样计算metric（如acc）时要把mask=1的位置的去除掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# real [b, targ_seq_len]\n",
    "# pred [b, targ_seq_len, target_vocab_size]\n",
    "def mask_accuracy_func(real, pred):\n",
    "    _pred = pred.argmax(dim=-1)  # [b, targ_seq_len, target_vocab_size]=>[b, targ_seq_len]\n",
    "    corrects = _pred.eq(real)  # [b, targ_seq_len] bool值\n",
    "\n",
    "    # logical_not  取非\n",
    "    # mask 每个元素为bool值，如果real中有pad，则mask相应位置就为False\n",
    "    # mask = torch.logical_not(real.eq(0)) # [b, targ_seq_len] bool值 pad=0的情况\n",
    "    mask = torch.logical_not(real.eq(pad))  # [b, targ_seq_len] bool值 pad!=0的情况\n",
    "\n",
    "    # 对应位置相乘，token上的值被保留了下来，pad上的值被置为0或False 去掉，不计算在内\n",
    "    corrects *= mask\n",
    "\n",
    "    return corrects.sum().float() / mask.sum().item()\n",
    "\n",
    "# 另一种实现方式\n",
    "def mask_accuracy_func2(real, pred):\n",
    "    _pred = pred.argmax(dim=-1) # [b, targ_seq_len, target_vocab_size]=>[b, targ_seq_len]\n",
    "    corrects = _pred.eq(real).type(torch.float32) # [b, targ_seq_len]\n",
    "    # mask = torch.logical_not(real.eq(0)) # [b, targ_seq_len] bool值\n",
    "    mask = torch.logical_not(real.eq(pad)) # [b, targ_seq_len] bool值\n",
    "    corrects = corrects.masked_select(mask) # [真正有token的个数] 平摊开成1维的\n",
    "\n",
    "    return corrects.mean()\n",
    "\n",
    "def mask_accuracy_func3(real, pred):\n",
    "    _pred = pred.argmax(dim=-1) # [b, targ_seq_len, target_vocab_size]=>[b, targ_seq_len]\n",
    "    corrects = _pred.eq(real) # [b, targ_seq_len] bool值\n",
    "    # mask = torch.logical_not(real.eq(0)) # [b, targ_seq_len] bool值\n",
    "    mask = torch.logical_not(real.eq(pad)) # [b, targ_seq_len] bool值\n",
    "    corrects = torch.logical_and(corrects, mask)\n",
    "    # print(corrects.dtype) # bool\n",
    "    # print(corrects.sum().dtype) #int64\n",
    "    return corrects.sum().float()/mask.sum().item()\n",
    "\n",
    "# y_pred = torch.randn(3,3) # [3,3]\n",
    "# y_true = torch.tensor([0,2,1]) # [3] 最后一个1表示pad噢~\n",
    "# print(y_true)\n",
    "# print(y_pred)\n",
    "# print('计算acc时去除mask:', mask_accuracy_func(y_true, y_pred))\n",
    "# print('计算acc时去除mask:', mask_accuracy_func2(y_true, y_pred))\n",
    "# print('计算acc时去除mask:', mask_accuracy_func3(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 15.生成mask\n",
    "\n",
    "$$Attention(Q,K,V)=softmax_{(k)}(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "注意：实现时对mask的处理\n",
    "\n",
    "mask=1的位置是pad或者future token，乘以-1e9（-1*10^9）成为负无穷，经过softmax后会趋于0\n",
    "\n",
    "【注意】：在计算decoder的第2个attention block时（encoder-decoder attention），\n",
    "从scaled dot-product的公式可以看出，此时Q是decoder的第一个attention block的输出，\n",
    "而K，V都来自encoder的输出。QK^T后得到[...,seq_len_q, seq_len_k]，而softmax是在seq_len_k维\n",
    "上进行的，softmax后seq_len_k维上pad的位置被置于0，乘以V（seq_len_k=seq_len_v=encoder output的seq_len）\n",
    "后那些pad的位置还是只会是0。所以仅从公式就可以看出decoder的第2个attention block的padding mask是基于\n",
    "encoder output 的seq_len即整个模型的inp_seq_len来设置的，而不是targ_seq_len。\n",
    "\n",
    "从注意力角度来看，可以理解成：当 Q 接收到decoder的第一个注意力块的输出，并且 K 接收到encoder的输出时，注意力权重表示根据encoder的输出赋予decoder输入的重要性。\n",
    "换一种说法，decoder通过查看encoder输出和对其自身输出的自注意`力，预测下一个词。\n",
    "\n",
    "另外 padding mask 的维度是[b,1,1,seq_len] 一般只关注最后一个seq_len维度上pad(无论是在\n",
    "self-attention 或者encoder-decoder attention上都比较容易理解)\n",
    "\n",
    "而 look-ahead mask 是decoder self-attention时用于mask future token的，因为是“自”注意力，\n",
    "所以维度是[...,seq_len, seq_len]，需要自己跟自己运算，所以在QK（此时Q=K=V）矩阵乘的结果的最后2个维度上都需要mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# inp [b, inp_seq_len] 序列已经加入pad填充\n",
    "# targ [b, targ_seq_len] 序列已经加入pad填充\n",
    "def create_mask(inp, targ):\n",
    "    # encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(inp)  # =>[b,1,1,inp_seq_len] mask=1的位置为pad\n",
    "\n",
    "    # decoder's first attention block(self-attention)\n",
    "    # 使用的padding create_mask & look-ahead create_mask\n",
    "    look_ahead_mask = create_look_ahead_mask(targ.shape[-1])  # =>[targ_seq_len,targ_seq_len] ##################\n",
    "    dec_targ_padding_mask = create_padding_mask(targ)  # =>[b,1,1,targ_seq_len]\n",
    "    combined_mask = torch.max(look_ahead_mask, dec_targ_padding_mask)  # 结合了2种mask =>[b,1,targ_seq_len,targ_seq_len]\n",
    "\n",
    "    # decoder's second attention block(encoder-decoder attention) 使用的padding create_mask\n",
    "    # 【注意】：这里的mask是用于遮挡encoder output的填充pad，而encoder的输出与其输入shape都是[b,inp_seq_len,d_model]\n",
    "    # 所以这里mask的长度是inp_seq_len而不是targ_mask_len\n",
    "    dec_padding_mask = create_padding_mask(inp)  # =>[b,1,1,inp_seq_len] mask=1的位置为pad\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
    "    # [b,1,1,inp_seq_len], [b,1,targ_seq_len,targ_seq_len], [b,1,1,inp_seq_len]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 16.训练和保存\n",
    "\n",
    "### 配置检查点 save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 50,309,349 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "transformer = mn_Transformer(num_layers,\n",
    "                          d_model,\n",
    "                          num_heads,\n",
    "                          dff,\n",
    "                          input_vocab_size,\n",
    "                          target_vocab_size,\n",
    "                          pe_input=input_vocab_size,\n",
    "                          pe_target=target_vocab_size,\n",
    "                          seq_len=MAX_LENGTH+2,\n",
    "                          rate=dropout_rate)\n",
    "\n",
    "# print(transformer) # 打印模型基本信息\n",
    "\n",
    "transformer = transformer.to(device)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {count_parameters(transformer):,} trainable parameters')\n",
    "\n",
    "\n",
    "if ngpu > 1: # 并行化\n",
    "    transformer = torch.nn.DataParallel(transformer,  device_ids=list(range(ngpu))) # 设置并行执行  device_ids=[0,1]\n",
    "\n",
    "# optimizer = torch.optim.Adam(transformer.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "# lr_scheduler = CustomSchedule(optimizer, d_model, warm_steps=4000)\n",
    "\n",
    "optimizer = torch.optim.AdamW(transformer.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.02, amsgrad=True)\n",
    "\n",
    "lr_scheduler = CustomSchedule(optimizer, d_model, warm_steps)\n",
    "lr_scheduler2 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max',factor=0.5,verbose=True,min_lr= 0,patience=1)\n",
    "\n",
    "#optimizer = adamod.AdaMod(transformer.parameters(), lr=1e-3, beta3=0.999)\n",
    "# print('optimizer：', optimizer)\n",
    "\n",
    "#lr_scheduler2 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max',factor=0.5,verbose=True,min_lr= 0,patience=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 关于position encoding的一点自己的理解：\n",
    "\"\"\"\n",
    "为什么这里传入的pe_input是input_vocab_size，也就是说position encoding传入的pos参数是词表大小\n",
    "而我一开始理解的max_seq_len的大小，所以才在encoder中获取位置编码是self.pos_encoding[:, :seq_len, :]\n",
    "但知道看到这里传入的是词表大小我才发现我理解错了\n",
    "\n",
    "其实position encoding 就跟embedding 一样，就是【vocab_size x d_model】的\n",
    "但此时另一个问题又出现了，那为什么在获取position encoding时 使用的是pos_encoding[:, :seq_len, :]而不是\n",
    "像embedding一样 去look-up 查询每个token的位置表示呢？而是直接取得前seq_len个表示作为位置表示？\n",
    "\n",
    "这就体现了这种位置编码的巧妙之处，他不需要训练，就能独特的表示每一个单独的token，而每个token之间又存在关系\n",
    "所以这并不需要位置编码和token是一一对应（固定死的），只需要位置编码能传达这2点信息（即token的独特性和相对依赖性）就够了\n",
    "即使是同一个token 在一个句子的不同的位置时 他的位置编码当然也是不一样的了 如果像embedding那样去查 那一个句子的\n",
    "不同位置的相同token  其表示就会是一样的  就体现不出位置关系\n",
    "\n",
    "这样一想的话，似乎把pe_input设置成max_seq_len也是可以的。但是这里实现position encoding方式（sin  cos）\n",
    "可以在测试阶段接受长度超过训练集实例的情况！所以干脆把pe_input设置成词表大小？\n",
    "\"\"\"\n",
    "\n",
    "# inp [b,inp_seq_len]\n",
    "# targ [b,targ_seq_len]\n",
    "\"\"\"\n",
    "拆分targ, 例如：sentence = \"SOS A lion in the jungle is sleeping EOS\"\n",
    "tar_inp = \"<start>> A lion in the jungle is sleeping\"\n",
    "tar_real = \"A lion in the jungle is sleeping <end>\"\n",
    "\"\"\"\n",
    "def train_step(model, inp, targ, dependency_matrix):\n",
    "    # 目标（target）被分成了 tar_inp 和 tar_real\n",
    "    # tar_inp 作为输入传递到解码器。\n",
    "    # tar_real 是位移了 1 的同一个输入：在 tar_inp 中的每个位置，tar_real 包含了应该被预测到的下一个标记（token）。\n",
    "    targ_inp = targ[:, :-1] # targ_inp的尺寸是[batch,MAX_LENGTH+1],因为舍弃了<end> https://zhuanlan.zhihu.com/p/166608727\n",
    "#     print('targ_inp:',targ_inp.shape)\n",
    "    targ_real = targ[:, 1:]\n",
    "    \n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_mask(inp, targ_inp)\n",
    "#     print('enc_padding_mask',enc_padding_mask.shape)\n",
    "#     print('combined_mask',combined_mask.shape)\n",
    "#     print('dec_padding_mask',dec_padding_mask.shape)\n",
    "    \n",
    "    inp = inp.to(device)\n",
    "    targ_inp = targ_inp.to(device)\n",
    "    targ_real = targ_real.to(device)\n",
    "    dependency_matrix = dependency_matrix.to(device)\n",
    "#     print('inp.shape',inp.shape)\n",
    "#     print('targ_inp.shape',targ_inp.shape)\n",
    "#     print('dependency_matrix.shape',dependency_matrix.shape)\n",
    "    \n",
    "    enc_padding_mask = enc_padding_mask.to(device)\n",
    "    combined_mask = combined_mask.to(device)\n",
    "    dec_padding_mask = dec_padding_mask.to(device)\n",
    "    \n",
    "    # print('device:', inp.device, targ_inp)\n",
    "\n",
    "    model.train()  # 设置train mode\n",
    "\n",
    "    optimizer.zero_grad()  # 梯度清零\n",
    "\n",
    "    # forward\n",
    "    prediction, _ = model(inp, targ_inp, dependency_matrix, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "    # [b, targ_seq_len, target_vocab_size]\n",
    "    # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "    #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "\n",
    "    loss = mask_loss_func(targ_real, prediction)\n",
    "    metric = mask_accuracy_func(targ_real, prediction)\n",
    "\n",
    "    # backward\n",
    "    loss.backward()  # 反向传播计算梯度\n",
    "    optimizer.step()  # 更新参数\n",
    "\n",
    "    return loss.item(), metric.item()\n",
    "\n",
    "\n",
    "# # 检查train_step()的效果\n",
    "# batch_src, batch_targ = next(iter(train_dataloader)) # [64,10], [64,10]\n",
    "# print(train_step(transformer, batch_src, batch_targ))\n",
    "# \"\"\"\n",
    "# x += pos_encoding  # [b, inp_seq_len, d_model]\n",
    "# RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0!\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def validate_step(model, inp, targ, dependency_matrix):\n",
    "    targ_inp = targ[:, :-1]\n",
    "    targ_real = targ[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_mask(inp, targ_inp)\n",
    "\n",
    "    inp = inp.to(device)\n",
    "    targ_inp = targ_inp.to(device)\n",
    "    targ_real = targ_real.to(device)\n",
    "    dependency_matrix = dependency_matrix.to(device)\n",
    "    \n",
    "#     print('inp.shape:',inp.shape)\n",
    "#     print('targ_inp.shape:',targ_inp.shape)\n",
    "#     print('dependency_matrix.shape:',dependency_matrix.shape)\n",
    "    \n",
    "    enc_padding_mask = enc_padding_mask.to(device)\n",
    "    combined_mask = combined_mask.to(device)\n",
    "    dec_padding_mask = dec_padding_mask.to(device)\n",
    "\n",
    "    model.eval()  # 设置eval mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # forward\n",
    "        prediction, _ = model(inp, targ_inp, dependency_matrix, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "        # [b, targ_seq_len, target_vocab_size]\n",
    "        # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "        #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "\n",
    "        val_loss = mask_loss_func(targ_real, prediction)\n",
    "        val_metric = mask_accuracy_func(targ_real, prediction)\n",
    "\n",
    "    return val_loss.item(), val_metric.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print_trainstep_every = 500  # 每50个step做一次打印\n",
    "\n",
    "metric_name = 'acc'\n",
    "# df_history = pd.DataFrame(columns=['epoch', 'loss', metric_name]) # 记录训练历史信息\n",
    "df_history = pd.DataFrame(columns=['epoch', 'loss', metric_name, 'val_loss', 'val_' + metric_name])\n",
    "\n",
    "\n",
    "# 打印时间\n",
    "def printbar():\n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m_%d %H:%M:%S')\n",
    "    print('\\n' + \"====\"*8 + '%s'%nowtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, epochs, train_dataloader, val_dataloader, dep_train_batch_path, dep_val, print_every):\n",
    "    starttime = time.time()\n",
    "    print('*' * 10, 'start training...')\n",
    "    printbar()\n",
    "\n",
    "    best_acc = 0.\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        loss_sum = 0.\n",
    "        metric_sum = 0.\n",
    "        dep_train_batch_pairs = open(dep_train_batch_path,'rb')\n",
    "        \n",
    "        # dependency_matrix\n",
    "        for step, (inp, targ) in enumerate(train_dataloader, start=1):\n",
    "            # inp [64, 10] , targ [64, 10] \n",
    "#             print('inp.shape:',inp.shape)\n",
    "#             print('targ.shape:',targ.shape)\n",
    "            \n",
    "            # 提取依存句法矩阵\n",
    "#             st = (step-1) * inp.shape[0]\n",
    "#             end = st + inp.shape[0]\n",
    "#             dependency_matrix = dep_train[st:end,:,:]\n",
    "            dependency_matrix = pickle.load(dep_train_batch_pairs)\n",
    "#             print('dependency_matrix.shape:',dependency_matrix.shape)\n",
    "            \n",
    "            loss, metric = train_step(model, inp, targ, dependency_matrix)\n",
    "\n",
    "            loss_sum += loss\n",
    "            metric_sum += metric\n",
    "\n",
    "            # 打印batch级别日志\n",
    "            if step % print_every == 0:\n",
    "                print('*' * 5, f'[step = {step}] loss: {loss_sum / step:.5f}, {metric_name}: {metric_sum / step:.5f}')\n",
    "\n",
    "            lr_scheduler.step()  # 更新学习率\n",
    "            \n",
    "        dep_train_batch_pairs.close()\n",
    "\n",
    "        # 一个epoch的train结束，做一次验证\n",
    "        # test(model, train_dataloader)\n",
    "#         print('val')\n",
    "        val_loss_sum = 0.\n",
    "        val_metric_sum = 0.\n",
    "#         dep_val_batch_pairs = open(dep_val_batch_path,'rb')\n",
    "        \n",
    "        for val_step, (inp, targ) in enumerate(val_dataloader, start=1):\n",
    "            # inp [64, 10] , targ [64, 10]\n",
    "            \n",
    "            # 提取依存句法矩阵\n",
    "            st = (val_step-1) * inp.shape[0]\n",
    "            end = st + inp.shape[0]\n",
    "            dependency_matrix = dep_val[st:end,:,:]\n",
    "#             dependency_matrix = pickle.load(dep_val_batch_pairs)\n",
    "            \n",
    "            loss, metric = validate_step(model, inp, targ, dependency_matrix)\n",
    "\n",
    "            val_loss_sum += loss\n",
    "            val_metric_sum += metric\n",
    "            \n",
    "#         dep_val_batch_pairs.close()\n",
    "        \n",
    "        # 记录和收集1个epoch的训练（和验证）信息\n",
    "        # record = (epoch, loss_sum/step, metric_sum/step)\n",
    "        record = (epoch, loss_sum/step, metric_sum/step, val_loss_sum/val_step, val_metric_sum/val_step)\n",
    "        df_history.loc[epoch - 1] = record\n",
    "\n",
    "        # 打印epoch级别的日志\n",
    "        # print('*'*8, 'EPOCH = {} loss: {:.3f}, {}: {:.3f}'.format(\n",
    "        #        record[0], record[1], metric_name, record[2]))\n",
    "        print('EPOCH = {} loss: {:.5f}, {}: {:.5f}, val_loss: {:.5f}, val_{}: {:.5f}'.format(\n",
    "            record[0], record[1], metric_name, record[2], record[3], metric_name, record[4]))\n",
    "        printbar()\n",
    "        \n",
    "        # 监视loss的同时监视val_acc，防止loss下降而val_loss不下降\n",
    "        lr_scheduler2.step(record[4])\n",
    "        \n",
    "        # 保存模型\n",
    "        # current_acc_avg = metric_sum / step\n",
    "        current_acc_avg = val_metric_sum / val_step # 看验证集指标\n",
    "        \n",
    "        # 每隔几个epoch保存更好的模型，最后的几个epoch只要优于best_acc都保存\n",
    "        if (current_acc_avg > best_acc and epoch%6==0) or (current_acc_avg > best_acc and (epochs + 1 - epoch < 2)):\n",
    "            best_acc = current_acc_avg\n",
    "            checkpoint = train_model_save + '{:03d}_ckpt.tar'.format(epoch)\n",
    "            if device.type == 'cuda' and ngpu > 1:\n",
    "                # model_sd = model.module.state_dict()  ##################\n",
    "                model_sd = copy.deepcopy(model.module.state_dict())\n",
    "            else:\n",
    "                # model_sd = model.state_dict(),  ##################\n",
    "                model_sd = copy.deepcopy(model.state_dict())  ##################\n",
    "            torch.save({\n",
    "                'loss': loss_sum / step,\n",
    "                'epoch': epoch,\n",
    "                'net': model_sd,\n",
    "                'opt': optimizer.state_dict(),\n",
    "                'lr_scheduler': lr_scheduler.state_dict()\n",
    "            }, checkpoint)\n",
    "    \n",
    "    print('finishing training...')\n",
    "    endtime = time.time()\n",
    "    time_elapsed = endtime - starttime\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    return df_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dep_train = torch.zeros(len(train_pairs), MAX_LENGTH+1, MAX_LENGTH+1)\n",
    "dep_val = torch.zeros(len(val_pairs), MAX_LENGTH+1, MAX_LENGTH+1)\n",
    "dep_test = torch.zeros(len(test_pairs), MAX_LENGTH+1, MAX_LENGTH+1)\n",
    "\n",
    "dep_train_batch_path = data_dir + 'data_sets/dep_train_batch_pairs'\n",
    "dep_val_batch_path = data_dir + 'data_sets/dep_val_batch_pairs'\n",
    "# for step, (inp, targ) in enumerate(test_dataloader, start=1):\n",
    "    \n",
    "#     st = (step-1) * inp.shape[0]\n",
    "#     end = st + inp.shape[0]\n",
    "#     dependency_matrix = dep_test[st:end,:,:]\n",
    "    \n",
    "#     if step%1 == 0:\n",
    "#         print(step)\n",
    "#         print(inp.shape)\n",
    "#         print(targ.shape)\n",
    "#         print(dependency_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import pickle\n",
    "# # import torch\n",
    "# with open(r'/home/chengkun/jupyter_projects/Magic-NLPer-main/data/data_sets/dep_train_pairs','rb') as f:\n",
    "#     dep_train = pickle.load(f)\n",
    "#     f.close()\n",
    "\n",
    "# with open(r'/home/chengkun/jupyter_projects/Magic-NLPer-main/data/data_sets/dep_val_pairs','rb') as f:\n",
    "#     dep_val = pickle.load(f)\n",
    "#     f.close()\n",
    "    \n",
    "# # dep_test = torch.zeros(len(test_pairs), MAX_LENGTH+1, MAX_LENGTH+1)\n",
    "\n",
    "# print(dep_train.shape)\n",
    "\n",
    "\n",
    "\n",
    "# with open(r'/home/chengkun/jupyter_projects/Magic-NLPer-main/data/data_sets/dep_val_pairs','rb') as f:\n",
    "#     count  = 0\n",
    "#     while True:\n",
    "#         try:\n",
    "#             aa=pickle.load(f)\n",
    "#             count = count + 1\n",
    "#             if count%240 == 0:\n",
    "#                 print(count)\n",
    "#         except EOFError:\n",
    "#             break\n",
    "# print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** start training...\n",
      "\n",
      "================================2021-11_10 10:16:20\n",
      "***** [step = 500] loss: 6.68521, acc: 0.10608\n",
      "***** [step = 1000] loss: 5.90174, acc: 0.15691\n",
      "***** [step = 1500] loss: 5.32744, acc: 0.20656\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "# df_history = train_model(transformer, EPOCHS, train_dataloader, val_dataloader, \n",
    "#                          dep_train_batch_path, dep_val_batch_path, print_trainstep_every)\n",
    "\n",
    "df_history = train_model(transformer, EPOCHS, train_dataloader, val_dataloader, \n",
    "                         dep_train_batch_path, dep_val, print_trainstep_every)\n",
    "\n",
    "# 保存df_history变量\n",
    "save_file = open(result_save + \"transformer_df_history.bin\", \"wb\")\n",
    "pickle.dump(df_history, save_file)\n",
    "save_file.close()\n",
    "print(df_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 绘制训练曲线\n",
    "def plot_metric(df_history, metric):\n",
    "    plt.figure()\n",
    "\n",
    "    train_metrics = df_history[metric]\n",
    "    val_metrics = df_history['val_' + metric]  #\n",
    "\n",
    "    epochs = range(1, len(train_metrics) + 1)\n",
    "\n",
    "    plt.plot(epochs, train_metrics, 'bo--')\n",
    "    plt.plot(epochs, val_metrics, 'ro-')  #\n",
    "\n",
    "    plt.title('Training and validation ' + metric)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([\"train_\" + metric, 'val_' + metric])\n",
    "    plt.savefig(result_save + metric + '.png')  # 保存图片\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "plot_metric(df_history, 'loss')\n",
    "plot_metric(df_history, metric_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 17.评估\n",
    "这里的评估 没有额外的使用测试集测评，还是拿验证集测试的。另外可以对一个法语句子进行翻译，看看翻译的结果如何\n",
    "\n",
    "以下步骤用于评估：\n",
    "- 用法语分词器（tokenizer_pt）编码输入语句。此外，添加<start>和<end>标记，这样输入就与模型训练的内容相同。这是编码器输入。\n",
    "- 解码器输入为 <start> token id\n",
    "- 计算padding mask 和 look ahead mask\n",
    "- 解码器通过查看编码器输出和它自身的输出（自注意力）给出预测。\n",
    "- 选择最后一个词并计算它的 argmax。\n",
    "- 将预测的词concat到解码器输入，然后传递给解码器。\n",
    "- 在这种方法中，解码器根据它之前预测的words预测下一个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 加载model\n",
    "a=os.listdir(train_model_save)\n",
    "a.sort()\n",
    "print(a[-1])\n",
    "checkpoint = train_model_save + a[-1]\n",
    "print('checkpoint:', checkpoint)\n",
    "\n",
    "# ckpt = torch.load(checkpoint, map_location=device)  # dict  save 在 CPU 加载到GPU\n",
    "ckpt = torch.load(checkpoint)  # dict  save 在 GPU 加载到 GPU\n",
    "# print('ckpt', ckpt)\n",
    "transformer_sd = ckpt['net']\n",
    "# optimizer_sd = ckpt['opt'] # 不重新训练的话不需要\n",
    "# lr_scheduler_sd = ckpt['lr_scheduler']\n",
    "\n",
    "reload_model = mn_Transformer(num_layers,\n",
    "                           d_model,\n",
    "                           num_heads,\n",
    "                           dff,\n",
    "                           input_vocab_size,\n",
    "                           target_vocab_size,\n",
    "                           pe_input=input_vocab_size,\n",
    "                           pe_target=target_vocab_size,\n",
    "                           seq_len=MAX_LENGTH+2,\n",
    "                           rate=dropout_rate)\n",
    "\n",
    "reload_model = reload_model.to(device)\n",
    "# reload_model.load_state_dict(transformer_sd)\n",
    "\n",
    "if ngpu > 1:\n",
    "    reload_model = torch.nn.DataParallel(reload_model,  device_ids=list(range(ngpu))) # 设置并行执行  device_ids=[0,1]\n",
    "\n",
    "\n",
    "print('Loading model ...')\n",
    "if device.type == 'cuda' and ngpu > 1:\n",
    "   reload_model.module.load_state_dict(transformer_sd)\n",
    "else:\n",
    "   reload_model.load_state_dict(transformer_sd)\n",
    "print('Model loaded ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test(model, dataloader, dep):\n",
    "    # model.eval() # 设置为eval mode\n",
    "\n",
    "    test_loss_sum = 0.\n",
    "    test_metric_sum = 0.\n",
    "    for test_step, (inp, targ) in enumerate(dataloader, start=1):\n",
    "        # inp [64, 10] , targ [64, 10]\n",
    "        \n",
    "        st = (test_step-1) * inp.shape[0]\n",
    "        end = st + inp.shape[0]\n",
    "        dependency_matrix = dep[st:end,:,:]\n",
    "        \n",
    "#         print('inp.shape',inp.shape)\n",
    "#         print('targ.shape',targ.shape)\n",
    "#         print('dependency_matrix.shape',dependency_matrix.shape)\n",
    "        \n",
    "        loss, metric = validate_step(model, inp, targ, dependency_matrix)\n",
    "        # print('*'*8, loss, metric)\n",
    "\n",
    "        test_loss_sum += loss\n",
    "        test_metric_sum += metric\n",
    "    # 打印\n",
    "    print('*' * 8,\n",
    "          'Test: loss: {:.5f}, {}: {:.5f}'.format(test_loss_sum / test_step, 'test_acc', test_metric_sum / test_step))\n",
    "\n",
    "\n",
    "# 在测试集上测试指标，这里使用val_dataloader模拟测试集\n",
    "print('*' * 8, 'final test...')\n",
    "test(reload_model, test_dataloader, dep_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def tokenizer_encode(tokenize, sentence, vocab):\n",
    "    # print(type(vocab)) # torchtext.vocab.Vocab\n",
    "    # print(len(vocab))\n",
    "    sentence = normalizeString(sentence)\n",
    "    # print(type(sentence)) # str\n",
    "    sentence = tokenize(sentence)  # list\n",
    "#     print(sentence)\n",
    "    sentence = ['<start>'] + sentence + ['<end>']\n",
    "    if len(sentence) < MAX_LENGTH + 2:\n",
    "        sentence = sentence + (MAX_LENGTH + 2 - len(sentence)) * ['<pad>']\n",
    "#     print(sentence)\n",
    "    sentence_ids = [vocab.stoi[token] for token in sentence]\n",
    "    # print(sentence_ids, type(sentence_ids[0])) # int\n",
    "    return sentence_ids\n",
    "\n",
    "\n",
    "def tokenzier_decode(sentence_ids, vocab):\n",
    "    sentence = [vocab.itos[id] for id in sentence_ids if id<len(vocab)]\n",
    "    # print(sentence)\n",
    "    return \" \".join(sentence)\n",
    "\n",
    "# 只有一个句子，不需要加pad\n",
    "s = 'ᠲᠦᠩᠯᠢᠶᠣᠣ ᠬᠣᠲᠠ ᠶ᠋ᠢᠨ ᡁᠢ ᠶᠤᠸᠠᠨ ᠠᠱᠢᠨ ᠲᠡᠷᠭᠡ ᠲᠦᠷᠢᠶᠡᠰᠦᠯᠡᠭᠦᠯᠬᠦ ᠬᠢᠵᠠᠭᠠᠷᠲᠤ ᠺᠣᠮᠫᠠᠨᠢ ᠬᠣᠲᠠ'\n",
    "# print(len(s.split()))\n",
    "print(tokenizer_encode(tokenizer, s, SRC_TEXT.vocab))\n",
    "\n",
    "\n",
    "s_ids = [3, 5, 251, 17, 46, 35, 12, 36, 4, 2]\n",
    "print(tokenzier_decode(s_ids, SRC_TEXT.vocab))\n",
    "print(tokenzier_decode(s_ids, TARG_TEXT.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inp_sentence 一个法语句子，例如\"je pars en vacances pour quelques jours .\"\n",
    "def evaluate(model, inp_sentence, dependency_matrix):\n",
    "    model.eval()  # 设置eval mode\n",
    "\n",
    "    inp_sentence_ids = tokenizer_encode(tokenizer, inp_sentence, SRC_TEXT.vocab)  # 转化为索引\n",
    "#     print(tokenzier_decode(inp_sentence_ids, SRC_TEXT.vocab))\n",
    "    encoder_input = torch.tensor(inp_sentence_ids).unsqueeze(dim=0)  # =>[b=1, inp_seq_len=10]\n",
    "#     print('encoder_input.shape：', encoder_input.shape)\n",
    "\n",
    "    decoder_input = [TARG_TEXT.vocab.stoi['<start>']]\n",
    "#     print(decoder_input)\n",
    "    decoder_input = torch.tensor(decoder_input).unsqueeze(0)  # =>[b=1,seq_len=1]\n",
    "#     print('decoder_input.shape：', decoder_input.shape)\n",
    "#     print(MAX_LENGTH)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(MAX_LENGTH + 2):\n",
    "            enc_padding_mask, combined_mask, dec_padding_mask = create_mask(encoder_input.cpu(), decoder_input.cpu())\n",
    "            # [b,1,1,inp_seq_len], [b,1,targ_seq_len,inp_seq_len], [b,1,1,inp_seq_len]\n",
    "\n",
    "            encoder_input = encoder_input.to(device)\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            enc_padding_mask = enc_padding_mask.to(device)\n",
    "            combined_mask = combined_mask.to(device)\n",
    "            dec_padding_mask = dec_padding_mask.to(device)\n",
    "            dependency_matrix = dependency_matrix.to(device)\n",
    "\n",
    "            # forward\n",
    "            \n",
    "#             print('encoder_input.shape',encoder_input.shape)\n",
    "#             print('decoder_input.shape',decoder_input.shape)\n",
    "#             print('dependency_matrix.shape',dependency_matrix.shape)\n",
    "            \n",
    "            predictions, attention_weights = model(encoder_input,\n",
    "                                                   decoder_input,\n",
    "                                                   dependency_matrix,\n",
    "                                                   enc_padding_mask,\n",
    "                                                   combined_mask,\n",
    "                                                   dec_padding_mask)\n",
    "            # [b=1, targ_seq_len, target_vocab_size]\n",
    "            # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "            #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "\n",
    "            # 看最后一个词并计算它的 argmax\n",
    "            prediction = predictions[:, -1:, :]  # =>[b=1, 1, target_vocab_size]\n",
    "            prediction_id = torch.argmax(prediction, dim=-1)  # => [b=1, 1]\n",
    "            # print('prediction_id:', prediction_id, prediction_id.dtype) # torch.int64\n",
    "            if prediction_id.squeeze().item() == TARG_TEXT.vocab.stoi['<end>']:\n",
    "                return decoder_input.squeeze(dim=0), attention_weights\n",
    "\n",
    "            decoder_input = torch.cat([decoder_input, prediction_id],\n",
    "                                      dim=-1)  # [b=1,targ_seq_len=1]=>[b=1,targ_seq_len=2]\n",
    "            # decoder_input在逐渐变长\n",
    "\n",
    "    return decoder_input.squeeze(dim=0), attention_weights\n",
    "    # [targ_seq_len],\n",
    "    # {'..block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "    #  '..block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "\n",
    "\n",
    "\n",
    "# s = 'je pars en vacances pour quelques jours .'\n",
    "# evaluate(s)\n",
    "\n",
    "\n",
    "# s = 'I want to know now .'\n",
    "# s_targ = 'Je veux le savoir maintenant.'\n",
    "# 翻译时不提供外部知识，因为目标句子不参与翻译，因此dependency_matrix设置为0\n",
    "s = 'ᠥᠪᠥᠷ ᠣᠩᠭᠣᠯ ᠤ᠋ᠨ ᠱᠸᠩ ᠾᠧ ᠠᠱᠢᠨ ᠲᠡᠷᠭᠡᠨ ᠦ᠌ ᠦᠢᠯᠡᠴᠢᠯᠡᠭᠡ ᠶ᠋ᠢᠨ ᠬᠢᠵᠠᠭᠠᠷᠲᠤ ᠺᠣᠮᠫᠠᠨᠢ'\n",
    "s_targ = '内 蒙 古 盛 和 汽 车 服 务 有 限 公 司'\n",
    "dep = torch.zeros(1)\n",
    "\n",
    "pred_result, attention_weights = evaluate(reload_model, s, dep)\n",
    "pred_sentence = tokenzier_decode(pred_result, TARG_TEXT.vocab)\n",
    "print('real target:', s_targ)\n",
    "print('pred_sentence:', pred_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 批量翻译\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "dep = torch.zeros(1)\n",
    "sentence_pairs = test_pairs\n",
    "print('测试集句子数目：', len(sentence_pairs))\n",
    "\n",
    "# total_score = 0\n",
    "number = 0.0\n",
    "f1 = open(result_save + 'target1.txt','w', encoding='utf-8')\n",
    "f2 = open(result_save + 'pred1.txt','w', encoding='utf-8')\n",
    "for pair in sentence_pairs:\n",
    "    pred_result, _ = evaluate(reload_model, pair[0],dep)\n",
    "    pred_sentence = tokenzier_decode(pred_result, TARG_TEXT.vocab).replace('<start>','').replace('<end>','').replace('\\n','')\n",
    "\n",
    "    f1.write(pair[1] + '\\n')\n",
    "    f2.write(pred_sentence + '\\n')\n",
    "\n",
    "    number = number + 1\n",
    "    if number%1000 ==0:\n",
    "        print(number)\n",
    "\n",
    "f1.close()\n",
    "f2.close()\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 随机选个句子翻译\n",
    "dep = torch.zeros(1)\n",
    "def evaluateRandomly(n=5):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('input:', pair[0])\n",
    "        print('target:', pair[1])\n",
    "        pred_result, attentions = evaluate(reload_model, pair[0],dep)\n",
    "        pred_sentence = tokenzier_decode(pred_result, TARG_TEXT.vocab)\n",
    "        print('pred:', pred_sentence)\n",
    "        print('')\n",
    "\n",
    "\n",
    "evaluateRandomly(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 18.attention 的可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 可视化attenton 这里我们只展示...block2的attention，即[b, num_heads, targ_seq_len, inp_seq_len]\n",
    "# attention: {'decoder_layer{i + 1}_block1': [b, num_heads, targ_seq_len, targ_seq_len],\n",
    "#             'decoder_layer{i + 1}_block2': [b, num_heads, targ_seq_len, inp_seq_len], ...}\n",
    "# sentence: [seq_len]，例如：'je recherche un assistant .'\n",
    "# pred_result: [seq_len]，例如：'<start> i m looking for an assistant .'\n",
    "# layer: 字符串类型，表示模型decoder的N层decoder-layer的第几层的attention，形如'decoder_layer{i}_block1'或'decoder_layer{i}_block2'\n",
    "def plot_attention_weights(attention, sentence, pred_sentence, layer):\n",
    "    \n",
    "    # block2 attention[layer] => [b=1, num_heads, targ_seq_len, inp_seq_len]\n",
    "    attention = torch.squeeze(attention[layer], dim=0) # => [num_heads, targ_seq_len, inp_seq_len]\n",
    "#     print(attention.shape)\n",
    "\n",
    "    # print(matplotlib.matplotlib_fname())\n",
    "    plt.rcParams['font.sans-serif'] = ['Mongolian Baiti']\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    \n",
    "    sentence = sentence.split()\n",
    "    pred_sentence = pred_sentence.split()\n",
    "\n",
    "    fig = plt.figure(figsize=(len(pred_sentence), len(sentence)+2)) #figsize=(attention.shape[1], attention.shape[2])\n",
    "    \n",
    "    for head in range(attention.shape[0]):\n",
    "        ax = fig.add_subplot(2, 4, head + 1)  # 111是单个整数编码的子绘图网格参数。例如，“111”表示“1×1网格，第一子图”，“234”表示“2×3网格，第四子图”\n",
    "\n",
    "        cax = ax.matshow(attention[head].cpu(), cmap='viridis')  # 绘制网格热图？注意力权重\n",
    "        # fig.colorbar(cax)#给子图添加colorbar（颜色条或渐变色条）\n",
    "\n",
    "        fontdict = {'fontsize': 10}\n",
    "\n",
    "        # 设置轴刻度线\n",
    "        ax.set_xticks(range(len(sentence)+2))  # 算上start和end\n",
    "        ax.set_yticks(range(len(pred_sentence)))\n",
    "\n",
    "        ax.set_xlim(-0.5,len(sentence) + 0.5)  # 设定x座标轴的范围\n",
    "        ax.set_ylim(len(pred_sentence) - 1.5, -0.5)  # 设定y座标轴的范围\n",
    "\n",
    "        # 设置轴\n",
    "        ax.set_xticklabels(['<start>']+sentence+['<end>'], fontdict=fontdict, rotation=90)  # 顺时间旋转90度\n",
    "        ax.set_yticklabels(pred_sentence, fontdict=fontdict, family = 'SimHei')\n",
    "\n",
    "        ax.set_xlabel('Head {}'.format(head + 1))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(result_save+'attention_{}.pdf'.format(layer),format='pdf',bbox_inches='tight')\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "def translate(sentence_pair, dep, plot=None):\n",
    "    print('input:', sentence_pair[0])\n",
    "    print('target:', sentence_pair[1])\n",
    "    pred_result, attention_weights = evaluate(reload_model, sentence_pair[0],dep)\n",
    "    print('attention_weights:', attention_weights.keys())\n",
    "    pred_sentence = tokenzier_decode(pred_result, TARG_TEXT.vocab)\n",
    "    print('pred:', pred_sentence)\n",
    "    print('')\n",
    "\n",
    "    if plot:\n",
    "        plot_attention_weights(attention_weights, sentence_pair[0], pred_sentence, plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# dep = torch.zeros(1)\n",
    "# translate(sentence_pairs[481], dep, 'decoder_layer4_block2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# translate(sentence_pairs[2], plot='decoder_layer4_block2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = os.popen('/home/chengkun/moses/mosesdecoder/scripts/generic/multi-bleu.perl -lc /home/chengkun/jupyter_projects/Magic-NLPer-main/best_save/{}/target1.txt < /home/chengkun/jupyter_projects/Magic-NLPer-main/best_save/{}/pred1.txt'.format(begin_time,begin_time))\n",
    "print(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = []\n",
    "with open('/home/chengkun/jupyter_projects/Magic-NLPer-main/best_save/{}/target1.txt'.format(begin_time),'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        target.append(line.strip('\\n'))#去掉列表中每一个元素的换行符\n",
    "target = [target]\n",
    "f.close()\n",
    "\n",
    "pred = []\n",
    "with open('/home/chengkun/jupyter_projects/Magic-NLPer-main/best_save/{}/pred1.txt'.format(begin_time),'r', encoding='utf-8') as f1:\n",
    "    for line in f1.readlines():\n",
    "        pred.append(line.strip('\\n'))#去掉列表中每一个元素的换行符\n",
    "\n",
    "f1.close()\n",
    "\n",
    "\n",
    "bleu = sacrebleu.corpus_bleu(pred, target,smooth_method='none')\n",
    "bleu_score = format(bleu.score,'.3f')\n",
    "print(bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.system('cp  {} {}'.format(checkpoint,result_save))\n",
    "os.system('cp  {} {}'.format('/home/chengkun/jupyter_projects/Magic-NLPer-main/re/primer-ez/pytask-primer.log',result_save))\n",
    "os.system('cp  {} {}'.format('/home/chengkun/jupyter_projects/Magic-NLPer-main/train_save/transformer_improved_encoder_decoder_dep/*',result_save))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep = torch.zeros(1)\n",
    "translate(test_pairs[481], dep, 'decoder_layer5_block2')\n",
    "translate(test_pairs[481], dep, 'decoder_layer5_block1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.rename(result_save[:-1], result_save[:-1]+'_'+str(bleu_score)+'_'+str(seed)+'_'+file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 19.总结\n",
    "\n",
    "代码还有以下待完善的地方：\n",
    "- 没有实现标签平滑\n",
    "- 在训练过程中使用了teacher-forcing，即总是会将target传递到下一时间步长。更好的做法是设置一个teacher_forcing_ration\n",
    "- 在evaluate阶段的解码使用的是greedy search decode，即对于每一步，我们只需从具有最高 softmax 值的 decoder_output 中选择单词。可以尝试使用更好的beam search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
